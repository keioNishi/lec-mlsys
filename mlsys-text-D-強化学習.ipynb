{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-D-PyTorch-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92.ipynb","timestamp":1640313351715},{"file_id":"1quqOzh8F3nQygNWiSXkd61FDzMwtGRID","timestamp":1609282296170}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gU-Arpc9zElH"},"source":["---\n",">「ムダが必要だと思うんだ。いろいろ練習して、自分の体に合ったものを見つける」\\\n",">中野 浩一\n","---"]},{"cell_type":"markdown","metadata":{"id":"UJP_ip5qtk1p"},"source":["# 強化学習\n","\n","強化学習はReinforcement Learningの訳であり試行錯誤しながら最適な制御を実現する機械学習手法のひとつ\n","- 強化学習の概念は古く、1950年代に自立制御として存在、その後深層強化学習と呼ばれるディープラーニングを応用した手法が登場した\n","- 強化学習は教師あり学習に似ているが、明確な答えは提示されず、行動の選択肢と、その行動に対する報酬が与えられる\n","\n","なお、強化学習に限っても様々な手法と考え方が存在している\n","- ここでは、連続行動空間ではなく、離散行動空間を考える\n","- 連続行動空間の場合、数式モデルによる古典制御などを用いた最適化なども想定されるため、手に負えないことからここでは敢えて触れない\n","\n"]},{"cell_type":"markdown","source":["## 強化学習とは何をするのか？\n","\n","**繰り返し学習**することにより、**収益最大化**を目的として**状態**に応じて**行動**を選択する手法、つまり行動選択をどうするかを追求している\n","\n","- ある環境の**状態(State)**に対してある**行動(Action)**を行うが、状態から行動を選択することを**方策$\\pi$**と呼ぶ\n","- 行動により問題解決に向けて望ましいかどうかやその程度により**報酬(Reward)**が獲得でき、またその行動が環境に作用し、次の状態に遷移するという一連の流れを**マルコフ決定過程(Markov decision process:MDP)**と呼ぶ\n","\n","この状態から行動を様々に選択してデータを集め、その集めたデータから、よりよい行動を選択するように方策を更新することで学習を進めるが、この一連の流れを**Generalized Policy Iteration(GPI)**と呼ぶ"],"metadata":{"id":"TIFqIOuHFoRP"}},{"cell_type":"markdown","source":["## より具体的な強化学習の流れ\n","\n","- **エージェント(Agent)**がある**環境(Environment)**に置かれ、その環境のその時の**状態(State)**に対して**行動(Action)**を起こす\n","  - エージェントはある条件で行動する主体であり、環境はエージェントを取り巻く周辺要素でエージェントに作用するものを指し、エージェントの環境に対するインタラクションを行動と呼ぶ\n","  \n","- 環境がエージェントに、行動により更新された状態と**報酬(Reward)**をフィードバックする\n","  - 状態はエージェントが存在する仮想空間の情報で行動により更新される\n","  - エージェントの行動指針として報酬を利用する\n","    - 望ましい行動に対して正の報酬、望ましくない行動に対して負の報酬を与え、その程度を値の大きさで表現する\n","    - 強化学習では、行動と報酬は直結しておらず「答え=報酬」ではない\n","    - 行動や行動の連続により環境がどのように変化したかなど、多角的に報酬が与えられる\n","\n","- 環境からのフィードバックを元に**方策(Policy:$\\pi$)**を修正する\n","  - エージェントは、行動に対する状態と報酬のフィードバックを元に、将来得られる価値を最大化する方策を導き出す\n","  - 方策の導出に用いられるのが2つの価値関数である、**状態価値関数(V)**と**行動価値関数(Q)**である\n","    - 状態価値関数は、ある状態$s$において、エージェントが方策$\\pi$を実行した際に得られる価値を求める\n","    - 行動価値関数は、エージェントがランダムな行動$a$を実行した後に方策πを実行した際に得られる価値を求める\n","    - 価値関数の値が大きくなれば収益が大きくなるため、価値関数を用いて評価する\n","  - 両方ともに最終的に方策$\\pi$を選択するが、学習過程では、しばしば行動価値関数はランダムな行動$a$を選択し、この行動により得られる価値が増えるならば、行動$a$を方策の一部として採用して方策を修正する\n","\n","- これまでの一連の行動の結果として変化した環境の中で、再びエージェントが環境に対して行動を起こす\n","\n","以上の、行動し、その行動による環境の変化と報酬のフィードバックをえて、その行動を取った場合と取らなかった場合の価値を比較し方策を修正するというサイクル(GPI)を繰り返すことで強化学習が進む\n"],"metadata":{"id":"a22XucasFkeY"}},{"cell_type":"markdown","source":["## 強化学習をどのようにすすめるのか\n","\n","教師あり学習では、データをモデルに投入してラベルが正しく推論できるように学習する\n","\n","強化学習では、様々な学習、学習対象が想定できる\n","- 様々な学習が想定できる\n","  - データを集めて、そのデータから方策を学習する\n","  - データおよび価値関数から方策を学習する\n","  - データと方策から価値関数を学習する\n","- 様々な学習対象が想定できる\n","  - 通常はVとQを学習させる\n","  - Qのみ学習させる\n","  - $\\pi$のみ学習させる\n","\n","この組み合わせが存在する\n","\n","つまり、データ、方策、価値関数、MDPを様々に駆使して学習をすすめることになる\n","- Q-Learningでは行動から収益を学習する\n","- Policy Networkでは状態から行動を学習する\n","- Value Networkでは状態から収益を学習する\n","- Alpha Goでは、Policy NetworkとValue Networkの両方を学習させる\n","\n","GPIのステップごとに、これらのうち何を用いて何をするのかを明確にすると理解しやすい\n","- 状態、行動、報酬、状態、と方策とMDPを繰り返すときの系列データをエピソード(Episode)、データ全体を軌跡(Trajectory)と呼ぶ\n","\n","なお、ここでは、深層機械学習について学んでいるという立場から、Deep Q Networkによる強化学習のみ学ぶ"],"metadata":{"id":"JsJ-xC-hLCDX"}},{"cell_type":"markdown","metadata":{"id":"4xfwqN_9Bzex"},"source":["# 価値反復法\n","\n","\n"]},{"cell_type":"markdown","source":["価値反復法(Vlaue Iteration)は、価値が最大となる行動を選択するという方策について、価値を繰り返し更新しながら価値を推定する方法のこと\n","\n","価値をベースに考えるValu-based手法である"],"metadata":{"id":"o2tlug1gY7Yf"}},{"cell_type":"markdown","metadata":{"id":"kyxHmKM7OzaR"},"source":["### 定式化\n","\n","次のように定式化する\n","- 状態空間(環境がとりうる状態の集)を$\\mathcal{S}=\\{s_1,\\ldots,s_m\\}$、行動空間(エージェントがとりうる行動の集合)を$\\mathcal{A}=\\{a_1,\\ldots,a_n\\}$、施策(ポリシー)を$\\pi(s,a)=P(a_t=a|s_t=s)$、報酬関数を$r_{t+1}=r(s_t,a_t)$とする\n","- エージェントは時刻$t$の状態$s_t$において、政策$\\pi$にしたがって行動$a_t$を選択し、次の状態$s_{t+1}$に遷移して、報酬$r_{t+1}$を得るとする\n","- なお、状態によって選択できる行動は異なることから、ある状態$s$であるときにとれる行動$a$の集合を$\\mathcal{A_s}$とする\n","\n","行動$a$を重ねるごとに受け取る報酬$r$について、時刻やステップなど、時間的遷移を付与して$r_t$と表現する\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2mG2eMz_O2UO"},"source":["### 報酬の表現\n","\n","その将来受け取るであろう全報酬を**報酬和**$G_t$と呼び次のように表すことができる\n","$$\n","G_t = r_{t+1}+r_{t+2}+r_{t+3}+\\cdots\n","$$\n","\n","ここで、今すぐもらえる報酬と、将来もらえる報酬を等価に扱うことが学習を進めるうえでよいことか？を考える必要がある\n","- 将来の成功はその都度うまくいったという積み重ねであり、将来を狙っても状況の変化に追従できなければ意味がない\n","- さらに、ステップを踏むことに負の報酬がない場合は、早く報酬を得たことに対するメリットが得られない\n","\n","これらを解決手法として**時間割引**があり、具体的には**時間割引率**$\\gamma(0<\\gamma<1)$を導入して、次の**割引報酬和**を用いる\n","\n","これを、期待割引収益とも表現し、遠い将来の報酬は割り引いて考え、期待値を求めることである\n","- 強化学習は、この期待割引収益を最大化することを目的とする場合が殆どである\n","\n","$$\n","G_t = r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+3}+\\cdots\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DuxOwejlO5Lz"},"source":["### 行動価値と状態価値\n"]},{"cell_type":"markdown","metadata":{"id":"2Pu3gOgWdAZb"},"source":["\n","#### 行動価値\n","\n","時刻$t$においてある状態$s$にあり、行動$a$を選択したときに得られる報酬が$r(s,a) = r_{t+1}$とすると、行動価値関数$Q$は、$Q(s,a) = r_{t+1}$となる\n","- エージェントは最もQ値の高い行動を選択することになる\n","\n","例えば、先に示した迷路を解くという問題では、Qテーブルは、S1からS9の全状態について、それぞれ4つの行動が想定されており、これを表にして期待できる報酬を記載したテーブルである\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Qtable.png\" width=300>\n","\n","Q学習は強化学習の一種で、各状態と行動の組み合わせにQ値を設定したテーブルを用いて学習する  \n","- エージェントは最もQ値の高い行動を選択し、Q-Tableの各値が最適化されることで学習が進む\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"drQqR085nLJ3"}},{"cell_type":"markdown","source":[],"metadata":{"id":"h7l4g8T1nLHD"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ufVsPA0knLEV"}},{"cell_type":"markdown","source":[],"metadata":{"id":"xQfkcD7znLBi"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1t3MS8brnK0Z"}},{"cell_type":"markdown","metadata":{"id":"dw2P-EgGdCAo"},"source":["#### 状態価値\n","\n","状態$s$において方策$\\pi$に従って行動することで、その後将来に渡って得られることが期待される割引報酬和$G_t$を状態価値と呼ぶ\n","\n","これを一般的に表現したのがベルマンの方程式であり次のように表す\n","\n","$$\n","V^\\pi(s) = max_a\\mathbb{E}[r_{s,a}+\\gamma\\times V^\\pi(s_{(s,a)})]\n","$$\n","\n","ここで、\n","- $V^\\pi(s)$: 状態$s$での状態価値$V$であり、左辺値が最も大きくなる行動$a$を選択したときに期待される値\n","- $s_{(s,a)}$: 状態$s$で行動$a$を選択して移動した次の状態$s_{t+1}$\n","\n","であり、ベルマン方程式は、新たな状態$s_{t+1}$における状態価値$V$に1ステップ分の時間割引率を掛けた項に即時報酬$r_{s,a}$を加えた値の最大値を現在の状態価値とする式\n","- ベルマン方程式が成り立つためには学習過程がマルコフ決定過程であることが必要\n","- マルコフ決定過程とは、定常性(常にその発生確率などは一定)、独立性(過去や未来の事象の発生が現在に影響しない)、希少性(同時に複数のイベントが発生しない)を満たす過程\n"]},{"cell_type":"markdown","metadata":{"id":"IQf1uqO5SYDE"},"source":["## SarsaとTD誤差\n","\n","行動価値関数$Q$が望ましい値になるように更新学習するための一手法\n","\n","ベルマンの方程式において、行動価値関数$Q(s,a)$が正しく求まっていれば、\n","\n","$$\n","Q_(s_t, a_t) = r_{t_+1}+\\gamma Q(s_{t_1}, a_{t+1})\n","$$\n","\n","と表すことができる\n","\n","学習途中では行動価値関数が決定できていない場合は、この式の等式が成り立たず、誤差が生じる\n","- この誤差(右辺-左辺)をTD誤差(Temporal difference error)と呼び\n","$$\n","TD_{error} = r_{t+1}+\\gamma Q(s_{t_1}, a_{t+1})-Q(s_t, a_t)\n","$$\n","と表現される\n","\n","TD誤差が0になれば行動価値関数が求まったことになるが、この$Q$の更新式は学習率$\\eta$とすると次のように表すことができる\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t))\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"T8ZEPdm4ke_T"},"source":["## $\\varepsilon$-greedy法\n","\n","$Q$値が最も大きくなる行動を選択しようとする場合、より良い選択をどのように探索する手法が必要\n","\n","そこで、エピソード初期はあえてランダムに行動を選択し、エピソードを重ねるに従いQ値に基づいて選択する代表的な方法が$\\varepsilon$-greedy法\n","\n","この$\\varepsilon$-greedy法として、例えば\n","- 確率$\\varepsilon$でランダムに行動選択する\n","- 確率$1-\\varepsilon$で$Q$が最大になる行動を選択する\n","\n","などの方法がある"]},{"cell_type":"markdown","metadata":{"id":"bwfi3DgnfHVy"},"source":["## Q学習\n"]},{"cell_type":"markdown","metadata":{"id":"8rpMLNTZdKta"},"source":["### Q学習の概要\n","\n","一般に次のような形態となり、プログラム構造もこれに倣って設計される\n","\n","主な構成要素は次の3つ\n","- Agent (行動を司る実態)\n","- Environment (Agentが行動する環境)\n","- Brain (行動を決定する頭脳)\n","\n","1. 行動: Agentが環境に実際に行動する\n","1. 状態更新・報酬: 環境が変化しそれによって次の状態を伝えると共に報酬を返す\n","1. 状態通知: AgentがBrainに現在の状態・行動・次の状態・報酬等の情報を伝える\n","1. 判断: 状態通知内容をもとに行動決定の施策を練ることで、Qテーブルの更新などが該当する\n","1. 行動指示: BrainがAgentにとるべき行動を指示する"]},{"cell_type":"markdown","metadata":{"id":"werGUiK7VBGv"},"source":["### 迷路の解法におけるQ学習\n","\n","例えば、強化学習で迷路を解くという問題を例に説明する\n","次のような迷路を想定する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze1.png\" width=200>\n","\n","ここでエージェントは迷路を解く人を意味し、環境は迷路である\n","\n","- 行動：まず、迷路を解く上では、行動とはエージェント(人)が迷路を移動することに該当する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze2.png\" width=200>\n","\n","この場合、いけない場所があるかもしれないが、検討する移動方向は上下左右の4つであり、この中から一つの行動を選択する\n","\n","- 状態：迷路を解く上での状態とは、迷路の中でどこにいるかを表し、行動により変化する\n","  - ここではS1からS9の状態をとる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze3.png\" width=200>\n","\n","- 報酬：例えばエージェントがゴールに到達すれば+1の報酬を手に入れ、途中の罠にはまれば-1の報酬を手に入れる\n","  - これで、罠を避けつつゴールに行くようになるであろう\n","  - 最短コースを選択するようにするのであれば、歩数を報酬に入れるとよい\n","  - この例では答えが報酬になってしまっているが、これは必須ではない\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/maze4.png\" width=200>\n","\n","- 方策：状態を考慮してエージェントがどのように行動するべきかを定めたルールであり、あとから出てくるQテーブルが該当する\n","  - この場合のQテーブルは次のようなイメージであり、例えばS1の状態では、右に進めば罠に近づくためマイナスの値となっており、下に進めばゴールに近づくためプラスの値を与える\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Qtable.png\" width=300>\n"]},{"cell_type":"markdown","metadata":{"id":"Yq6eENrPaEch"},"source":["### Q学習の更新式\n","\n","Saraの更新式は\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})-Q(s_t, a_t))\n","$$\n","\n","と与えられ、行動価値関数$Q$の更新に次の行動$a_{t+1}$を利用する\n","- これを方策オン型と呼ぶ\n","\n","Q学習の更新式は\n","\n","$$\n","Q(s_t, a_t) \\leftarrow Q(s_t, a_t)+\\eta \\times (r_{t+1}+\\gamma max_a Q(s_{t+1}, a)-Q(s_t, a_t))\n","$$\n","\n","と与えられ、状態$s_{t+1}$の行動価値関数の値のうち最も大きい場合で更新する\n","- これを方策オフ型と呼ぶ\n","- Q値の更新量 = 学習係数x(**報酬**+割引率x**次の状態で最大のQ値**-現在のQ値)である\n","  - 学習係数$\\eta$は0.1といった値が用いられる\n","- 行動の結果得られた報酬と次の状態で最大のQ値（割り引かれる）から現在のQ値を差し引くことを意味する\n","- $\\varepsilon$-greedy法をのようなランダム性が更新式に含まれないため、収束がSarsaよりも速くなる"]},{"cell_type":"markdown","metadata":{"id":"JorPB2eHpbET"},"source":["## Q学習の例\n","\n","ここでは、深層強化学習ではなく、Qテーブルのみ利用した簡単なQ学習を実装し、機械学習の仕組みについて理解する\n","- 深層ではないため、PyTorchは用いず、NumPyとmatplotlibのみ用いる\n","- 一般にQテーブルは巨大になるためQテーブルの学習にDNNを応用するが、この応用については後述する\n","\n","ここでは、飛べ！飛行船ゲームにトライする\n","- エージェントは飛行船であり、基本的には自由落下するが、ジェット噴射で上向き加速度を与えることができる\n","  - 選択できる行動は上下方向のみで、行動は2つ、自由落下行動0と、ジェット噴射行動1のみ\n","- フィールドを左から右へ等速度で運動する\n","- フィールドの下端は墜落、上端は空気が薄いため、どちらもゲームオーバー\n","\n","このゲームを訓練するが、報酬は飛行船が右端まで到達したときに+1、ゲームオーバーの時-1とする\n"]},{"cell_type":"code","metadata":{"id":"-sw4eAa1k-Ll","executionInfo":{"status":"ok","timestamp":1724735799427,"user_tz":-540,"elapsed":296,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["cuda = \"cuda:0\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation, rc"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8BBKCnfp6r3"},"source":["## Brainクラス\n","まず、エージェントの頭脳となるBrainクラスを設計する\n","\n","より多くの報酬が得られるようにQ-TableのQ値を調整、Q値の更新量は次式で表現可能\n","- `Q値の更新量 = 学習係数 x ( 報酬 + 割引率 x 次の状態で最大のQ値 - 現在のQ値 )  `\n","  \n","Brainクラスの`get_action`メソッドは、ある状態における行動をε-greedy法により選択\n","- 従って、学習初期はランダムに行動が選択され、学習が進むと徐々にQ値の高い行動が選択されるようになる\n"]},{"cell_type":"code","metadata":{"id":"2eLczBSCq12y","executionInfo":{"status":"ok","timestamp":1724735800073,"user_tz":-540,"elapsed":330,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["class Brain:\n","    def __init__(self, n_state, w_y, w_vy, n_action, gamma=0.9, r=0.99, lr=0.01):\n","        self.n_state = n_state  # 状態の数\n","        self.w_y = w_y  # 位置の刻み幅\n","        self.w_vy = w_vy  # 速度の刻み幅\n","        self.n_action = n_action  # 行動の数\n","        self.eps = 1.0  # ε\n","        self.gamma = gamma  # 割引率\n","        self.r = r  # εの減衰率\n","        self.lr = lr  # 学習係数\n","        self.q_table = np.random.rand(n_state*n_state, n_action)  # Q-Tableを乱数で初期化\n","\n","    def quantize(self, state, n_state, w):  # 状態の値を整数のインデックスに変換\n","        min = - n_state / 2 * w\n","        nw = (state - min) / w\n","        nw = int(nw)\n","        nw = 0 if nw < 0 else nw\n","        nw = n_state-1 if nw >= n_state-1 else nw\n","        return nw\n","\n","    def train(self, states, next_states, action, reward, terminal):  # Q-Tableを訓練\n","        i = self.quantize(states[0], self.n_state, self.w_y)  # 位置のインデックス\n","        j = self.quantize(states[1], self.n_state, self.w_vy)  # 速度のインデックス\n","        q = self.q_table[i*self.n_state+j, action]  # 現在のQ値\n","\n","        next_i = self.quantize(next_states[0], self.n_state, self.w_y)  # 次の位置のインデックス\n","        next_j = self.quantize(next_states[1], self.n_state, self.w_vy)  # 次の速度のインデックス\n","        q_next = np.max(self.q_table[next_i*self.n_state+next_j])  # 次の状態で最大のQ値\n","\n","        if terminal:\n","            self.q_table[i*self.n_state+j, action] = q + self.lr*reward  # 終了時は報酬のみ使用\n","        else:\n","            self.q_table[i*self.n_state+j, action] = q + self.lr*(reward + self.gamma*q_next - q)  # Q値の更新式\n","\n","    def get_action(self, states):\n","        if np.random.rand() < self.eps:  # ランダムな行動\n","            action = np.random.randint(self.n_action)\n","        else:  # Q値の高い行動を選択\n","            i = self.quantize(states[0], self.n_state, self.w_y)\n","            j = self.quantize(states[1], self.n_state, self.w_vy)\n","            action = np.argmax(self.q_table[i*self.n_state+j])\n","        if self.eps > 0.1:  # εの下限\n","            self.eps *= self.r\n","        return action"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUxvMa_eq8j_"},"source":["## エージェントのクラス\n","\n","エージェントクラスは、実際に行動し報酬を受け取るように実装\n","\n","- x座標が-1から1、y座標が-1から1の正方形の領域を考え、エージェントの初期位置は左端中央とする\n","- エージェントが右端に達した際は報酬として1を与え終了とする\n","- エージェントが上端もしくは下端に達した際は報酬として-1を与え、終了とする\n","- x軸方向には等速度で移動\n","- 行動は自由落下とジャンプの2種類\n"," - 自由落下の場合は重量加速度をy速度に加える\n"," - ジャンプの場合はy速度を予め設定した値に変更"]},{"cell_type":"code","metadata":{"id":"vODVCQGqrcxx","executionInfo":{"status":"ok","timestamp":1724735800073,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["class Agent:\n","    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n","        self.v_x = v_x  # x速度\n","        self.v_y_sigma = v_y_sigma  # y速度、初期値の標準偏差\n","        self.v_jump = v_jump  # ジャンプ速度\n","        self.brain = brain\n","        self.reset()\n","\n","    def reset(self):\n","        self.x = -1  # 初期x座標\n","        self.y = 0  # 初期y座標\n","        self.v_y = self.v_y_sigma * np.random.randn()  # 初期y速度\n","\n","    def step(self, g):  # 時間を1つ進める g:重力加速度\n","        states = np.array([self.y, self.v_y])\n","        self.x += self.v_x\n","        self.y += self.v_y\n","\n","        reward = 0  # 報酬\n","        terminal = False  # 終了判定\n","        if self.x>1.0:\n","            reward = 1\n","            terminal = True\n","        elif self.y<-1.0 or self.y>1.0:\n","            reward = -1\n","            terminal = True\n","        reward = np.array([reward])\n","\n","        action = self.brain.get_action(states)\n","        if action == 0:\n","            self.v_y -= g   # 自由落下\n","        else:\n","            self.v_y = self.v_jump  # ジャンプ\n","        next_states = np.array([self.y, self.v_y])\n","        self.brain.train(states, next_states, action, reward, terminal)\n","\n","        if terminal:\n","            self.reset()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CiJfy0AHreqM"},"source":["## 環境のクラス\n","\n","環境をクラスとして実装\n","- 役割は重力加速度を設定し時間を前に進めることのみ"]},{"cell_type":"code","metadata":{"id":"nKcZDM98rfm9","executionInfo":{"status":"ok","timestamp":1724735800073,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["class Environment:\n","    def __init__(self, agent, g):\n","        self.agent = agent\n","        self.g = g  # 重力加速度\n","\n","    def step(self):\n","        self.agent.step(self.g)\n","        return (self.agent.x, self.agent.y)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gzn5P5mZrhEK"},"source":["## アニメーション\n","今回は、matplotlibを使ってエージェントの飛行をアニメーションで表します。  \n","アニメーションには、matplotlib.animationのFuncAnimation関数を使用します。  "]},{"cell_type":"code","metadata":{"id":"j_GjTw5Aridm","executionInfo":{"status":"ok","timestamp":1724735800073,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["def animate(environment, interval, frames):\n","    fig, ax = plt.subplots()\n","    plt.close()\n","    ax.set_xlim(( -1, 1))\n","    ax.set_ylim((-1, 1))\n","    sc = ax.scatter([], [])\n","    def plot(data):\n","        x, y = environment.step()\n","        sc.set_offsets(np.array([[x, y]]))\n","        return (sc,)\n","    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrRY0cDArj41"},"source":["## ランダムな行動\n","\n","比較としてエージェントがランダムに行動する例を試す\n","- `r`の値を1に設定しεが減衰しないようにする\n"," - これでエージェントは完全にランダムな行動を選択する\n","\n","実行には、10分程度必要\n","- 結果を再生すると、運良く右端に到達することもあるが多くの場合ゲームオーバーとなることがわかる"]},{"cell_type":"code","metadata":{"id":"iAzf-1IcrlvT","colab":{"base_uri":"https://localhost:8080/","height":701,"output_embedded_package_id":"1saZkDJ6t9_Y-vdDN4i_Kv6zMymzjpKIw"},"executionInfo":{"status":"ok","timestamp":1724735877410,"user_tz":-540,"elapsed":77340,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e3df9995-833a-4bef-d324-a8ab81039b34"},"source":["n_state = 50\n","w_y = 0.2\n","w_vy = 0.2\n","n_action = 2\n","brain = Brain(n_state, w_y, w_vy, n_action, r=1.0)  # εが減衰しない\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc(\"animation\", html=\"jshtml\")\n","anim"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"G5NHaB2SrqCB"},"source":["## Q学習の導入\n","\n","`r`の値を0.99に設定しεが減衰するようにする\n","- Q学習の結果が行動に反映されるようになる"]},{"cell_type":"code","metadata":{"id":"gsOC7SgfrrZM","colab":{"base_uri":"https://localhost:8080/","height":701,"output_embedded_package_id":"1r6GeUdj5nPBmi6yIzvzHDqZtLODzAs47"},"executionInfo":{"status":"ok","timestamp":1724735941780,"user_tz":-540,"elapsed":64374,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"eb64b939-97da-400c-9fc1-0b4a63d7bc4b"},"source":["n_state = 50\n","w_y = 0.2\n","w_vy = 0.2\n","n_action = 2\n","brain = Brain(n_state, w_y, w_vy, n_action, r=0.99)  # εが減衰する\n","\n","v_x = 0.05\n","v_y_sigma = 0.1\n","v_jump = 0.2\n","agent = Agent(v_x, v_y_sigma, v_jump, brain)\n","\n","g = 0.2\n","environment = Environment(agent, g)\n","\n","anim = animate(environment, 50, 1024)\n","rc(\"animation\", html=\"jshtml\")\n","anim"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"_enFs-ndrs7K"},"source":["学習が進むと、上下の端にぶつらずに右端まで飛べるようになる"]},{"cell_type":"markdown","metadata":{"id":"4WktY8P62Cnr"},"source":["## 深層強化学習\n","\n","ディープラーニングは、特に複雑な特徴量入力から自律的に特徴抽出を行うことができるが、強化学習における環境からの複雑な入力から特徴を抽出し、行動を得ることで、自律的特徴抽出と自律制御に応用できる\n","\n","エージェントは環境の知識は事前に持たないため、自ら探索し情報収集することが必要であり、これが強化学習における特徴量抽出に相当する\n","\n","さらに、エージェントは環境を把握した上で長期的な価値を最大化するであろう行動を時系列と各行動の相互の影響を考慮して行動を決定し、これが強化学習における時系列データ生成に相当する\n","\n","この両方に効果的に利用できるのがディープラーニングである\n","- 入力データが画像である場合、特徴量抽出にCNNが用いられ、時系列データである倍RNNが用いられることが多いが、様々な層構造の応用が想定される"]},{"cell_type":"markdown","metadata":{"id":"eZUh7Blktobg"},"source":["# Deep Q-Network(DQN)\n","\n","- すべての状態数と行動数が把握できるのであれば、これをすべて記述してテーブルをこうせいすればよいが、一般に状態数や行動数は大きく、組み合わせの数も膨大となり計算が困難となる\n","- そこでDQNでは$Q$をテーブルではなく、関数や学習モデルで近似する\n","- 構築する深層学習モデルは、状態×行動を入力とせず、状態を入力して行動のQ値を出力するモデルとなる\n"]},{"cell_type":"markdown","metadata":{"id":"tvNp4SBMdTWi"},"source":["## Q-Netowrk\n","\n","DQNにおけるニューラルネットワークの構造をQ-Networkと呼ぶ\n","- 特に深層ニューラルネットワークを用いる場合にDQNと呼ぶ\n"]},{"cell_type":"markdown","metadata":{"id":"EyqAis74dVkm"},"source":["## TD学習とQ-learning\n","\n","ここで扱う手法はTD学習(Temporal Difference learning)と呼ばれる\n","\n","TD学習は強化学習の手法の一つで、価値ベースの手法である\n","- 次の式のように行動価値関数$Q$についてTD誤差を求め、これが0になるようにする\n","- Q-learningは行動価値関数$Q$で価値の高い行動を選択し、状態・報酬の観測を繰り返すことで、次の状態での$Q$の値と現時点での$Q$の値の間に生じるTD誤差を用い最適な行動価値関数を推定する手法\n","\n","$$\n","Q(s_t,a_t){\\leftarrow}(1-\\eta)Q(s_t,a_t)+\\eta(r_{t+1}-\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1}))\n","$$\n","\n","左辺はQ値の更新値、右辺は現状態のQ値$Q(s_t,a_t)$、報酬$r_{t+1}$、行動後の状態のQ値の最大値$\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$を意味する\n","\n","この式を変形すると、\n","\n","$$\n","Q(s_t,a_t){\\leftarrow}Q(s_t,a_t)+\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})-Q(s_t,a_t))\n","$$\n","\n","となり、$\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})-Q(s_t,a_t))$の部分をTD誤差と呼ぶ\n","\n","$Q(s_t,a_t)$を$r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$に近づけるため、後者を教師信号targetとして、現在の$Q$との誤差関数$L(s,a)$を使ってネットワークを学習させる\n","\n","$$\n","target=r_{t+1}+\\gamma\\max_{a_{t+1}}Q(s_{t+1},a_{t+1})\n","$$\n","$$\n","L(s,a)=\\frac{1}{2}(target-Q(s,a))^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"dEu5bilL8rpp"},"source":["## DQNにおけるテクニック\n","\n","DLにおいても様々なテクニックが利用され、その進歩に貢献したように、DQNにおいても次のようなテクニックが提案され、利用されている\n"]},{"cell_type":"markdown","metadata":{"id":"MNUbUYoYd98x"},"source":["### **Experience Replay**\n","\n","エージェントが繰り返し行動し、得られた経験(Experience)を時系列に獲得するが、経験に相関があるとネットワークが過学習するためこれを避ける\n","- 時刻$t$と時刻$t+1$のの学習内容は類似し、類似した部分のみ学習が進行しやすい\n","- 解決のためにミニバッチに類似した方針として、経験をメモリに保存し、そのメモリの中から経験をランダムに選んで(Replay)学習させる\n"]},{"cell_type":"markdown","metadata":{"id":"gbDx1U3heBmp"},"source":["### **Prioritized Experience Replay**\n","\n","Experience Replayについて、ランダムに選択せず、優先順位を付けて選択する\n","- 例えば、教師信号との差を求め、その差が大きい学習内容を優先的に選択するなど\n","\n","具体的にはTD誤差に相当する、次の項\n","$$\n","r_{t+1}+\\gamma Q_t(s_{t+1},a_{t+1})-Q(s_t,a_t)\n","$$\n","\n","の値が大きいtransitionを優先的にExperience Replay時に学習させる\n","\n","実装する際には、値の大きいtransisionを探すために探索問題を解くことになり、例えば2分木探索を行う手法が提案されている\n"]},{"cell_type":"markdown","metadata":{"id":"wjgOFQgPeIgs"},"source":["### **Fixed Target Q-Network**\n","\n","行動を決定するmain-networkと誤差関数の計算時に行動価値を決めるtarget-networkの2種類を用いる\n","- 更新の「時刻差」を用いており、main-networkとtarget-networkが完全に独立しているわけではない\n","- 完全に独立して持たせるのは、次のDDQN\n","\n","その内容は次の通り\n","\n","- DQNでは価値関数$Q(s_t,a)$を更新するが、それには次の時刻の状態$s_{t+1}$での価値関数$Q(S_{t+1}, a)$が必要となり、Q関数の更新のために同じQ関数を利用すると学習が不安定になりやすい\n","\n","- そこで更新に必要な$max_a Q(s_{t+1}, a)$を求める際に、時刻を遡った(ネットワークパラメータの更新タイミングにおいて前の)別のQ関数(Fixed Target Q-network)を用いて計算する\n","  - target-networkは定期的にmain-networkで上書きする\n","- Fixed Target Q-Netowrkは、Freezing the target networkとも呼ばれ、より具体的には、TD誤差の目標値に古い$Q$(target Q-network)を使うことである\n","  - $L_{\\theta}(s_t,a)=\\frac{1}{2}(r_{t+1}+\\gamma\\max_aQ_{\\theta^{-1}}(s_{t+1}, a)-Q_{\\theta}(s_t,a))^2$  \n","  とする\n","  - 一定周期で学習中の$Q_\\theta$のパラメータと同期させるため、次のようにする$(\\theta^{-1}\\leftarrow\\theta)$\n"]},{"cell_type":"markdown","metadata":{"id":"6_JBRgsLeMcy"},"source":["### **DDQN (Double-DQN)**\n","\n","Fixed Target Q-Networkでは同一ネットワークの時刻差を用いるが、DDQNでは完全に独立した2つのネットワークを利用する\n","\n","まず、TD学習でも利用した式を、main Q-Netowrkを$Q_m$、target Q-Netowrkを$Q_t$として明確に分けて記述すると次のようになる\n","\n","$$\n","Q_m(s_t,a_t){\\leftarrow}Q_m(s_t,a_t)+\\eta(r_{t+1}+\\gamma\\max_{a_{t+1}}Q_t(s_{t+1},a_{t+1})-Q_m(s_t,a_t))\n","$$\n","\n","このように、次の状態$s_{t+1}$での$Q$値が最大となる行動$a = a_{t+1}$およびその時の$Q$値の2つをTarget Q-Networkから求める\n","\n","DDQNではこれをさらに安定させるため、次のような更新式を利用する\n","\n","$$\n","a_m = arg \\max_a Q_m(s_{t+1}, a_{t+1})\n","$$\n","\n","$$\n","Q_m(s_t,a_t){\\leftarrow}Q_m(s_t,a_t)+\\eta(r_{t+1}+\\gamma Q_t(s_{t+1},a_m)-Q_m(s_t,a_t))\n","$$\n","\n","このように、次の状態$s_{t+1}$での$Q$値が最大となる行動$a_m$はMain Q-Networkから求め、その行動$a_m$での$Q$値はTarget Q-Netowrkから求める\n","\n","- Main Q-Networkの更新量を2つのネットワークを使用して求めるためDouble DQNと呼ばれる\n"]},{"cell_type":"markdown","metadata":{"id":"N4rx_BBqeQfh"},"source":["### **Huber関数**\n","\n","二乗誤差関数を用いずHuber関数を用いて計算する手法\n","\n","- Huber関数は次の通りで、例えば$\\epsilon=1$として、$-1<x<1$の範囲で$y=\\frac{1}{2}x^2$、それ以外で$y=|x|-\\frac{1}{2}$とすることで、誤差が大きいときに誤差関数の結果が二乗と大きくなるため学習が不安定になる問題を解決\n","\n","$$\n","H(x)=\\left\\{\\begin{array}{ll}x^2/2 & \\text{if }|x|\\le\\epsilon\\\\\\epsilon|x|-\\epsilon^2/2&\\text{otherwise}\\end{array}\\right. |x|\\le\\epsilon\n","$$\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/huber.JPG\" width=\"300\">"]},{"cell_type":"markdown","metadata":{"id":"l5ECvjI0eV5Q"},"source":["### **Clipping Rewards**: 報酬のスケールを、正スコア+1、負スコア−1などと統一する\n","- 結果として学習対象に依存せず、同じハイパーパラメータでDQNを実行できる\n"]},{"cell_type":"markdown","metadata":{"id":"iEvrjMpqebaH"},"source":["### **Dueling Network**\n","\n","行動価値関数$Q(s,a)$の出力層において状態価値$V(s)$とアドバンテージ関数$A(s,a) = Q(s,a)-V(s)$を配置する手法\n","- 数式表現上はこれで正しいが、実際にはネットワーク構造上の表現で理解する方がわかりやすく、次のような層構造となる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/Q2Dueling.png\" width=\"500\">\n","\n","- $V(s)$は行動価値が最大となる行動を採用したときの$Q$値ではなく、**全行動の平均の$Q$値となる**\n","- meanすなわち平均を引くことで、行動の種類により異なるバイアスが加わること排除し、学習を安定化させる\n","  - 誤差逆伝播では、どのような指令でも必ず全ネットワークを更新でき、同程度の重みで指令を生成するようになる\n","  - つまり、とった行動の違いに依存せず状態価値$V(s)$を得ることができるため、学習性能が向上する"]},{"cell_type":"markdown","metadata":{"id":"4mHmSB7BeePD"},"source":["### **A3C**\n","\n","Asynchronous Advantage Actor-Criticの略\n","\n","3つのAから始まる手法を用いることを意味する\n","- Asynchronous: GORILA(General Reinforcement Learning Architecture)で提案されたように、複数のエージェントを用意して分散学習させる手法を利用する\n","- Advantage: Q学習の更新において1ステップ先の状態を使用するが、2ステップ先以上を用いて更新するAdvantage手法を利用する\n","\n","更新式を定式化すると次の通りとなる\n","$$\n","Q(s_t,a_t) = r_{t+1}+\\gamma r_{t+2}+\\gamma^2\\max_a Q(s_{t+2}, a)\n","$$\n","\n","- Actor-Critic: 強化学習における方策反復法と価値反復法の両方を組み合わせた手法を利用する  \n","  Actor側出力: 方策反復法と同じで状態$s_t$に対して行動それぞれがどの程度お勧めかを示す\n","  - これをsoftmax関数に入力すれば行動の採用確率のように表現できる\n","  - Actor学習に次の方策のエントロピー項が加えられることが多い\n","$$Actor_{entropy} = \\sum^a[\\pi_\\theta(a|s)log \\pi_\\theta(a|s)]\n","$$\n","  ここで、行動の種類について総和を計算しており、方策が行動をランダムに選択する学習初期では最大の値となり、どれか一つの行動しか選択しない場合最小となるように設計されており、このエントロピーをActorの誤差関数から引くことで学習初期はパラメータ学習がゆっくりと進み、局所回に落ちるのを避ける狙いがある\n","\n","  Critic側出力: 状態の価値$V^\\pi_{s_t}$であり、状態$s_t$になった場合にその先得られるであろう割引報酬和の期待値となる\n","    - この割引報酬和$J(\\theta, S_t)$は、$\\mathbb{E}$をミニバッチ平均で期待値を表すとすると\n","$$\n","J(\\theta, S_t) = \\mathbb{E}[log \\pi_\\theta(a|s)(Q^\\pi(s,a)-V^\\pi_s)]\n","$$\n","となる\n","    - $log \\pi_\\theta(a|s)$は状態$s$の時に行動$a$を選択する条件付確率の$log$を意味する\n","    - $Q^\\pi(s,a)$は状態$s$で行動$a$を選択する場合の行動価値を表し、行動$a$に対する定数であり、Advantage学習で求める  \n","  状態価値$V^\\pi_s$を正しく出力できるように学習させたいため、実際に行動して得られる行動価値$Q^\\pi(s,a)$と出力$V^{\\pi}_{s}$が一致するようにすることから、損失関数は$Loss_{critic}=(Q^\\pi(s,a)-V^\\pi_s)^2$とする\n"]},{"cell_type":"markdown","metadata":{"id":"SxfEf6zBehXE"},"source":["### **UNREAL**\n","\n","UNsupervised Reinforcement and Auxiliary Learningの略\n","\n","- 最終目的となる課題(環境)を用いて学習を完結させるのではなく、その傍題となるより簡易な補助問題を用いて学習させ、この補助問題を徐々に高度化させることで最終的に本来の問題に対応させる手法\n","- 要するにいきなり難しい問題に取り組むのではなく、徐々に難しくする手法、この問題を考えて与えるのは人間\n","\n","**Skipping frames**: 計算コストを削減するという観点から毎フレーム行動選択を行わず、数フレームおきに行動選択する\n"]},{"cell_type":"markdown","metadata":{"id":"EtA5HQYsGAKS"},"source":["# OpenAI Gymによる強化学習環境設計とDQNによる強化学習\n","\n","〇×ゲーム(英語でTicTacToe)程度であれば一から構成可能であるが、今後の拡張を考えて非営利団体 OpenAIが提供する強化学習の開発・評価用のプラットフォームOpenAI Gymを利用する\n","\n","強化学習は、与えられた環境(Environment)の中で、エージェントが試行錯誤しながら報酬を最大化する行動を学習する機械学習アルゴリズムである\n","- エージェントの学習アルゴリズムに加えて環境も重要な要素となることから、強化学習用の環境を共通インターフェイスを提供する\n","- 実験結果をアップロードし他のアルゴリズムと比較することができる\n","- 完全に準備されていればそれを用いるが、準備されていない場合でも独自環境を構築することで上記メリットをえることができる\n","\n","まず最初に必要なモジュールを読み込む"]},{"cell_type":"code","metadata":{"id":"pjPyg25JR8Zn","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":8,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["#import click\n","#import sys\n","import numpy as np\n","import random\n","import gym\n","from gym import spaces\n","from collections import namedtuple\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nIsVICHzJmhY"},"source":["## 環境構築\n","\n","gym.Envを継承し、次の関数を準備したのち、`gym.envs.registration.register`関数を用いてgymに登録することで環境が利用可能となる\n","\n","次のメソッドを揃えたクラスを設計する\n","- `_step(self, action)`: action を実行し、その結果を返すメソッド(必須)\n","- `_reset(self)`: 環境の状態を初期化し最初の観測値を返すメソッド(必須)\n","- `_render(self, mode='human', close=False)`: 環境を可視化するメソッド(必須)、の引数の`mode`はclass毎に受け取れる任意の文字列を指定し、慣習として以下を列挙する\n","  - `human`: 人が操作できるように画面表示を行うこと意味し、戻り値はない  \n","  特に実際のゲーム画面を利用する場合は、画像表示に`gym.envs.classic_control.rendering.SimpleImageViewer`が利用でき、`rgb_array`で画面の画像情報をnumpy.array型(x, y, 3)のRGBピクセル配列で返し、`ansi`では文字列もしくはStringIOで返す\n","- `_close(self)`:\t環境を閉じて後処理をするメソッド(必須ではない)\n","- `_seed(self, seed=None)`: 乱数にシードを与えるメソッド(必須ではない)\n","\n","また、次のメンバ変数をプロパティとして与える\n","- `action_space`: Actionが及ぶ空間(入力)\n","- `observation_space`: Observationが及ぶ空間(出力)\n","- `reward_range`: 報酬の最小値と最大値のリスト\n"]},{"cell_type":"markdown","metadata":{"id":"NNySpd_bCFZ3"},"source":["## 状態空間と行動空間の型\n","\n","OpenAI Gymの空間として、**Box**(連続値)と**Discrete**(離散値)があり、状態空間は多くがBoxで表現され、行動空間はDiscreteの方がより学習が容易となる\n","- Box: 範囲[low、high]の連続値、Float型のn次元配列  \n","`gym.spaces.Box(low=-100, high=100, shape=(2,))`\n","\n","- Discrete: 範囲[0、n-1]の離散値、Int型の数値  \n","`gym.spaces.Discrete(4)`\n","\n","- MultiBinary: ステップ毎に任意の行動を任意の組み合わせで使用できる行動リスト  \n","`gym.spaces.MultiBinary(5)`\n","\n","- MultiDiscrete: ステップ毎に各離散セットの1つの行動のみ使用できる行動リスト  \n","`gym.spaces.MultiDiscrete([-10,10], [0,1])`\n"]},{"cell_type":"markdown","metadata":{"id":"ByxlZxsXBVx6"},"source":["## 実際の環境の宣言\n"]},{"cell_type":"markdown","metadata":{"id":"qwqanT0PdeNn"},"source":["### プロパティの宣言\n","- `reward range` で報酬の値の範囲を設定、ここでは制限なし\n","- `observation_space`で盤面の入力を定義\n","  - 0が駒無し、1がCOMの駒、2がPLAYERの駒であり3種類、よって、9x9の盤面で、ワンホットになっている\n","- `action_space`は、どこに駒を置くかを指定するため9個のワンホットで指定する\n","- `winning_streaks`は全ての勝ちパターンであり、ここに並べた駒の位置のいずれかで3つ並びが完成する\n"]},{"cell_type":"markdown","metadata":{"id":"M2cT3rh7dg34"},"source":["### `__init__`\n","- `summary`にゲームの結果を保存するためその初期化を行う\n","- 一般的にはここで`_reset`を呼び出す場合が多いが、この場合は不要である\n"]},{"cell_type":"markdown","metadata":{"id":"i96fw9p2djJy"},"source":["### `one_hot_board`\n","- `board`には盤面情報が保存されており、0が空白、1が注目プレイヤーで、2が相手プレイヤー1である(例えば`[1, 0, 0, 0, 2, 1, 0, 2, 1]`)\n","- この0,1,2をワンホットに変換するためにnp.eye(3)で3x3単位行列を作り、その第i行目を取得、これをブロードキャストし例えば\n","```\n","[[0, 1, 0]\n"," [1, 0, 0]\n"," [1, 0, 0]\n","\n"," [1, 0, 0]\n"," [0, 0, 1]\n"," [0, 1, 0]\n","\n"," [1, 0, 0]\n"," [0, 0, 1]\n"," [0, 1, 0]]\n"," ```\n","  とする\n","\n","- さらに`reshape(-1)`で1次元配列に変換している\n","- プレイヤーが0つまりCOMか、1つまりHumanかで、注目プレーヤーを変更する。COMの場合はそのままでよいが、1の場合は上記の1と2を入れ替える必要がある(例えば、`[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]`)\n","  - 実際に学習に用いるのはこのワンホットのボード情報で、9x3の27個の数列で構成される\n","  - `observation_space`の宣言と一致する"]},{"cell_type":"markdown","metadata":{"id":"CMUyeBQrdml6"},"source":["### `_reset`\n","まずプレーヤーを0つまりCOMとし、boardを全て空白に初期化、one_hot_boardを生成する\n","- boardを変更したら必ずone_hot_boardを更新する\n"]},{"cell_type":"markdown","metadata":{"id":"nbLgbGPBdoOT"},"source":["### `_step`\n","引数actionsで、取られた行動を取得する\n","- これは、学習中はNNの出力であったり乱数であったりし、Humanであればインタラクティブな入力の値となる\n","- 次に報酬計算を行う\n","  - 置いてはいけない場所に置いた場合-10ポイントとする\n","    - これはboardが0ではないことを確認すればよい\n","  - 置けるので、おいてしまう\n","  - 次のステップで相手が勝てるかどうかを3つ並び条件それぞれで調べて(for)、相手が勝つ場合-2ポイントとする\n","    - ブロードキャストを用いて巧妙に描かれているが、`2 - self.current_playe`は相手を表し、`self.board[streak]`は3つ並ぶパターンに入っている駒をブロードキャストで全て取り出しつつ、それが相手に一致すると1、一致しない場合は0であることから、一致数が2以上となり、残りが0つまり空白であれば相手が勝つことになる\n","    - この残りが0であることを調べるために、`(self.board[streak] == 0).any()`とし、anyつまりいずれか1つが0となることを確認している\n","  - 勝利していれば+1ポイント\n","    - `self.board[streak] == self.current_player + 1`はもうわかるであろう\n","  - 引き分けの場合は0ポイントとする\n","    - `(self.board != 0).all()`ももうわかるであろう\n","- プレーヤーを0(COM)でああれば1(Human)に、1であれば0にする\n","- 最後にワンホットのボートの状態と、報酬、結果を返す"]},{"cell_type":"code","metadata":{"id":"89bYsOmKR29x","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":8,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"89501629-b972-4a56-e3c9-7a09d36ad290"},"source":["class TicTacToe(gym.Env):\n","    reward_range = (-np.inf, np.inf)\n","    observation_space = spaces.MultiDiscrete([2 for _ in range(0, 9 * 3)])\n","    action_space = spaces.Discrete(9)\n","    winning_streaks = [\n","        [0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8],\n","        [0, 3, 6],\n","        [1, 4, 7],\n","        [2, 5, 8],\n","        [0, 4, 8],\n","        [2, 4, 6],\n","    ]\n","    def __init__(self, summary: dict = None):\n","        super().__init__()\n","        if summary is None:\n","            summary = {\n","                \"total games\": 0,\n","                \"ties\": 0,\n","                \"illegal moves\": 0,\n","                \"player 0 wins\": 0,\n","                \"player 1 wins\": 0,\n","            }\n","        self.summary = summary\n","\n","    def _seed(self, seed=None):\n","        pass\n","\n","    def one_hot_board(self):\n","        if self.current_player == 0:\n","            return np.eye(3)[self.board].reshape(-1)\n","        if self.current_player == 1:\n","            # permute for symmetry\n","            return np.eye(3)[self.board][:, [0, 2, 1]].reshape(-1)\n","\n","    def _reset(self):\n","        self.current_player = 0\n","        self.board = np.zeros(9, dtype=\"int\")\n","        return self.one_hot_board()\n","\n","    def _step(self, actions):\n","        exp = {\"state\": \"in progress\"}\n","        # get the current player's action\n","        action = actions\n","        reward = 0\n","        done = False\n","        # illegal move\n","        if self.board[action] != 0:\n","            reward = -10  # illegal moves are really bad\n","            exp = {\"state\": \"done\", \"reason\": \"Illegal move\"}\n","            done = True\n","            self.summary[\"total games\"] += 1\n","            self.summary[\"illegal moves\"] += 1\n","            return self.one_hot_board(), reward, done, exp\n","        self.board[action] = self.current_player + 1\n","        # check if the other player can win on the next turn:\n","        for streak in self.winning_streaks:\n","            if ((self.board[streak] == 2 - self.current_player).sum() >= 2) and \\\n","                (self.board[streak] == 0).any():\n","                reward = -2\n","                exp = {\n","                    \"state\": \"in progress\",\n","                    \"reason\": \"Player {} can lose on the next turn\".format(\n","                        self.current_player\n","                    ),\n","                }\n","        # check if we won\n","        for streak in self.winning_streaks:\n","            if (self.board[streak] == self.current_player + 1).all():\n","                reward = 1  # player wins!\n","                exp = {\n","                    \"state\": \"in progress\",\n","                    \"reason\": \"Player {} has won\".format(self.current_player),\n","                }\n","                self.summary[\"total games\"] += 1\n","                self.summary[\"player {} wins\".format(self.current_player)] += 1\n","                done = True\n","        # check if we tied, which ends the game\n","        if (self.board != 0).all():\n","            reward = 0\n","            exp = {\n","                \"state\": \"in progress\",\n","                \"reason\": \"Player {} has tied\".format(self.current_player),\n","            }\n","            done = True\n","            self.summary[\"total games\"] += 1\n","            self.summary[\"ties\"] += 1\n","        # move to the next player\n","        self.current_player = 1 - self.current_player\n","        return self.one_hot_board(), reward, done, exp\n","\n","    def _render(self, mode: str = \"human\"):\n","        print(\"{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\".format(*self.board.tolist()))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","metadata":{"id":"mYwhJBOYfjIw"},"source":["DQNでは訓練にエクスペリエンス・リプレイ・メモリを利用する\n","- エージェントが観測した遷移を記憶し、後で再利用できるようにする\n","- ランダムに並び替えてバッチで利用される\n","  - これにより、DQNの学習手順が安定化する\n","- ここで、次の2つのクラスを用いて遷移を表現する\n","  - Transition: 記憶の一つの要素であり、ある一つの遷移を表す名前付きタプル\n","    - 基本的には状態とアクションのペアをnext_stateとrewardの結果にマッピングする\n","  - ReplayMemory: 最近のTransitionを記憶するメモリで、サイズが制限されたサイクリックバッファとして実装される\n","    - 学習用のランダムなバッチを構成するため`sample()`メソッドが用意される\n","\n","なお、Transitionは、最初に`Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))`と宣言されており、このnamedtupleについては理解を深める必要がある\n","\n","namedtupleはイミュータブル(変更不能)な組込データ型のtupleの拡張である\n","- tupleはシンプルなデータ型で、任意オブジェクトをグループ化することができる\n","- しかしながら、これでは要素番号でしかアクセスできないため、名前でアクセスできるようにしたのがnamedtupleである\n","- コメントの行も含め3つの書き方を示しているがすべて同じ意味\n","  - 2つ目は`from typing import NamedTuple`が必要であるが、現在は不要である\n","- アンパック代入(a, b = c)や、可変長引数(*)も利用できる\n","- namedtupleクラスにメソッドやプロパティを追加でき、継承も可能\n","\n","このnamedtupleオブジェクトは、内部的に、pythonの通常のクラスとして実装されておりメモリ効率がよい\n","- namedtupleはメモリ効率がよいイミュータブルなクラスであり、クラス定義のショートカットとなる\n","\n","ここでは、イミュータブルなstate, action, next_state, rewardのプロパティを持つクラスTransactionを定義しているかのように考えればよい\n","\n","なお、この部分の記述はPyTorchのTutorialと同一でありself.positionがサイクリックにアクセスするように設定されている"]},{"cell_type":"code","metadata":{"id":"2G3vIYWrflT1","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":7,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["#Transition = namedtuple(\"Transition\", ['state', 'action', 'next_state'. 'reward'])\n","#Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n","Transition = namedtuple(\"Transition\", \"state action next_state reward\")\n","\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dpWZi997jmg"},"source":["## ポリシーのモデル\n","\n","RNNを用いてもよいが、明確な周期性があるとは言い切れないため、全結合網で表現している\n","\n","`self.forward(state).max(1)[1].view(1, 1)`について、まず3x3x3のワンホット入力に対して、3x3の行動を求め、その中で最も大きな値を持つ、最も望ましい行動の候補を選び、maxがvalues(値)とindices(場所)の情報を返すため、場所の情報[1]だけ取り出し、これをバッチも考えて2重配列に変更している"]},{"cell_type":"code","metadata":{"id":"_51GRXzQApIV","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":7,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["class Policy(nn.Module):\n","    def __init__(self, n_inputs=3 * 9, n_outputs=9):\n","        super(Policy, self).__init__()\n","        self.fc1 = nn.Linear(n_inputs, 128)\n","        self.fc2 = nn.Linear(128, 256)\n","        self.fc3 = nn.Linear(256, 128)\n","        self.fc4 = nn.Linear(128, n_outputs)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.relu(self.fc4(x))\n","        return x\n","\n","    def act(self, state):\n","        with torch.no_grad():\n","            return self.forward(state).max(1)[1].view(1, 1)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkpF1h5m96zi"},"source":["## 訓練によるモデル最適化\n","\n","こちらもPyTorchのチュートリアルそのままである\n","\n","引数は次の通り\n","\n","- device (torch.device): cpuかgpuか\n","- optimizer (torch.optim.Optimizer): Optimizerの指定\n","- policy (Policy): Policy用モデルの指定\n","- target (Policy): Target用モデルの指定\n","- memory (ReplayMemory) -- Replay記憶領域\n","- batch_size (int): バッチサイズの指定\n","- gamma (float): 報酬の時間割引率\n","\n","その処理内容は次の通り\n","\n","1. まずバッチをサンプリングする\n","2. またトリッキーだが、  \n","`original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]`  \n","この形を  \n","`result = (['a', 'b', 'c', 'd'], [1, 2, 3, 4])`  \n","この形に変換する技として、`zip(*original)`がある"]},{"cell_type":"code","metadata":{"id":"36oy9uDPsYpW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":7,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e3c4f576-502b-4eb7-b7b9-f0ae48c55509"},"source":["original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n","([ a for a,b in original ], [ b for a,b in original ])"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['a', 'b', 'c', 'd'], [1, 2, 3, 4])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"dtIvUy4WsjzM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":6,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"fba58ae0-771b-4d3d-9305-528195d8a91a"},"source":["tuple([list(tup) for tup in zip(*original)])"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['a', 'b', 'c', 'd'], [1, 2, 3, 4])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"or6C8_OYwHXM"},"source":["\n","  - このテクニックを使って、ミニバッチサイズ(ここでは128)個のboardを同時に管理して手を進めていくが、中には勝負がついている(終了状態)のboardと、そうでないboardが混在する\n","- 計算するのは勝負がついていないboradだけでよいので、終了しているboradを除くためのTrueとFalseのテーブル(マスク)を作成する\n","  - `tuple(map(lambda s: s is not None, batch.next_state))`では、batch.next_stateで勝負がついている場合None、ついていない場合はboard情報が格納されている\n","  - 's is not None'であるため、バッチ全体について勝負がついている場合はFalse、ついていない場合はTrueのタプルが取得される\n","  - `torch.cat([s for s in batch.next_state if s is not None])`では、同様にTrue/Falseを1/0で表現する\n","\n","3. $Q(s_t, a)$を計算する\n","\n","- `policy(state_batch).gather(1, action_batch)`のgatherは、PyTorchが提供するデータ列選択手法である\n","  - dimension=1であるとき、[1, 2, 3]に対して[1]は、0オリジンで1番目、つまり[2]を選択し、これをバッチ全体で行う、つまりここでは、次にとる選択肢を順に選び出していることになる\n","\n","  - 例えば、[[10 11][12 13]]に対して[[0 0][1 0]]とインデキシングした場合、最初は[0 0]つまり、[10 11]に対して0列目を選び出すので、[10 10]となり、次の[1 0]は[12 13]に対して1列目、0列目と選択するので[13 12]となり、最終的に[[10 10][13 12]]となる\n","\n","- モデルは$Q(s_t)$を計算し、次に、取られたアクションの列を選択する。これらは、policy_netに従って各バッチ状態に対して取られたであろうアクションを選択することになる\n","\n","4. ミニバッチの全ての要素について、次の状態$てV(s_{t+1})$を計算する\n","  - 4-1でミニバッチ分確保し初期化する\n","  - 4-2で実際に行動を選択するが、non_final_next_statesの行動の期待値を過去のtarget_netに基づいて計算し、max(1)[0]で最も高い報酬を選択する\n","  - この時勝負が終了しているかどうかのマスクに基づいて値が代入される\n","\n","5. Q値の期待値を求める\n","- 時間割引率$\\gamma$を掛けて、その行動による報酬を加える\n","\n","6. Hubarロスを計算する\n","- F.smooth_l1_lossを利用している\n","\n","7. パラメータが大きくなりすぎないように$-1<p<1$でクランプする\n"]},{"cell_type":"code","metadata":{"id":"gGdAmWxqAeoY","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":5,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["def optimize_model(\n","    device: torch.device,\n","    optimizer: optim.Optimizer,\n","    policy: Policy,\n","    target: Policy,\n","    memory: ReplayMemory,\n","    batch_size: int,\n","    gamma: float):\n","\n","    if len(memory) < batch_size:\n","        return\n","    transitions = memory.sample(batch_size) #(1)\n","    batch = Transition(*zip(*transitions)) #(2)\n","    non_final_mask = torch.tensor(\n","        tuple(map(lambda s: s is not None, batch.next_state)),\n","        device=device,\n","        dtype=torch.bool,\n","    )\n","    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","    state_action_values = policy(state_batch).gather(1, action_batch) #(3)\n","    next_state_values = torch.zeros(batch_size, device=device) #(4-1)\n","    next_state_values[non_final_mask] = target(non_final_next_states).max(1)[0].detach() #(4-2)\n","    expected_state_action_values = (next_state_values * gamma) + reward_batch #(5)\n","    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) #(6)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    for param in policy.parameters():\n","        param.grad.data.clamp_(-1, 1) #(7)\n","    optimizer.step()"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcKAq5YTfFjv"},"source":["これを実行することでわかってもらえると思うが、TicTacToeでも1時間程度学習する\n","- ブロック崩しやマリオのゲームを強化学習で解くといっているが、並列でゲーム盤面を進めることも難しく、学習が極めて困難であることがわかるであろう\n","- 強化学習はエグイ、膨大な経験に基づいて、あらゆる手法の良し悪しを判断しつつ簡単に学ぶ人間は本当にすごい"]},{"cell_type":"markdown","metadata":{"id":"VQ94GzTQgUqf"},"source":["乱択するための関数、でたらめな行動をとる"]},{"cell_type":"code","metadata":{"id":"zrzJGXfhAjQV","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["def select_dummy_action(state: np.array):\n","    state = state.reshape(3, 3, 3)\n","    open_spots = state[:, :, 0].reshape(-1)\n","    p = open_spots / open_spots.sum()\n","    return np.random.choice(np.arange(9), p=p)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bhAkXKnMg4_s"},"source":["行動の選択を行う関数を定義する\n","\n","引数は次の通り\n","- device (torch.device): cpuかgpuか\n","- model (Policy): モデルの指定\n","- state (torch.tensor): 現在のboard(盤面)の状態\n","- eps (float): 乱択する確率\n","\n","戻り値(torch.tensor, bool)のタプルで、行動と行動が乱宅であるかどうかのブール値である\n","\n","処理内容は、乱数がepsの値よりも大きければモデルの出力を、それ以外であれば乱択する"]},{"cell_type":"code","metadata":{"id":"2btrrvx3AmQz","executionInfo":{"status":"ok","timestamp":1724735941781,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"source":["def select_model_action(\n","    device: torch.device, model: Policy, state: torch.tensor, eps: float):\n","    sample = random.random()\n","    if sample > eps:\n","        return model.act(state), False\n","    else:\n","        return (\n","            torch.tensor(\n","                [[random.randrange(0, 9)]],\n","                device=device,\n","                dtype=torch.long,\n","            ),\n","            True,\n","        )"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XuNQ8CzovhO"},"source":["実際の学習処理を行う\n","\n","- 最初は全て乱数、その後確率10%で乱択するまでeps_steps毎徐々に確率を減らしていく\n","- ここでは、Fixed Target Q-Networkを利用しており、同一ネットワークでtargetとpolicyの2つのモデルを準備するが、target_update毎にtargetは既に学習を済ませた過去のpolicyのパラメタを読み出している\n","\n","実行には、40分程度必要"]},{"cell_type":"code","metadata":{"id":"-aqwrlgIgPq0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724737557618,"user_tz":-540,"elapsed":1615841,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f43a12d8-1166-44d5-a768-3a34629353b3"},"source":["n_steps: int = 500000\n","batch_size: int = 128\n","gamma: float = 0.99\n","eps_start: float = 1.0\n","eps_end: float = 0.1\n","eps_steps: int = 200000\n","device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","print(\"Beginning training on: {}\".format(device))\n","target_update = int((1e-2) * n_steps)\n","policy = Policy(n_inputs=3 * 9, n_outputs=9).to(device)\n","target = Policy(n_inputs=3 * 9, n_outputs=9).to(device)\n","target.load_state_dict(policy.state_dict())\n","target.eval()\n","\n","optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n","memory = ReplayMemory(50_000)\n","\n","env = TicTacToe()\n","state = torch.tensor([env._reset()], dtype=torch.float).to(device)\n","old_summary = {\n","    \"total games\": 0,\n","    \"ties\": 0,\n","    \"illegal moves\": 0,\n","    \"player 0 wins\": 0,\n","    \"player 1 wins\": 0,\n","}\n","_randoms = 0\n","summaries = []\n","\n","for step in range(n_steps):\n","    t = np.clip(step / eps_steps, 0, 1)\n","    eps = (1 - t) * eps_start + t * eps_end\n","\n","    action, was_random = select_model_action(device, policy, state, eps)\n","    if was_random:\n","        _randoms += 1\n","    next_state, reward, done, _ = env._step(action.item())\n","    # player 2 goes\n","    if not done:\n","        next_state, _, done, _ = env._step(select_dummy_action(next_state))\n","        next_state = torch.tensor([next_state], dtype=torch.float).to(device)\n","    if done:\n","        next_state = None\n","    memory.push(state, action, next_state, torch.tensor([reward], device=device))\n","    state = next_state\n","    optimize_model(\n","        device=device,\n","        optimizer=optimizer,\n","        policy=policy,\n","        target=target,\n","        memory=memory,\n","        batch_size=batch_size,\n","        gamma=gamma)\n","    if done:\n","        state = torch.tensor([env._reset()], dtype=torch.float).to(device)\n","    if step % target_update == 0:\n","        target.load_state_dict(policy.state_dict())\n","    if step % 5000 == 0:\n","        delta_summary = {k: env.summary[k] - old_summary[k] for k in env.summary}\n","        delta_summary[\"random actions\"] = _randoms\n","        old_summary = {k: env.summary[k] for k in env.summary}\n","        print(\"{} : {}\".format(step, delta_summary))\n","        summaries.append(delta_summary)\n","        _randoms = 0\n","print(\"Complete\")\n","torch.save(policy.state_dict(), 'tictactoe_model.pt')"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning training on: cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-724ceda7e467>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  state = torch.tensor([env._reset()], dtype=torch.float).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["0 : {'total games': 0, 'ties': 0, 'illegal moves': 0, 'player 0 wins': 0, 'player 1 wins': 0, 'random actions': 1}\n","5000 : {'total games': 1571, 'ties': 15, 'illegal moves': 1333, 'player 0 wins': 112, 'player 1 wins': 111, 'random actions': 4951}\n","10000 : {'total games': 1578, 'ties': 8, 'illegal moves': 1325, 'player 0 wins': 150, 'player 1 wins': 95, 'random actions': 4862}\n","15000 : {'total games': 1579, 'ties': 9, 'illegal moves': 1307, 'player 0 wins': 154, 'player 1 wins': 109, 'random actions': 4746}\n","20000 : {'total games': 1556, 'ties': 5, 'illegal moves': 1270, 'player 0 wins': 162, 'player 1 wins': 119, 'random actions': 4609}\n","25000 : {'total games': 1560, 'ties': 14, 'illegal moves': 1243, 'player 0 wins': 202, 'player 1 wins': 101, 'random actions': 4491}\n","30000 : {'total games': 1581, 'ties': 6, 'illegal moves': 1273, 'player 0 wins': 195, 'player 1 wins': 107, 'random actions': 4371}\n","35000 : {'total games': 1560, 'ties': 12, 'illegal moves': 1213, 'player 0 wins': 244, 'player 1 wins': 91, 'random actions': 4268}\n","40000 : {'total games': 1547, 'ties': 8, 'illegal moves': 1229, 'player 0 wins': 223, 'player 1 wins': 87, 'random actions': 4142}\n","45000 : {'total games': 1570, 'ties': 14, 'illegal moves': 1208, 'player 0 wins': 257, 'player 1 wins': 91, 'random actions': 3998}\n","50000 : {'total games': 1548, 'ties': 11, 'illegal moves': 1182, 'player 0 wins': 288, 'player 1 wins': 67, 'random actions': 3897}\n","55000 : {'total games': 1545, 'ties': 10, 'illegal moves': 1160, 'player 0 wins': 291, 'player 1 wins': 84, 'random actions': 3818}\n","60000 : {'total games': 1534, 'ties': 11, 'illegal moves': 1071, 'player 0 wins': 373, 'player 1 wins': 79, 'random actions': 3688}\n","65000 : {'total games': 1525, 'ties': 11, 'illegal moves': 1039, 'player 0 wins': 395, 'player 1 wins': 80, 'random actions': 3568}\n","70000 : {'total games': 1521, 'ties': 10, 'illegal moves': 1019, 'player 0 wins': 412, 'player 1 wins': 80, 'random actions': 3477}\n","75000 : {'total games': 1515, 'ties': 11, 'illegal moves': 1055, 'player 0 wins': 380, 'player 1 wins': 69, 'random actions': 3363}\n","80000 : {'total games': 1511, 'ties': 9, 'illegal moves': 963, 'player 0 wins': 466, 'player 1 wins': 73, 'random actions': 3183}\n","85000 : {'total games': 1500, 'ties': 7, 'illegal moves': 918, 'player 0 wins': 507, 'player 1 wins': 68, 'random actions': 3153}\n","90000 : {'total games': 1484, 'ties': 7, 'illegal moves': 934, 'player 0 wins': 473, 'player 1 wins': 70, 'random actions': 3029}\n","95000 : {'total games': 1476, 'ties': 3, 'illegal moves': 897, 'player 0 wins': 504, 'player 1 wins': 72, 'random actions': 2930}\n","100000 : {'total games': 1486, 'ties': 8, 'illegal moves': 860, 'player 0 wins': 538, 'player 1 wins': 80, 'random actions': 2837}\n","105000 : {'total games': 1479, 'ties': 7, 'illegal moves': 815, 'player 0 wins': 587, 'player 1 wins': 70, 'random actions': 2750}\n","110000 : {'total games': 1453, 'ties': 6, 'illegal moves': 779, 'player 0 wins': 608, 'player 1 wins': 60, 'random actions': 2565}\n","115000 : {'total games': 1478, 'ties': 11, 'illegal moves': 733, 'player 0 wins': 678, 'player 1 wins': 56, 'random actions': 2440}\n","120000 : {'total games': 1438, 'ties': 3, 'illegal moves': 710, 'player 0 wins': 670, 'player 1 wins': 55, 'random actions': 2304}\n","125000 : {'total games': 1429, 'ties': 4, 'illegal moves': 722, 'player 0 wins': 659, 'player 1 wins': 44, 'random actions': 2241}\n","130000 : {'total games': 1434, 'ties': 6, 'illegal moves': 694, 'player 0 wins': 687, 'player 1 wins': 47, 'random actions': 2117}\n","135000 : {'total games': 1446, 'ties': 11, 'illegal moves': 637, 'player 0 wins': 751, 'player 1 wins': 47, 'random actions': 1993}\n","140000 : {'total games': 1441, 'ties': 8, 'illegal moves': 582, 'player 0 wins': 802, 'player 1 wins': 49, 'random actions': 1915}\n","145000 : {'total games': 1423, 'ties': 3, 'illegal moves': 644, 'player 0 wins': 729, 'player 1 wins': 47, 'random actions': 1807}\n","150000 : {'total games': 1424, 'ties': 7, 'illegal moves': 579, 'player 0 wins': 793, 'player 1 wins': 45, 'random actions': 1708}\n","155000 : {'total games': 1445, 'ties': 9, 'illegal moves': 542, 'player 0 wins': 856, 'player 1 wins': 38, 'random actions': 1597}\n","160000 : {'total games': 1439, 'ties': 3, 'illegal moves': 502, 'player 0 wins': 913, 'player 1 wins': 21, 'random actions': 1433}\n","165000 : {'total games': 1423, 'ties': 6, 'illegal moves': 447, 'player 0 wins': 926, 'player 1 wins': 44, 'random actions': 1340}\n","170000 : {'total games': 1411, 'ties': 3, 'illegal moves': 428, 'player 0 wins': 953, 'player 1 wins': 27, 'random actions': 1208}\n","175000 : {'total games': 1404, 'ties': 4, 'illegal moves': 353, 'player 0 wins': 1016, 'player 1 wins': 31, 'random actions': 1091}\n","180000 : {'total games': 1416, 'ties': 5, 'illegal moves': 385, 'player 0 wins': 997, 'player 1 wins': 29, 'random actions': 1018}\n","185000 : {'total games': 1409, 'ties': 2, 'illegal moves': 352, 'player 0 wins': 1037, 'player 1 wins': 18, 'random actions': 896}\n","190000 : {'total games': 1393, 'ties': 4, 'illegal moves': 310, 'player 0 wins': 1060, 'player 1 wins': 19, 'random actions': 823}\n","195000 : {'total games': 1383, 'ties': 2, 'illegal moves': 249, 'player 0 wins': 1102, 'player 1 wins': 30, 'random actions': 643}\n","200000 : {'total games': 1365, 'ties': 5, 'illegal moves': 240, 'player 0 wins': 1100, 'player 1 wins': 20, 'random actions': 553}\n","205000 : {'total games': 1375, 'ties': 1, 'illegal moves': 227, 'player 0 wins': 1131, 'player 1 wins': 16, 'random actions': 509}\n","210000 : {'total games': 1363, 'ties': 1, 'illegal moves': 235, 'player 0 wins': 1101, 'player 1 wins': 26, 'random actions': 515}\n","215000 : {'total games': 1353, 'ties': 1, 'illegal moves': 209, 'player 0 wins': 1124, 'player 1 wins': 19, 'random actions': 484}\n","220000 : {'total games': 1383, 'ties': 3, 'illegal moves': 224, 'player 0 wins': 1132, 'player 1 wins': 24, 'random actions': 550}\n","225000 : {'total games': 1375, 'ties': 3, 'illegal moves': 263, 'player 0 wins': 1084, 'player 1 wins': 25, 'random actions': 523}\n","230000 : {'total games': 1380, 'ties': 2, 'illegal moves': 219, 'player 0 wins': 1139, 'player 1 wins': 20, 'random actions': 525}\n","235000 : {'total games': 1378, 'ties': 6, 'illegal moves': 224, 'player 0 wins': 1114, 'player 1 wins': 34, 'random actions': 500}\n","240000 : {'total games': 1388, 'ties': 1, 'illegal moves': 215, 'player 0 wins': 1152, 'player 1 wins': 20, 'random actions': 514}\n","245000 : {'total games': 1365, 'ties': 5, 'illegal moves': 222, 'player 0 wins': 1106, 'player 1 wins': 32, 'random actions': 472}\n","250000 : {'total games': 1360, 'ties': 1, 'illegal moves': 223, 'player 0 wins': 1122, 'player 1 wins': 14, 'random actions': 507}\n","255000 : {'total games': 1387, 'ties': 2, 'illegal moves': 236, 'player 0 wins': 1127, 'player 1 wins': 22, 'random actions': 499}\n","260000 : {'total games': 1386, 'ties': 1, 'illegal moves': 208, 'player 0 wins': 1159, 'player 1 wins': 18, 'random actions': 523}\n","265000 : {'total games': 1382, 'ties': 2, 'illegal moves': 220, 'player 0 wins': 1142, 'player 1 wins': 18, 'random actions': 482}\n","270000 : {'total games': 1354, 'ties': 3, 'illegal moves': 218, 'player 0 wins': 1108, 'player 1 wins': 25, 'random actions': 468}\n","275000 : {'total games': 1377, 'ties': 1, 'illegal moves': 238, 'player 0 wins': 1121, 'player 1 wins': 17, 'random actions': 516}\n","280000 : {'total games': 1368, 'ties': 7, 'illegal moves': 245, 'player 0 wins': 1097, 'player 1 wins': 19, 'random actions': 486}\n","285000 : {'total games': 1393, 'ties': 5, 'illegal moves': 244, 'player 0 wins': 1124, 'player 1 wins': 20, 'random actions': 525}\n","290000 : {'total games': 1369, 'ties': 1, 'illegal moves': 245, 'player 0 wins': 1101, 'player 1 wins': 22, 'random actions': 496}\n","295000 : {'total games': 1386, 'ties': 3, 'illegal moves': 217, 'player 0 wins': 1147, 'player 1 wins': 19, 'random actions': 473}\n","300000 : {'total games': 1388, 'ties': 5, 'illegal moves': 223, 'player 0 wins': 1142, 'player 1 wins': 18, 'random actions': 486}\n","305000 : {'total games': 1368, 'ties': 3, 'illegal moves': 235, 'player 0 wins': 1112, 'player 1 wins': 18, 'random actions': 499}\n","310000 : {'total games': 1368, 'ties': 4, 'illegal moves': 251, 'player 0 wins': 1088, 'player 1 wins': 25, 'random actions': 507}\n","315000 : {'total games': 1376, 'ties': 4, 'illegal moves': 267, 'player 0 wins': 1089, 'player 1 wins': 16, 'random actions': 534}\n","320000 : {'total games': 1354, 'ties': 2, 'illegal moves': 266, 'player 0 wins': 1071, 'player 1 wins': 15, 'random actions': 529}\n","325000 : {'total games': 1401, 'ties': 5, 'illegal moves': 263, 'player 0 wins': 1115, 'player 1 wins': 18, 'random actions': 532}\n","330000 : {'total games': 1366, 'ties': 2, 'illegal moves': 205, 'player 0 wins': 1142, 'player 1 wins': 17, 'random actions': 465}\n","335000 : {'total games': 1382, 'ties': 2, 'illegal moves': 209, 'player 0 wins': 1150, 'player 1 wins': 21, 'random actions': 458}\n","340000 : {'total games': 1398, 'ties': 1, 'illegal moves': 244, 'player 0 wins': 1135, 'player 1 wins': 18, 'random actions': 528}\n","345000 : {'total games': 1393, 'ties': 3, 'illegal moves': 244, 'player 0 wins': 1123, 'player 1 wins': 23, 'random actions': 496}\n","350000 : {'total games': 1409, 'ties': 6, 'illegal moves': 305, 'player 0 wins': 1076, 'player 1 wins': 22, 'random actions': 539}\n","355000 : {'total games': 1410, 'ties': 4, 'illegal moves': 306, 'player 0 wins': 1064, 'player 1 wins': 36, 'random actions': 549}\n","360000 : {'total games': 1367, 'ties': 3, 'illegal moves': 277, 'player 0 wins': 1071, 'player 1 wins': 16, 'random actions': 503}\n","365000 : {'total games': 1391, 'ties': 3, 'illegal moves': 248, 'player 0 wins': 1123, 'player 1 wins': 17, 'random actions': 533}\n","370000 : {'total games': 1399, 'ties': 1, 'illegal moves': 227, 'player 0 wins': 1157, 'player 1 wins': 14, 'random actions': 471}\n","375000 : {'total games': 1378, 'ties': 0, 'illegal moves': 255, 'player 0 wins': 1104, 'player 1 wins': 19, 'random actions': 480}\n","380000 : {'total games': 1381, 'ties': 4, 'illegal moves': 221, 'player 0 wins': 1137, 'player 1 wins': 19, 'random actions': 478}\n","385000 : {'total games': 1372, 'ties': 1, 'illegal moves': 240, 'player 0 wins': 1102, 'player 1 wins': 29, 'random actions': 495}\n","390000 : {'total games': 1390, 'ties': 4, 'illegal moves': 227, 'player 0 wins': 1137, 'player 1 wins': 22, 'random actions': 506}\n","395000 : {'total games': 1381, 'ties': 2, 'illegal moves': 219, 'player 0 wins': 1134, 'player 1 wins': 26, 'random actions': 510}\n","400000 : {'total games': 1376, 'ties': 2, 'illegal moves': 239, 'player 0 wins': 1114, 'player 1 wins': 21, 'random actions': 512}\n","405000 : {'total games': 1384, 'ties': 1, 'illegal moves': 253, 'player 0 wins': 1108, 'player 1 wins': 22, 'random actions': 506}\n","410000 : {'total games': 1387, 'ties': 2, 'illegal moves': 210, 'player 0 wins': 1159, 'player 1 wins': 16, 'random actions': 517}\n","415000 : {'total games': 1369, 'ties': 0, 'illegal moves': 238, 'player 0 wins': 1113, 'player 1 wins': 18, 'random actions': 497}\n","420000 : {'total games': 1370, 'ties': 3, 'illegal moves': 270, 'player 0 wins': 1083, 'player 1 wins': 14, 'random actions': 509}\n","425000 : {'total games': 1384, 'ties': 3, 'illegal moves': 244, 'player 0 wins': 1118, 'player 1 wins': 19, 'random actions': 520}\n","430000 : {'total games': 1376, 'ties': 1, 'illegal moves': 247, 'player 0 wins': 1112, 'player 1 wins': 16, 'random actions': 489}\n","435000 : {'total games': 1371, 'ties': 2, 'illegal moves': 269, 'player 0 wins': 1075, 'player 1 wins': 25, 'random actions': 494}\n","440000 : {'total games': 1379, 'ties': 5, 'illegal moves': 248, 'player 0 wins': 1119, 'player 1 wins': 7, 'random actions': 516}\n","445000 : {'total games': 1381, 'ties': 2, 'illegal moves': 237, 'player 0 wins': 1129, 'player 1 wins': 13, 'random actions': 503}\n","450000 : {'total games': 1372, 'ties': 3, 'illegal moves': 260, 'player 0 wins': 1077, 'player 1 wins': 32, 'random actions': 477}\n","455000 : {'total games': 1389, 'ties': 3, 'illegal moves': 238, 'player 0 wins': 1121, 'player 1 wins': 27, 'random actions': 465}\n","460000 : {'total games': 1372, 'ties': 3, 'illegal moves': 253, 'player 0 wins': 1080, 'player 1 wins': 36, 'random actions': 524}\n","465000 : {'total games': 1378, 'ties': 5, 'illegal moves': 239, 'player 0 wins': 1119, 'player 1 wins': 15, 'random actions': 491}\n","470000 : {'total games': 1394, 'ties': 3, 'illegal moves': 244, 'player 0 wins': 1135, 'player 1 wins': 12, 'random actions': 485}\n","475000 : {'total games': 1392, 'ties': 1, 'illegal moves': 229, 'player 0 wins': 1144, 'player 1 wins': 18, 'random actions': 500}\n","480000 : {'total games': 1382, 'ties': 3, 'illegal moves': 277, 'player 0 wins': 1070, 'player 1 wins': 32, 'random actions': 475}\n","485000 : {'total games': 1382, 'ties': 2, 'illegal moves': 230, 'player 0 wins': 1134, 'player 1 wins': 16, 'random actions': 501}\n","490000 : {'total games': 1369, 'ties': 3, 'illegal moves': 246, 'player 0 wins': 1102, 'player 1 wins': 18, 'random actions': 513}\n","495000 : {'total games': 1371, 'ties': 0, 'illegal moves': 232, 'player 0 wins': 1125, 'player 1 wins': 14, 'random actions': 482}\n","Complete\n"]}]},{"cell_type":"markdown","metadata":{"id":"-GHC5ePQqewO"},"source":["学習が終了したので、実際に学習が済んだモデルと勝負してみる\n","- モデルが先攻なので、モデルが手を示した後、動作が止まる\n","- 一番左上を0として順に8までの数字でマスを表す\n","- このマス番号を入力して自身の手を入力する\n","\n","上手く学習が進んでいれば、モデルがミスしない限り引き分けになり、勝つことは相当困難になる\n","- ただし、現状のネットワークではそこまで強くはならない\n","- なお、ここで負けるようでは〇×ゲームがわかっていないことになる"]},{"cell_type":"code","metadata":{"id":"8i3hml71idP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724740077844,"user_tz":-540,"elapsed":2520238,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"d419a873-0319-43e2-ac0d-2eb5e99313a1"},"source":["device = torch.device(cuda if torch.cuda.is_available() else \"cpu\")\n","env = TicTacToe()\n","model = Policy(n_inputs=3*9, n_outputs=9).to(device)\n","model_state_dict = torch.load(\"tictactoe_model.pt\", map_location=device)\n","model.load_state_dict(model_state_dict)\n","model.eval()\n","done = False\n","obs = env._reset()\n","exp = {}\n","player = 0\n","while not done:\n","    print(\"Commands:\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n\\nBoard:\".format(*[x for x in range(0, 9)]))\n","    env._render()\n","    action = None\n","    if player == 1:\n","        action = int(input())\n","    else:\n","        state = torch.tensor([obs], dtype=torch.float).to(device)\n","        with torch.no_grad():\n","            p = F.softmax(model.forward(state), dim=1).cpu().numpy()\n","            valid_moves = (state.cpu().numpy().reshape(3,3,3).argmax(axis=2).reshape(-1) == 0)\n","            p = valid_moves*p\n","            action = p.argmax().item()\n","    obs, _, done, exp = env._step(action)\n","    player = 1 - player\n","print(\"Commands:\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n-----\\n{}|{}|{}\\n\\nBoard:\".format(*[x for x in range(0, 9)]))\n","env._render()\n","print(exp)\n","if \"tied\" in exp[\"reason\"]:\n","    print(\"A strange game. The only winning move is not to play.\")"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","0|0|0\n","-----\n","0|0|0\n","-----\n","0|0|0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","<ipython-input-24-fcd7fef26f22>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_state_dict = torch.load(\"tictactoe_model.pt\", map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","0|0|0\n","-----\n","0|1|0\n","-----\n","0|0|0\n","0\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|0|0\n","-----\n","0|1|0\n","-----\n","0|0|0\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|0|0\n","-----\n","1|1|0\n","-----\n","0|0|0\n","5\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|0|0\n","-----\n","1|1|2\n","-----\n","0|0|0\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|1|0\n","-----\n","1|1|2\n","-----\n","0|0|0\n","7\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|1|0\n","-----\n","1|1|2\n","-----\n","0|2|0\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|1|1\n","-----\n","1|1|2\n","-----\n","0|2|0\n","6\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|1|1\n","-----\n","1|1|2\n","-----\n","2|2|0\n","Commands:\n","0|1|2\n","-----\n","3|4|5\n","-----\n","6|7|8\n","\n","Board:\n","2|1|1\n","-----\n","1|1|2\n","-----\n","2|2|1\n","{'state': 'in progress', 'reason': 'Player 0 has tied'}\n","A strange game. The only winning move is not to play.\n"]}]},{"cell_type":"markdown","metadata":{"id":"CLdt8G-JdwMc"},"source":["# 課題1\n","\n","〇×ゲームの勝率を少しでも良いので改善しなさい"]},{"cell_type":"markdown","source":["# 課題2\n","\n","〇×ゲームをGRUやLSTNなどRNNに属するモデルを用いて学習させなさい\n","\n","また、その結果をオリジナルと比較しなさい"],"metadata":{"id":"sb3TchyDY2Ni"}},{"cell_type":"markdown","source":["# 課題3\n","\n","次のPytorchチュートリアルにある強化学習(DQN)をColaboratoryに実装し、動作を確認しなさい\n","\n","https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n","\n","以下の例は、env.action_sampleを利用している\n","\n","これを、新たに定義するselect_actionを用いて実現する"],"metadata":{"id":"mgjAqNdNa8Ul"}},{"cell_type":"markdown","source":["まず、ブラウザ上でアニメーションを行うため、pyvirtualdisplayをインストールする"],"metadata":{"id":"L0G9TBUI6H6-"}},{"cell_type":"code","source":["!pip install pyvirtualdisplay\n","!apt install xvfb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SgjxI2RS6FTd","executionInfo":{"status":"ok","timestamp":1724740092384,"user_tz":-540,"elapsed":14551,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"8e949968-bf90-4e81-f7e6-9e71eb30c62d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 7,813 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n","Fetched 7,813 kB in 1s (6,619 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 123595 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n"]}]},{"cell_type":"code","source":["import gym\n","from IPython import display\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","\n","# 環境と状態を初期化\n","env = gym.make('CartPole-v1')\n","env.reset()\n","\n","im = []\n","# 動画化\n","img = plt.imshow(env.render('rgb_array'))\n","for t in range(100):\n","    _, reward, done, _ = env.step(env.action_space.sample())\n","    display.clear_output(wait=True)\n","    img.set_data(env.render('rgb_array'))\n","    plt.axis('off')\n","    display.display(plt.gcf())\n","    if done:\n","      env.reset()"],"metadata":{"id":"MCRnZvjudR7G","colab":{"base_uri":"https://localhost:8080/","height":717},"executionInfo":{"status":"ok","timestamp":1724740099603,"user_tz":-540,"elapsed":7224,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"84f3c0d5-9a01-4b09-c33f-d22274925b2a"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["以上のように、動作するが、課題としてサンプルのactionを用いずに、指令を割いて定義すること\n","\n","以下にPytorchのTutrialの内容を記す\n","\n"],"metadata":{"id":"9sC0OCgPir28"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"JwJBsMBGKNrj","executionInfo":{"status":"ok","timestamp":1724740099603,"user_tz":-540,"elapsed":10,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"TPIqEslgKNrl"},"source":["\n","# Reinforcement Learning (DQN) Tutorial\n","**Author**: [Adam Paszke](https://github.com/apaszke)\n","            [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n","\n","\n","このチュートリアルでは、PyTorchを使ってDeep Q Learning (DQN)エージェントを学習する方法[Gymnasium](https://www.gymnasium.farama.org)を紹介している\n","\n","**Task**\n","\n","エージェントは、カートに取り付けられたポールがまっすぐに保たれるように、カートを左右どちらかに動かすという2つのアクションを決定する\n","\n","[Gymnasium's website](https://gymnasium.farama.org/environments/classic_control/cart_pole/)において、より難しいタスクを含む様々なタスクが照会されている\n"]},{"cell_type":"markdown","source":["## カートポール\n","\n","エージェントが環境の現在の状態を観察し、アクションを選択すると、環境は新しい状態に*遷移*し、アクションの結果を示す報酬を返す\n","\n","このタスクでは、報酬はタイムステップが増加するごとに+1され、ポールが大きく倒れるか、カートが中心から2.4ユニット以上離れると環境が終了する\n","- より良いパフォーマンスのシナリオがより長い時間実行され、より大きなリターンが蓄積されることを意味する\n","\n","CartPoleタスクにおいて、エージェントへの入力は、環境状態（位置、速度など）を表す4つの実数値である\n","- これらの4つの入力をスケーリングなしで受け取り、小さな完全連結ネットワークを用いて2つの出力を獲得する\n","- このネットワークは、入力状態が与えられたときに、各アクションの期待値を予測するように学習される\n","- また、このネットワークにより、期待値が最も高いアクションが選択される\n","\n","**Packages**\n","\n","まず、必要なパッケージをインポートする"],"metadata":{"id":"Xlz3SSFrysbS"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"QMUcBEIHKNrm","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":3069,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bca76a35-bd94-423d-828f-5f8308f83453"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium[classic_control]\n","  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium[classic_control])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.6.0)\n","Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 953.9/953.9 kB 11.9 MB/s eta 0:00:00\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}],"source":["%%bash\n","pip3 install gymnasium[classic_control]"]},{"cell_type":"markdown","metadata":{"id":"wlqj4mDdKNrm"},"source":["We'll also use the following from PyTorch:\n","\n","-  neural networks (``torch.nn``)\n","-  optimization (``torch.optim``)\n","-  automatic differentiation (``torch.autograd``)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"ltiJT-zWKNrn","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":5,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"50daa81c-80a0-40a0-a259-68df76277bb0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import gymnasium as gym\n","import math\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","# set up matplotlib\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()\n","\n","# if GPU is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"8YZsOtoBKNrn"},"source":["## リプレイメモリ\n","\n","DQNのトレーニングには経験リプレイメモリを利用する\n","- このメモリはエージェントが観察した遷移を保存し、後で再利用できるようにする\n","- このメモリからランダムにサンプリングすることで、バッチを構成する遷移が無相関となり、DQNの学習手順が安定するようになる\n","\n","そのために次の2つのクラスが定義されている\n","\n","- Transition（遷移）`` - 環境内の1つの遷移を表す名前付きタプルのクラス\n","  - このクラスは基本的に（状態、アクション）のペアを（次の状態、報酬）の結果にマッピングする\n","- ReplayMemory`` - 境界サイズの周期的なバッファ\n","  - 最近観測された遷移を保持する\n","  - ``.sample()``メソッドにより、学習用の遷移をランダムに選択することができる\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"8EGNLJvcKNrn","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"184b95ed-aacf-4f74-dbec-cd588f8327df"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["Transition = namedtuple('Transition',\n","                        ('state', 'action', 'next_state', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        \"\"\"Save a transition\"\"\"\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"]},{"cell_type":"markdown","source":["モデルを定義する前に、DQNについて簡単に復習する"],"metadata":{"id":"E-fGwL2GpBiw"}},{"cell_type":"markdown","metadata":{"id":"_kDWkwrcKNrn"},"source":["## DQNアルゴリズム\n","\n","まず、環境は決定論的に表現でき、すべて定式化可能であるとみなし、その上でDQNは環境の状態が確率的に遷移することを想定したとき、その遷移の期待値を表現する\n","\n","ここでは、割引を最大化しようとするポリシーを学習する\n","\n","まず、累積報酬を\n","$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$\n","とする\n","\n","$R_{t_0}$ を*return*とする\n","\n","割引 $\\gamma$ は $0$ から $1$ の定数であり、エージェントにとって、不確実な遠い将来からの報酬 $\\gamma$ は値が小さく、近い将来 の報酬よりも重要ではない\n","\n","また、エージェントは同じ報酬であっても、遠い未来の報酬よりも、時間的に近い報酬を集めるようになる\n","\n","Q-ラーニングの主な考え方は次のとおりである\n","\n","次の関数$Q^*: State \\times Action \\rightarrow \\mathbb{R}$を想定する\n","\n","この関数が、ある状態である行動を選択した場合どのような見返りがあるかを与える関数である場合、報酬を最大化するような政策を構築することは容易である\n","\n","報酬を最大化するポリシーは次の通りとなる\n","\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n","\n","世界のすべてを知っているわけではないいため、この$Q^*$を求めることは容易ではないが、ニューラルネットワークは関数近似器であり、ニューラルネットワークを作成して$Q^*$の近似関数を獲得するように訓練すればよい\n","\n","訓練には、あるポリシーのすべての$Q$関数がベルマン(Bellman)の方程式に従うことを利用する\n","\n","\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n","\n","この式の等号の両辺の差を$temporal difference errorと呼び、$delta$と表記する\n","\n","\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\\end{align}\n","\n","この誤差を最小化するために、[Huber 損失](https://en.wikipedia.org/wiki/Huber_loss)を使用する\n","- フーバー損失は、誤差が小さいときは平均二乗誤差、誤差が大きいときは平均絶対誤差のようにふるまう\n","- 従って、$Q$の推定値においてノイズが大きいとき、外れ値に対してよりロバストになる\n","\n","リプレイメモリからサンプリングした遷移のバッチ$B$について計算する：\n","\n","\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n","\n","\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n","     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n","     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n","   \\end{cases}\\end{align}\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### Q-network\n","\n","ここで構築するニューラルネットワークのモデルは、現在の画面パッチと以前の画面パッチの差を入力とするフィードフォワードのモデルである\n","\n","まず、現在のスクリーンパッチと以前のスクリーンパッチの差を求める\n","- このモデルは、$s$をネットワークへの入力とすると、$Q(s, \\mathrm{left})$とQ(s, \\mathrm{right})$の2つの出力がある\n","\n","このモデルは、ある入力について、ある行動をとるときの*期待収益(リターン)*を予測する\n","\n"],"metadata":{"id":"9wiyE3hppAdd"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"YYL3Bxu6KNro","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["class DQN(nn.Module):\n","\n","    def __init__(self, n_observations, n_actions):\n","        super(DQN, self).__init__()\n","        self.layer1 = nn.Linear(n_observations, 128)\n","        self.layer2 = nn.Linear(128, 128)\n","        self.layer3 = nn.Linear(128, n_actions)\n","\n","    # Called with either one element to determine next action, or a batch\n","    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        return self.layer3(x)"]},{"cell_type":"markdown","source":["## トレーニング"],"metadata":{"id":"cObBJaU_s_VR"}},{"cell_type":"markdown","metadata":{"id":"cqfcEBHbKNro"},"source":["### ハイパーパラメータとユーティリティ\n","\n","まず、モデルとそのオプティマイザをインスタンス化し、次の関連する関数を定義する\n","\n","- select_action`` - ε-greedy ポリシーに従ってアクションを選択する\n","  - アクションの選択にモデルを使うこともあれば、ランダムなアクションを選択することもある\n","  - ランダムなアクションを選択する確率は ``EPS_START`` から始まり、指数関数的に ``EPS_END`` に向かって減衰します\n","  - 減衰速度は``EPS_DECAY``により制御される\n","- plot_durations`` - エピソードの継続時間をプロットするヘルパーである\n","  - 直近100エピソードの(公式評価に基づく指標である)平均値とともに、エピソードの継続時間をプロットする\n","  - プロットはメイン・トレーニング・ループを実行する際、そのセル内に表示される\n","  - 各エピソードの後に即時に更新される\n","\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"JsQg071-KNro","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":2,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["# BATCH_SIZE is the number of transitions sampled from the replay buffer\n","# GAMMA is the discount factor as mentioned in the previous section\n","# EPS_START is the starting value of epsilon\n","# EPS_END is the final value of epsilon\n","# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n","# TAU is the update rate of the target network\n","# LR is the learning rate of the ``AdamW`` optimizer\n","BATCH_SIZE = 128\n","GAMMA = 0.99\n","EPS_START = 0.9\n","EPS_END = 0.05\n","EPS_DECAY = 1000\n","TAU = 0.005\n","LR = 1e-4\n","\n","# Get number of actions from gym action space\n","n_actions = env.action_space.n\n","# Get the number of state observations\n","state, info = env.reset()\n","n_observations = len(state)\n","\n","policy_net = DQN(n_observations, n_actions).to(device)\n","target_net = DQN(n_observations, n_actions).to(device)\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n","memory = ReplayMemory(10000)\n","\n","\n","steps_done = 0\n","\n","\n","def select_action(state):\n","    global steps_done\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","    steps_done += 1\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            # t.max(1) will return the largest column value of each row.\n","            # second column on max result is index of where max element was\n","            # found, so we pick action with the larger expected reward.\n","            return policy_net(state).max(1)[1].view(1, 1)\n","    else:\n","        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n","\n","\n","episode_durations = []\n","\n","\n","def plot_durations(show_result=False):\n","    plt.figure(1)\n","    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n","    if show_result:\n","        plt.title('Result')\n","    else:\n","        plt.clf()\n","        plt.title('Training...')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Duration')\n","    plt.plot(durations_t.numpy())\n","    # Take 100 episode averages and plot them too\n","    if len(durations_t) >= 100:\n","        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n","        means = torch.cat((torch.zeros(99), means))\n","        plt.plot(means.numpy())\n","\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","    if is_ipython:\n","        if not show_result:\n","            display.display(plt.gcf())\n","            display.clear_output(wait=True)\n","        else:\n","            display.display(plt.gcf())"]},{"cell_type":"markdown","metadata":{"id":"CYaVYfCGKNro"},"source":["### トレーニングループ\n","\n","モデルをトレーニング\n","\n","``optimize_model`` 関数により1ステップ分の最適化を行う\n","- 最初にバッチをサンプリングし、すべてのテンソルを1つに連結する\n","-  $Q(s_t, a_t)$ と $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$ を計算し、それらを組み合わせて損失を求める\n","- 定義により、$s$が終端状態であれば$V(s) = 0$とする\n","- 安定して$V(s_{t+1})$を計算するためにターゲットネットワークを利用する\n","- ターゲット・ネットワークは毎ステップ更新され、ハイパーパラメータ ``TAU`` によって制御された[ソフト更新](https://arxiv.org/pdf/1509.02971.pdf)で更新する\n","\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"5Dx_axvaKNrp","executionInfo":{"status":"ok","timestamp":1724740102663,"user_tz":-540,"elapsed":2,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["def optimize_model():\n","    if len(memory) < BATCH_SIZE:\n","        return\n","    transitions = memory.sample(BATCH_SIZE)\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","    # detailed explanation). This converts batch-array of Transitions\n","    # to Transition of batch-arrays.\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                          batch.next_state)), device=device, dtype=torch.bool)\n","    non_final_next_states = torch.cat([s for s in batch.next_state\n","                                                if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","    # columns of actions taken. These are the actions which would've been taken\n","    # for each batch state according to policy_net\n","    state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","    # Compute V(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n","    with torch.no_grad():\n","        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n","    # Compute the expected Q values\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","    # Compute Huber loss\n","    criterion = nn.SmoothL1Loss()\n","    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # In-place gradient clipping\n","    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"Il6N3AQBKNrp"},"source":["### メインのトレーニングループ\n","\n","処理内容は次の通り\n","- 最初に環境をリセットし、初期状態のテンソルを取得する\n","- 次に、アクションをサンプリング、実行した後、次の状態と報酬(常に1)を勘案してモデルを最適化する\n","- エピソードが終了、つまりモデルが失敗した場合、もう一度ループを再実行する\n","- エピソードはGPUが利用可能であれば`num_episodes`は600に設定され、そうでなければ50となる\n","  - CartPoleで良いパフォーマンスを観察するには50エピソードでは不十分である\n","  - 600のトレーニングエピソード内でモデルが500ステップを成功するように学習させる必要がある\n","\n","RLエージェントのトレーニングはノイズの多いプロセスであるため、収束が観察されない場合、トレーニングをやり直すことでよい結果が得られる場合がある\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"h1v89xepKNrp","executionInfo":{"status":"ok","timestamp":1724740645487,"user_tz":-540,"elapsed":542826,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":524},"outputId":"81f73f7c-fffb-4806-a0b4-48ddb665c31e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Complete\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{}}],"source":["if torch.cuda.is_available():\n","    num_episodes = 600\n","else:\n","    num_episodes = 50\n","\n","for i_episode in range(num_episodes):\n","    # Initialize the environment and get it's state\n","    state, info = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","    for t in count():\n","        action = select_action(state)\n","        observation, reward, terminated, truncated, _ = env.step(action.item())\n","        reward = torch.tensor([reward], device=device)\n","        done = terminated or truncated\n","\n","        if terminated:\n","            next_state = None\n","        else:\n","            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n","\n","        # Store the transition in memory\n","        memory.push(state, action, next_state, reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","        # Perform one step of the optimization (on the policy network)\n","        optimize_model()\n","\n","        # Soft update of the target network's weights\n","        # θ′ ← τ θ + (1 −τ )θ′\n","        target_net_state_dict = target_net.state_dict()\n","        policy_net_state_dict = policy_net.state_dict()\n","        for key in policy_net_state_dict:\n","            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n","        target_net.load_state_dict(target_net_state_dict)\n","\n","        if done:\n","            episode_durations.append(t + 1)\n","            plot_durations()\n","            break\n","\n","print('Complete')\n","plot_durations(show_result=True)\n","plt.ioff()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IaD6PnYyKNrp"},"source":["以下は、全体的なデータの流れを示す図である。\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/reinforcement_learning_diagram.jpg\">\n","\n","- アクションはランダムもしくはポリシーに基づいて選択され、ジム環境から次のステップのサンプルを取得\n","- その結果をリプレイ・メモリに記録する\n","- 反復毎に最適化ステップを実行する\n","- 最適化では、新しいポリシーのトレーニングを行うために、リプレイメモリからランダムなバッチを選択する\n","- 古いtarget_netは、Q値の期待値を求める最適化処理で使用される\n","- 重みのソフト更新はステップごとに行われる\n","\n"]}]}