{"cells":[{"cell_type":"markdown","metadata":{"id":"bHZ54p4iIfem"},"source":["---\n",">「不用意にもらす言葉こそ、ほんとうらしいものをふくんでいるのだ。 」\n",">\n","> 太宰治\n","---"]},{"cell_type":"markdown","source":["# はじめに\n","\n","ここでは、LangChainを主として、ChatGPTなどのLLMモデルを便利に使うための仕組みについて学ぶ\n","\n","なお、ChatGPTを対象とする場合、ChatGPTには、GPTsという独自のChatGPTをNoCodeで作成するツールが準備されている\n","- 従って、以下で説明する内容の多くは、この機能を用いて実現できる\n","- ただし、仕様がすぐに変更されるなど、混乱している状況にあるため、各自で触って試してみるとよい\n","- GPTsの開発状況により状況が大きく変わる可能性がある点に注意する事"],"metadata":{"id":"hTExuC4mcoQo"}},{"cell_type":"markdown","metadata":{"id":"L9HXMz2Zvn8D"},"source":["# LangChain\n","LangChainは日々更新されている\n","- これは、この授業テキスト全般に言えることであるが、特に後半は更新が頻繁に行われている\n","- アップデートにより実行できない場合もあるが、その場合は速やかに申し出ること\n","\n","その前に、OpenAPIのChat APIについて学ぶ\n","\n","なお、このノートブックは、GPUを使わないCPUランタイムを利用している\n","\n","***注意***\n","\n","ChatGPTは大人気のサービスのため負荷が集中しており、無償利用枠がかなり少なく、期限切れや、無料利用料金枠切れ、さらには、1分あたり利用は3回までという厳しい制限が課せられている\n","\n","無料料金枠などの問題は再度アカウントを取得すればよいが、1分あたり利用は3回までという制限はかなり厳しい\n","\n","例えば、次のような文章を含むエラーが出力された場合は、しばらく待って再度実行する必要がある\n","\n","```\n","WARNING:RateLimitError: Rate limit reached for default-gpt-3.5-turbo on requests per min. Limit: 3 / min. Please try again in 20s.\n","```\n","\n","したがって、無償枠の場合、このノートブックを纏めて全て実行とするとエラーになるため注意すること\n","\n","もし、まとめて実行する場合、途中で実行を待ってスロットを使いつくさないようにする必要があるため、次の設定を行うとよい\n","- ***特に無償ユーザの場合は、openai_wait = Trueにすること***"]},{"cell_type":"code","source":["import time\n","#openai_wait = True\n","openai_wait = False"],"metadata":{"id":"WMatVzE3Xfaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["また、途中でバージョン不一致によるエラーを回避するため、次のコードを実行後、速やかにセッションを再帰同すること(2024年時点で問題発生)"],"metadata":{"id":"dEvnudgbfOTB"}},{"cell_type":"code","source":["!pip install --upgrade nltk\n","import nltk"],"metadata":{"id":"1vnn8kmhfNuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chat APIを利用する\n"],"metadata":{"id":"-dyt3hia9vY1"}},{"cell_type":"markdown","source":["## API Keyの発行\n","\n","openai.comにアクセスし、DASHBOARDめゆーにあるAPI keysでAPI keyを発行する\n","- セキュリティのため、プロジェクトごとに異なるキーを利用すること\n","- ここでは、dataai-keyという鍵をつくるとよい\n","- 発行された鍵をコピーしておくこと"],"metadata":{"id":"3ojZHInfAtB9"}},{"cell_type":"markdown","source":["## API Keyを使えるようにする\n","\n","発行したKeyを次のコードにペーストして利用する\n","\n","次のセルにある、\n","\n","%env OPENAI_API_KEY= に続けて、APIキーを記録して実行すること"],"metadata":{"id":"oCgLsK6z_wCo"}},{"cell_type":"code","source":["%env OPENAI_API_KEY=\"INPUT YOUR OPENAI KEY\""],"metadata":{"id":"r0cuW-PeOD_s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724741537405,"user_tz":-540,"elapsed":277,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"38813d5a-15fd-4854-b056-f1dffbf646cc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["env: OPENAI_API_KEY=\"INPUT YOUR OPENAI KEY\"\n"]}]},{"cell_type":"code","source":["import os\n","print(os.environ['OPENAI_API_KEY'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjWTvXKWUTT9","executionInfo":{"status":"ok","timestamp":1724741538820,"user_tz":-540,"elapsed":2,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"846ced9c-3bcf-4c8f-ae59-93a22565fd1d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\"INPUT YOUR OPENAI KEY\"\n"]}]},{"cell_type":"markdown","source":["APIはOpenAI Chat APIに統一された\n","\n","モデルは、2023年夏時点で最新かつ無料で利用できる、gpt-3.5-turboを利用する\n","- model=\"gpt-4\"などとすることで、GPT-4を利用することができるが、利用料が高くなることに注意すること"],"metadata":{"id":"pZ8jrQtABH08"}},{"cell_type":"markdown","source":["## APIを使う\n","\n","まず、ChatGPTに挨拶して、APIが使えているかどうかを確認する\n","\n","\"Hello. Am I using the API correctly?\"\n","\n","と聞いて回答を実際に得る\n","\n","必要なライブラリは、単純に次の2つ\n","- インターネットを利用して、シンプルにRESTでリクエストを投げてレスポンスを得るためのrequests\n","- JSONフォーマットを扱うためのjson"],"metadata":{"id":"McSntSfqJWVP"}},{"cell_type":"code","source":["import requests\n","import os\n","import json"],"metadata":{"id":"Rr8d5MVyeTIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://api.openai.com/v1/chat/completions\"\n","headers = {\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","}\n","data = {\n","    \"model\": \"gpt-3.5-turbo\",\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"Hello. Am I using the API correctly?\"}\n","    ],\n","    \"temperature\": 0,\n","}\n","\n","response = requests.post(url=url, headers=headers, json=data)\n","print(json.dumps(response.json(), indent=2))"],"metadata":{"id":"bOzgI4KGOIcj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705801154,"user_tz":-540,"elapsed":1052,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"5075b7ee-3bb7-4e28-a5f6-8b8d4258b2bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"id\": \"chatcmpl-A0b38KCzbWJ8fGqza5Wb6rp85QY9k\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1724705782,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Hello! I'm an AI assistant and I'm here to help you with any questions you may have about using APIs. Can you provide more information about the specific API you are using and what you are trying to achieve?\",\n","        \"refusal\": null\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 16,\n","    \"completion_tokens\": 44,\n","    \"total_tokens\": 60\n","  },\n","  \"system_fingerprint\": null\n","}\n"]}]},{"cell_type":"markdown","source":["返事は\"content\"にあるように、\n","\n","\"Hello! I'm an AI language model and I'm here to help you. Could you please provide more details about the API you are using and what you are trying to achieve?\"\n","\n","となるが、意味としては、\n","\n","「こんにちは！私はAI言語モデルです。使用されているAPIの詳細と、何を達成しようとしているのかを教えていただけますか？」\n","\n","となる\n","\n","なお、パラメータについて、\n","- temperatureは0に近いほど、同じ回答を出力するようになる\n","- max_tokensは返答として最大何文字返すかを指定する\n","  - ChatGPTのAPIは、利用文字数(トークン数)により課金されるため、コスト低減を考えるのであれば重要である  \n","  デフォルトは16であるが、これは通常の利用ではかなり少ないといえる\n","- nは同一質問に対する返答数を指定する  \n","  この場合temparatureを大きめの値にしなければ、同じ回答が並ぶことになる\n","\n","これで、ひとまず使えるようになったであろう"],"metadata":{"id":"hw4z_Ng7KUco"}},{"cell_type":"markdown","source":["### エラートラブル(1)\n","\n","次のように表示される場合は、APIを利用するための無償枠を既に使い切ったか、時間が過ぎたためExpireしたことを意味する\n","\n","```\n","{\n","  \"error\": {\n","    \"message\": \"You exceeded your current quota, please check your plan and billing details.\",\n","    \"type\": \"insufficient_quota\",\n","    \"param\": null,\n","    \"code\": \"insufficient_quota\"\n","  }\n","}\n","```\n","\n","- 無償で続けたい場合は、新しいアカウントをつくるとよい\n","- もちろん、コストを支払ってもよい\n","\n","新しいアカウントをつくる\n","\n","- ただし、メールアドレスが必要となる\n","  - 既にメールアドレスが枯渇している場合は、フリーメールアドレスを取得するとよい\n","\n","- ブラウザのシークレットモードで、openai.comを開く(クッキーで既に持っているアカウント情報を利用しないようにするため)\n","\n","- 右上のメニューからLoginを選択し、Sign upを選択する\n","  - メールアドレスとパスワードを入力する\n","  - 確認メールが届くのでVerifyする\n","\n","- OpenAIのページに行くと、名前や誕生日の入力が求められる\n","\n","- スマートフォンの番号を入れて、コードを受け取る\n","  - 現状では利用済の番号でも問題ないが、将来は不明である\n","\n","- APIをクリックして、OpenAI platformに行き、右上の丸いアイコンをクリックして個人メニューに入る\n","\n","- 左のタブでAPI Keysを選択する\n","\n","- \"Create new secret key\"を選択し、dataai-keyという鍵を作成してコピーしておく\n","\n","以上、あらたに入手した鍵を利用して再実行すること\n","- ただし5ドル分しかないので注意すること"],"metadata":{"id":"vI5oDCjZEcJb"}},{"cell_type":"markdown","source":["### エラートラブル(2)\n","\n","頻繁にアクセスすると、エラーが発生する\n","\n","これは、単位時間あたりのアクセス数が制限されているためである\n","- しばらく待ってリトライすること"],"metadata":{"id":"12TmNkeaPkMV"}},{"cell_type":"markdown","source":["### roleについて\n","\n","\"role\"は、次の3つがある\n","- userはChatGPTのユーザで、皆さんのこと\n","- assistantがChatGPTによる回答を指し、ChatGPTのこと\n","- systemはassistantのふるまいを制御するために利用\n","\n","但し、gpt-3.5-turboでは、systemは利用しない\n"],"metadata":{"id":"o6grQLKrN9xo"}},{"cell_type":"markdown","source":["## API利用において重要なこと"],"metadata":{"id":"1LggZY0MRJ7W"}},{"cell_type":"markdown","source":["### ブラウザ版との違い\n","\n","API利用と、ブラウザ利用における最も大きな違いは、API利用では過去の会話のやり取りを一切考慮しないという点である\n","\n","したがって、APIを利用する場合で過去の会話を参照したい場合は、過去の会話そのものを全てmessageに記載する必要がある\n"],"metadata":{"id":"hMArjv5yOllm"}},{"cell_type":"markdown","source":["### 料金の確認\n","\n","https://openai.com/pricing にアクセスすると、\n","\n","| Model\t| Input\t| Output |\n","|:---|:---|:---|\n","| GPT-3.5-Turbo 4K context | \\$0.0015 / 1K tokens | \\$0.002 / 1K tokens |\n","\n","と記載されている\n","\n","現在いくらつかったかは、\n","\n","https://platform.openai.com/account/usage\n","\n","にアクセスして確認するとよい\n"],"metadata":{"id":"xTfjkD7QOrIy"}},{"cell_type":"markdown","source":["### トークン数\n","\n","課金対象にもなっているトークン数について、トークンは基本的に単語のことであり、トークン数は入力した単語の数を意味する\n","\n","ChatGPTを含む多くのLMにおいて、膨大な単語を効率よく学習するため、一つの単語を複数のトークンに分割して処理している\n","- 例えば、\"humburger\"は、\"hum\", \"bur\", \"ger\"の3つに分解される\n","\n","また、日本語と英語ではトークン数のカウント方法が異なる\n","- 日本語は内部で英語に変換されて処理されているため日本語は似た内容の文章において課金上不利となる\n","\n","- 日本語は1トークンはおよそ4文字、英語ではおよそ0.75語に相当する\n","  - 一般に直接英語を利用した方がお得といわれている所以\n","  - 実際2倍程度の開きがある\n","\n","このトークン数は、各モデルの入力や出力サイズの制限にも利用される\n","\n","直接文字数を計数したい場合は、https://platform.openai.com/tokenizer にアクセスして、文章を入力するとよい\n","\n","\n"],"metadata":{"id":"YZ6vL8YXOvFN"}},{"cell_type":"markdown","source":["# GPT APIを用いたアプリケーション実装\n","\n","***ここでは、料理名を入力することで、材料と手順、調理時間、お勧めのサイドメニューなどを表示するというアプリを作成することを念頭に説明する***"],"metadata":{"id":"yw6W-1k9kQ7T"}},{"cell_type":"markdown","source":["## 全体の構成\n","\n","### APIまでの通信手順\n","\n","以下の手順を踏む\n","- スマートフォンやPC、Webのアプリを利用して、レシピ名をサーバプログラムに送信する\n","- サーバプログラムは、プロンプトエンジニアリングを行い、APIキーを付与してOpenAIのAPIを叩く\n","\n","レシピ生成アプリが直接OpenAI APIを叩くような構成は、APIキーが漏えいするため普通は行わない\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi1.jpg\" width=700>"],"metadata":{"id":"toaMHvttly4S"}},{"cell_type":"markdown","source":["# LangChain"],"metadata":{"id":"AFbLFE6gqLkD"}},{"cell_type":"markdown","source":["## LangChainとは？\n","\n","LLMを使ったアプリケーション開発フレームワーク\n","- PythonとJavaScript/TypeScriptの2つがある\n","- フリーで利用できる\n","\n","詳細は公式のドキュメントを参照すること\n","- 過去のバージョンのマニュアルを見る場合は、githubにあるlangchainに行き、docsのreleasesから目的のバージョンを選択するとよい"],"metadata":{"id":"XpZ3ySBfqQGu"}},{"cell_type":"markdown","source":["## Module\n","\n","LangChainにはmoduleと呼ばれる構成要素がある\n","- moduleとして、Models, Prompts, Chains, Indexes, Memory, Agentsがある"],"metadata":{"id":"gUfbxuHqrE-W"}},{"cell_type":"markdown","source":["### Models module\n","\n","LongChainで利用する機械学習モデルである\n","\n","- Chat Models: Open AIのChatAPIのためのモジュール\n","- Text Embedding Models: テキストをベクトル化するモデル\n","\n"],"metadata":{"id":"Czl2ZbdordE3"}},{"cell_type":"markdown","source":["先に示した簡単な対話プログラムを再構成する\n","- 先ほどよりもシンプルに記述できることがわかるであろう\n","- 内部でjsonに変換され通信が行なわれている"],"metadata":{"id":"6sB2-PQgsrMO"}},{"cell_type":"markdown","source":["openaiをインストールする\n","\n","ライブラリの開発速度が速く、バージョン競合が簡単に発生するため注意すること\n","- できれば、自分で解決する能力を身に着けるとよい\n","- (2023/12) openai、cohere、tiktokenは同時に導入しないと警告が表示される\n","- (2023/12) tensorflow-probabilityと一部ライブラリがコンフリクトするため、uninstallする\n","\n","次のようにインストールすることで対処する"],"metadata":{"id":"QiLIDe5732PJ"}},{"cell_type":"code","source":["!pip uninstall -y --quiet tensorflow-probability"],"metadata":{"id":"GrJA882U5Blz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705802777,"user_tz":-540,"elapsed":1625,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"fdbb8dd7-e74f-47a7-caf1-7f26d7e187ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install --quiet openai cohere tiktoken"],"metadata":{"id":"lWZ181WMBpLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet chromadb kaleido python-multipart"],"metadata":{"id":"LxnKpkivFMKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet langchain-community"],"metadata":{"id":"A8IbLgpEGqJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次のようにpredict関数を用いることで、問い合わせと応答を行うことができる"],"metadata":{"id":"0HX01Spxvna6"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","result = llm.invoke(\"自己紹介してください。\")\n","print(result.content)"],"metadata":{"id":"gJHDUhwwgKc6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705828801,"user_tz":-540,"elapsed":4129,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"7feca328-fe03-4c70-872a-876c97f15105"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["はじめまして、私はAIアシスタントです。自然言語処理技術を用いて、さまざまな質問や会話に対応することができます。お手伝いが必要なことがあれば、遠慮なくお知らせください。どうぞよろしくお願いいたします。\n"]}]},{"cell_type":"markdown","source":["### Prompts module\n","\n","モデルへの入力を組み立てるmoduleであり、次の要素がある\n","- Prompt Templates\n","- Chat Prompt Templates\n","- Example Selectors\n","- Output Parsers\n","\n","ここでは、Prompt Templatesについて説明する\n","- ChatGPTへのプロンプトについてテンプレートつまり例文を作成することができる\n","\n"],"metadata":{"id":"eVvTlTgDtQgX"}},{"cell_type":"markdown","source":["次のコードでは、commandがlsに置き換わる\n","- Promptの長さを考慮して埋め込む\n","- 出力する形式を指定して埋め込む\n","\n","などが可能であり、単純なPythonコードによる埋め込みよりも高度な処理が可能である"],"metadata":{"id":"0giJqxftt69A"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","\n","コマンド: {command}\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","\n","result = prompt.format(command=\"ls\")\n","print(result)"],"metadata":{"id":"-sZIx1e8iwJ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705828801,"user_tz":-540,"elapsed":9,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"40619307-4419-440b-a772-cbe7b2bcd9ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","次のコマンドの概要を説明してください。\n","\n","コマンド: ls\n","\n"]}]},{"cell_type":"markdown","source":["### Chains module\n","\n","Models, Templates, Chainsなどのmoduleを連結する\n","\n","なお、LangChainの挙動の詳細を確認するため、\n","`langchain.verbose = True`\n","としている\n","- いろいろと意味のない文章や宣伝も表示されるが、不要な場合は、Falseとするとよい"],"metadata":{"id":"cI2F8zKJuV-M"}},{"cell_type":"code","source":["import langchain\n","from langchain.chains import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","\n","langchain.verbose = True\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Prompt を用意\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","\n","コマンド: {command}\n","\"\"\"\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","\n","# Chain を作成\n","chain = LLMChain(llm=chat, prompt=prompt)\n","\n","# 実行\n","result = chain.invoke(\"ls\")\n","print(result)"],"metadata":{"id":"9QyYR_Iixyuy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705831645,"user_tz":-540,"elapsed":2852,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"1fa46ac0-cd15-467c-d2ee-6458cede51ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","次のコマンドの概要を説明してください。\n","\n","コマンド: ls\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","{'command': 'ls', 'text': 'lsコマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。lsコマンドを実行することで、ファイルやディレクトリの名前や属性、更新日時などの情報を確認することができます。'}\n"]}]},{"cell_type":"markdown","source":["chain.invokeを用いて、構築したchainが順に実行される\n","- 最初に文字の埋め込み(prompt)が行なわれる\n","- 次にchat modelにより実際に通信が行なわれる\n","\n","このChainには各種存在する"],"metadata":{"id":"B-A400ZJu2ws"}},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"OfIPskvyXo_3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### SimpleSequentialChain\n","\n","ChainとChainを直列に連結する\n","\n"],"metadata":{"id":"nSMyGRodvnbL"}},{"cell_type":"markdown","source":["例えば、次の例を考えてみよう\n","- 簡単な算数の問題を問い合わせ、それに対して回答を得る場合、詳細な手順を聞くようにすると解答の精度が向上する\n","- しかしながら、欲しいのは最終的な答えであって、途中経過は不要であるとする\n","- すると、まず、詳細な手順を含む答えを得てから、その答えを要約して最後の答えだけ得るようにするとよい\n","\n","つまり、ChatGPTを2回利用して最終的に欲しい回答を獲得することになる"],"metadata":{"id":"DwtbSXduwJO-"}},{"cell_type":"code","source":["from langchain.chains import LLMChain\n","from langchain.chains import SimpleSequentialChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","\n","langchain.verbose = True\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# 1 つ目の Prompt と Chain を用意\n","cot_template = \"\"\"\n","以下の質問に回答してください。\n","\n","### 質問 ###\n","{question}\n","### 質問終了 ###\n","\n","ステップバイステップで考えましょう。\n","\"\"\"\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=cot_template,\n",")\n","cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n","\n","# 2 つ目の Prompt と Chain を用意\n","summarize_template = \"\"\"\n","入力を結論だけ抜き出して記述してください。\n","\n","### 入力 ###\n","{input}\n","### 入力終了 ###\n","\"\"\"\n","summarize_prompt = PromptTemplate(\n","    input_variables=[\"input\"],\n","    template=summarize_template,\n",")\n","summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n","\n","# 2 つの Chain を直列に繋ぐ\n","cot_summarize_chain = SimpleSequentialChain(\n","    chains=[cot_chain, summarize_chain])\n","\n","# 実行\n","result = cot_summarize_chain(\n","    \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\")\n","print(result[\"output\"])"],"metadata":{"id":"BETIBsg104QZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705835555,"user_tz":-540,"elapsed":3913,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"739fbd81-da00-419f-caa8-77778c9bec3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","以下の質問に回答してください。\n","\n","### 質問 ###\n","私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\n","### 質問終了 ###\n","\n","ステップバイステップで考えましょう。\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3m1. 最初に市場で10個のリンゴを買いました。\n","2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n","3. その後、5つのリンゴを追加で購入しました。残りは6 + 5 = 11個です。\n","4. 最後に1つのリンゴを食べたので、残りは11 - 1 = 10個です。\n","\n","したがって、最終的には10個のリンゴが残ります。\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m\n","入力を結論だけ抜き出して記述してください。\n","\n","### 入力 ###\n","1. 最初に市場で10個のリンゴを買いました。\n","2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n","3. その後、5つのリンゴを追加で購入しました。残りは6 + 5 = 11個です。\n","4. 最後に1つのリンゴを食べたので、残りは11 - 1 = 10個です。\n","\n","したがって、最終的には10個のリンゴが残ります。\n","### 入力終了 ###\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3m最終的には10個のリンゴが残ります。\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","最終的には10個のリンゴが残ります。\n"]}]},{"cell_type":"markdown","source":["なお、独自のpromptsやchains moduleを作成することも可能である\n","\n","詳細は、LangChainのマニュアル https://python.langchain.com/docs/get_started/introduction を参照されたい\n"],"metadata":{"id":"V-oQdKENxWo_"}},{"cell_type":"markdown","source":["#### Output Parsers\n","\n","出力形式を指定するプロンプトの作成とPythonオブジェクトとのマッピングを提供する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi2.jpg\" width=700>"],"metadata":{"id":"CXZ034oXyCvv"}},{"cell_type":"code","source":["from langchain.chains import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain.prompts import PromptTemplate\n","from pydantic import BaseModel, Field, validator\n","from typing import List\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","class Recipe(BaseModel):\n","    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n","    steps: List[str] = Field(description=\"steps to make the dish\")\n","    time: List[str] = Field(description=\"time to make the dish\")\n","    sides: List[str] = Field(description=\"side menu of the dish\")\n","#    tools: List[str] = Field(description=\"tools which are required to cook the dish\")\n","\n","template = \"\"\"料理のレシピを教えてください。\n","\n","{format_instructions}\n","\n","料理名: {dish}\n","\"\"\"\n","\n","parser = PydanticOutputParser(pydantic_object=Recipe)\n","\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"dish\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","chain = LLMChain(llm=chat, prompt=prompt)\n","\n","output = chain.run(dish=\"カレー\")\n","#output = chain.run(dish=\"インド本格カレー\")\n","print(\"=== output ===\")\n","print(output)"],"metadata":{"id":"qkm0Ys9k4t3h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705839799,"user_tz":-540,"elapsed":4256,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"ab53928c-2980-4009-a685-d8d7c0feec8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m料理のレシピを教えてください。\n","\n","The output should be formatted as a JSON instance that conforms to the JSON schema below.\n","\n","As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n","the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n","\n","Here is the output schema:\n","```\n","{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}, \"time\": {\"description\": \"time to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Time\", \"type\": \"array\"}, \"sides\": {\"description\": \"side menu of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Sides\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\", \"time\", \"sides\"]}\n","```\n","\n","料理名: カレー\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","=== output ===\n","```json\n","{\n","    \"ingredients\": [\n","        \"カレールー\",\n","        \"肉（牛肉、豚肉、鶏肉など）\",\n","        \"じゃがいも\",\n","        \"にんじん\",\n","        \"玉ねぎ\",\n","        \"油\",\n","        \"水\"\n","    ],\n","    \"steps\": [\n","        \"1. じゃがいも、にんじん、玉ねぎを適当な大きさに切る。\",\n","        \"2. 鍋に油を熱し、肉を炒める。\",\n","        \"3. 野菜を加えて炒める。\",\n","        \"4. 水を加えて煮込む。\",\n","        \"5. カレールーを加えて溶かし、とろみがつくまで煮込む。\",\n","        \"6. 器に盛り付けて完成。\"\n","    ],\n","    \"time\": [\n","        \"準備時間: 15分\",\n","        \"調理時間: 30分\"\n","    ],\n","    \"sides\": [\n","        \"ご飯\",\n","        \"フライドポテト\",\n","        \"サラダ\"\n","    ]\n","}\n","```\n"]}]},{"cell_type":"markdown","source":["最後に、recipe型にマッピングする"],"metadata":{"id":"YEJA39jxzwdW"}},{"cell_type":"code","source":["recipe = parser.parse(output)\n","print(\"=== recipe object ===\")\n","print(recipe)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TW2oh6_QzrGk","executionInfo":{"status":"ok","timestamp":1724705839799,"user_tz":-540,"elapsed":12,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e1af1984-7ad2-4ac4-a139-7e883a7194c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== recipe object ===\n","ingredients=['カレールー', '肉（牛肉、豚肉、鶏肉など）', 'じゃがいも', 'にんじん', '玉ねぎ', '油', '水'] steps=['1. じゃがいも、にんじん、玉ねぎを適当な大きさに切る。', '2. 鍋に油を熱し、肉を炒める。', '3. 野菜を加えて炒める。', '4. 水を加えて煮込む。', '5. カレールーを加えて溶かし、とろみがつくまで煮込む。', '6. 器に盛り付けて完成。'] time=['準備時間: 15分', '調理時間: 30分'] sides=['ご飯', 'フライドポテト', 'サラダ']\n"]}]},{"cell_type":"code","source":["recipe.ingredients"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkf3FX9080tZ","executionInfo":{"status":"ok","timestamp":1724705839799,"user_tz":-540,"elapsed":11,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"5982bf24-1219-4c26-b55f-c7dbd541093e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['カレールー', '肉（牛肉、豚肉、鶏肉など）', 'じゃがいも', 'にんじん', '玉ねぎ', '油', '水']"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### Indexes"],"metadata":{"id":"prFUg4G3QfbA"}},{"cell_type":"markdown","source":["ChatGPTは学習に用いたデータセットの範疇でのみ答えを出すため、新しい知識や概念については、正しく解答することが難しい\n","\n","例えば、次のような質問に対しては、お手上げになっている\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain1.jpg\" width=500>\n","\n","ここで、質問に対する回答を得るうえで必要となる情報を渡してから処理させると、おおよそ正しい回答を得ることができるようになる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain2.jpg\" width=500>\n"],"metadata":{"id":"p3Gp3u-hJHR_"}},{"cell_type":"markdown","source":["コンテキストとして、情報を加えることで正しい回答を与えることができており、これは強力な方法であるが、実際に用いる場合は注意が必要である\n","\n","- ChatGPTには、入力文字数に制限があるため、長い文章を入力する必要がある場合はともかく、様々な情報を大量に与えておいて、そこから適切な回答を得るという利用は困難である\n","- 文字数、つまり入力トークン数も課金に関係するため、高コストとなる\n","\n","まず、全ての情報を与えておいて、何かしら回答を得るということは非現実的であるが、自動化という点では有効な手段である\n","\n"],"metadata":{"id":"R59skBRVYMJp"}},{"cell_type":"markdown","source":["#### Vector Store\n","\n","そこで、Vector Storeを活用する\n","- 文章をベクトル化してVector Storeに保存、入力と近しいベクトルの文章をVector Storeから検索してcontextに含める手法\n","- 全文章ではなく、「大事と思われる部分文章群についてのみ」情報を与える\n","  - これには、、embeddingと呼ばれる内部ベクトル表現への変換を行い、そのベクトルの近接性を用いて、どの文章が重要かを判断している\n","  - 単語をベクトル化するトークンではない点に注意すること\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain3.jpg\" width=700>\n","\n"],"metadata":{"id":"ob50Tp4IZaSW"}},{"cell_type":"markdown","source":["実際にindexを扱う\n","\n","ここでは、LangChainのドキュメントを参考にして、質問できるようにする\n","\n","最新のドキュメントはgitで公開されているため、LangChainのgitをcloneする"],"metadata":{"id":"PVg3gQRLcb2X"}},{"cell_type":"code","source":["!git clone https://github.com/hwchase17/langchain.git"],"metadata":{"id":"8VY_0R_VGcHv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705840230,"user_tz":-540,"elapsed":441,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"824e4a01-e153-489a-b497-62c9aded17c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'langchain' already exists and is not an empty directory.\n"]}]},{"cell_type":"markdown","source":["langchainのディレクトリに入る"],"metadata":{"id":"UdmH-q4VcpFn"}},{"cell_type":"code","source":["!cd langchain"],"metadata":{"id":"uhcQzqUqR1t3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["以降の動作確認で必要となるライブラリを導入する"],"metadata":{"id":"gK1vJgKmgpG1"}},{"cell_type":"code","source":["!pip install --quiet unstructured tabulate pdf2image pytesseract chromadb tiktoken"],"metadata":{"id":"znXiN1W1Rd8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["まず、ディレクトリの中の全体を読むための便利なDirectoryLoaderを用いて、ある場所にあるファイルをサブディレクトリもまとめて取得する\n","- ここでは拡張子がmdであるファイルに限定している\n","\n","- そこから次々に文章を取得して、VectorstoreIndexCreatorで、Vectorsoreに格納していく\n","  - この時、embeddingと呼ばれる内部ベクトル表現への変換も同時に行っている"],"metadata":{"id":"waRfWzSLhL_p"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INKjEoCQXbft","executionInfo":{"status":"ok","timestamp":1724705858579,"user_tz":-540,"elapsed":8049,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"cedfd41a-ed8e-4555-8de9-257248858a54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import DirectoryLoader\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain.embeddings import OpenAIEmbeddings\n","nltk.download('punkt')\n","loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.mdx\")\n","embeddings = OpenAIEmbeddings()\n","index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])"],"metadata":{"id":"Q10Ff7vNQuV1","executionInfo":{"status":"ok","timestamp":1724705903767,"user_tz":-540,"elapsed":45190,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"70bc1c66-a3aa-4b99-9f9d-1c3b415f43e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n","  warn_deprecated(\n","/usr/local/lib/python3.10/dist-packages/langchain/indexes/vectorstore.py:127: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["では、そのvector storeを利用して、質問する\n","\n","なお、現時点でChatGPTに\"LangChain\"について問い合わせると、データセットに含まれていないという回答になる"],"metadata":{"id":"2-k97WTohvnK"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","result = index.query(\"LangChainにおけるIndex moduleについて概要を1文で説明してください。\", llm=chat)\n","print(result)"],"metadata":{"id":"mJbppn-lRblc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724705905992,"user_tz":-540,"elapsed":2228,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"1d40d240-83e7-4a7a-c4c5-9ba29dfda565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","----------------\n","--\n","\n","sidebar_position: 0\n","\n","sidebar_class_name: hidden\n","\n","---\n","\n","# Introduction\n","\n","**LangChain** is a framework for developing applications powered by large language models (LLMs).\n","\n","LangChain simplifies every stage of the LLM application lifecycle: - **Development**: Build your applications using LangChain's open-source [building blocks](/docs/concepts#langchain-expression-language-lcel), [components](/docs/concepts), and [third-party integrations](/docs/integrations/platforms/). Use [LangGraph](/docs/concepts/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support. - **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence. - **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/).\n","\n","### [Integrations](/docs/integrations/providers/) LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/integrations/providers/).\n","\n","### [Contributing](/docs/contributing) Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.\n","\n","# arXiv\n","\n","LangChain implements the latest research in the field of Natural Language Processing. This page contains `arXiv` papers referenced in the LangChain Documentation, API Reference, Templates, and Cookbooks.\n","\n","From the opposite direction, scientists use `LangChain` in research and reference it in the research papers. Here you find papers that reference: - [LangChain](https://arxiv.org/search/?query=langchain&searchtype=all&source=header) - [LangGraph](https://arxiv.org/search/?query=langgraph&searchtype=all&source=header) - [LangSmith](https://arxiv.org/search/?query=langsmith&searchtype=all&source=header)\n","\n","## Summary\n","\n","LangChain Expression Language, or LCEL, is a declarative way to chain LangChain components. LCEL was designed from day 1 to **support putting prototypes in production, with no code changes**, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n","\n","**First-class streaming support** When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n","Human: LangChainにおけるIndex moduleについて概要を1文で説明してください。\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","申し訳ありませんが、LangChainのIndex moduleに関する具体的な情報は提供されていません。\n"]}]},{"cell_type":"markdown","source":["`langchain.verbose = True`のため、無意味な動作確認メッセージも表示されているが、要約文が出力されていることがわかる\n","- なお、若干回答は的を得ていない\n"],"metadata":{"id":"jFRjpFYqiJh3"}},{"cell_type":"markdown","source":["DocumentLoadersは、webサイト、GoogleDrive、Slackなどを読み込む機能が存在しているため、様々な情報をVctor Storeに格納することができる"],"metadata":{"id":"8PBuUnC4jU0_"}},{"cell_type":"markdown","source":["このように、ある特定分野のデータを一連の要素としてエンコードし、それぞれが内部で 1 つの「ベクトル」として表現している\n","- これには、単語であればWord2Vecなどが利用できるが、ここでは文章であることから、BERTをFine-TuningしたSentenceTransformerの利用が検討される\n","- このベクトル数値は、多次元ベクトル空間で要素を相互に関連づけてマッピングしている\n","\n","ベクトル要素がセマンティックであり、ある一つの意味を表していると考えるならば、そのベクトルの近接性が文脈関係の指標となりえる\n","- このベクトルはエンベディングと呼ばれる\n","- 互いに関連性のある意味要素がまとまって配置されるようにエンベディングを行う\n","\n","特定分野の文脈によって、セマンティック要素は単語、フレーズ、センテンス、パラグラフ、文書全体、画像、あるいはまったく別のものになる可能性があり、エンベディングが最善というわけではない\n","\n","\n","\n","プロンプトで必要となる文脈を生成するため、データベースに問い合わせを行い、ベクトル空間の入力と密接に関連する要素を抽出する必要がある\n","\n","ベクトルデータストアは、大量のベクトルを保存し、問い合わせに答えるシステム\n","- 効率的な最近傍クエリアルゴリズム(k-NNなど)と適切なインデックスにより、データ検索を行う"],"metadata":{"id":"S10hxRg_kURG"}},{"cell_type":"markdown","source":["### Memory"],"metadata":{"id":"ARj8Y8_RdlD6"}},{"cell_type":"markdown","source":["ChatGPTをブラウザで利用した場合、過去の会話の履歴を踏まえて返答するが、APIではそのような振る舞いは行わない\n","\n","APIを利用する場合は、過去の履歴をプロンプトに入力する必要がある\n","\n","実際にその振る舞いを確認する"],"metadata":{"id":"B0EPjzsTQntU"}},{"cell_type":"markdown","source":["次のような関数を用意してAPIを用いて文章を渡す"],"metadata":{"id":"YzABPdjkRm7e"}},{"cell_type":"code","source":["def post_chat_completions(content):\n","  url = \"https://api.openai.com/v1/chat/completions\"\n","  headers = {\n","      \"Content-Type\": \"application/json\",\n","      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","  }\n","  data = {\n","      \"model\": \"gpt-3.5-turbo\",\n","      \"messages\": [\n","          {\"role\": \"user\", \"content\": content}\n","      ],\n","      \"temperature\": 0,\n","  }\n","\n","  response = requests.post(url=url, headers=headers, json=data)\n","  print(json.dumps(response.json(), indent=2))"],"metadata":{"id":"f7pxRwGnUDvn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["では、実際に名前を伝える"],"metadata":{"id":"2tIxorsKRxtu"}},{"cell_type":"code","source":["post_chat_completions(\"Hi! I'm Keio Yukichi!\")"],"metadata":{"id":"V_OBzUQGVAwa","executionInfo":{"status":"ok","timestamp":1724705906515,"user_tz":-540,"elapsed":525,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9031a37-da68-4ee9-91fd-cb4ce4ff0945"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"id\": \"chatcmpl-A0b4qGbmDZ2xH3BoHUONUcjdJMJeD\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1724705888,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Hello Keio Yukichi! How can I assist you today?\",\n","        \"refusal\": null\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 16,\n","    \"completion_tokens\": 13,\n","    \"total_tokens\": 29\n","  },\n","  \"system_fingerprint\": null\n","}\n"]}]},{"cell_type":"markdown","source":["\"Hello Keio Yukichi! How can I assist you today?\" といった回答が得られているであろう\n","- Generativeであるため、この答えは毎回異なる\n","\n","その上で、名前を憶えているか聞いてみよう"],"metadata":{"id":"3-ruccLKR-Zi"}},{"cell_type":"code","source":["post_chat_completions(\"Do you know my name?\")"],"metadata":{"id":"EYjoBKp8VAjZ","executionInfo":{"status":"ok","timestamp":1724705907422,"user_tz":-540,"elapsed":909,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"06a10690-9674-446e-a2ee-2ffad97205a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"id\": \"chatcmpl-A0b4rhCFpIeJsttLcePdmZKQOC9gr\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1724705889,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"I'm sorry, I do not have the ability to know your name unless you tell me. How may I assist you today?\",\n","        \"refusal\": null\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 13,\n","    \"completion_tokens\": 26,\n","    \"total_tokens\": 39\n","  },\n","  \"system_fingerprint\": null\n","}\n"]}]},{"cell_type":"markdown","source":["当然であるが、知らないという答えになる\n","\n","そこで、過去の会話の履歴を全て入れて、同じ質問を行ってみよう"],"metadata":{"id":"RpXJqn09SJS0"}},{"cell_type":"code","source":["post_chat_completions(\"\"\"A: Hi! I'm Keio Yukichi!\n","B: Hello Keio Yukichi! How can I assist you today?\n","A: Do you know my name?\n","B: \"\"\")"],"metadata":{"id":"2ctwcsZ-VLdD","executionInfo":{"status":"ok","timestamp":1724705908043,"user_tz":-540,"elapsed":622,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1575be50-6b96-421c-b9fe-f6fcdef9a836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"id\": \"chatcmpl-A0b4rI5W3kdzh9A1kH6prNgjLi0AH\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1724705889,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Yes, I do! Your name is Keio Yukichi. How can I assist you today?\",\n","        \"refusal\": null\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 44,\n","    \"completion_tokens\": 20,\n","    \"total_tokens\": 64\n","  },\n","  \"system_fingerprint\": null\n","}\n"]}]},{"cell_type":"markdown","source":["となり、今度は\"Yes, you introduced yourself as Keio Yukichi.\"と回答している\n","\n","A:やB:は、会話しているのがどちらかを示す識別子であり、どのような形でもよい\n","- ただしLangChainは、内部でhumanとAIという用語を利用している\n","\n","このように、会話の過去の履歴を含めて問い合わせを行うため、過去の履歴を記録し、挿入するという処理が必要となる\n","- これがMemory moduleの役割である"],"metadata":{"id":"PuqgDlE5SipG"}},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"KZHfxu9hY62e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["実際に使うには、Chainの中にmemoryとしてConversationBufferMemory()を加えるだけでよい\n","\n","プロンプト(文字を入力するための入力窓)が出てきたら、\n","- Hello. I'm Keio Yukichi\n","- Do you know my name?\n","- EOC\n","と入力する\n","\n","EOCは会話を終了させるおまじないである\n","- これは、ChatGPTの機能ではなく、そのようにプログラムしている"],"metadata":{"id":"Bu3uKzYrYo8a"}},{"cell_type":"code","source":["from langchain.chains import ConversationChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","conversation = ConversationChain(\n","    llm=chat,\n","    memory=ConversationBufferMemory()\n",")\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    ai_message = conversation.predict(input=user_message)\n","    print(f\"AI: {ai_message}\")"],"metadata":{"id":"LjkhKO7nVeEK","executionInfo":{"status":"ok","timestamp":1724705947249,"user_tz":-540,"elapsed":39208,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e57c8c4c-38d7-4afd-cc0e-3533a940aa32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["You: Hello. I'm Keio Yukichi\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hello. I'm Keio Yukichi\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","AI: Hello Keio Yukichi! It's nice to meet you. How can I assist you today?\n","You: Do you know my name?\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hello. I'm Keio Yukichi\n","AI: Hello Keio Yukichi! It's nice to meet you. How can I assist you today?\n","Human: Do you know my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","AI: Yes, you just told me your name is Keio Yukichi. It's a pleasure to have you here. How can I help you today?\n","You: EOC\n"]}]},{"cell_type":"markdown","source":["roleには、system, assistant, userの3つがあることは述べた\n","- systemは設定に利用され、例えばChatGPTにキャラを与えるような命令も存在する\n","\n","assistantとuserについて、assistant はAIからの回答、 user はユーザーからの発話であり、基本的にuserとして聞きたいことや、会話履歴を含めてを送ることになる\n","- この例では、userに、ユーザとChatGPTの両方の会話を入れ込んでいる\n","\n","しかしながら、本来は、assistantがAIからの回答であることから、userにはユーザからの発話履歴のみ、assistantはAIからの発話履歴のみを入れるという形が望ましい\n","- これについては後で説明する\n","\n","その他、様々なMemory moduleが提供されている\n","- ConversationBufferWindowMemory\n","  - ある範囲の会話履歴のみ入力する\n","- ConversationSummaryMemory\n","  - 会話履歴の要約を入力する"],"metadata":{"id":"sva3QIpBVImC"}},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"2JPsQbr0ZJ1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agents"],"metadata":{"id":"C0xTWbzzV4_n"}},{"cell_type":"markdown","source":["LLMが必要に応じて様々なタスクを実行すると便利と思うであろう\n","\n","例えば、\n","- 検索エンジンで検索させる\n","- 実際にコマンドを実行させる\n","- プログラミング言語やスクリプト言語でコードを実行させる\n","\n","これを行うのがAgents moduleである\n","\n","Agentsの利用により、実際にはLLMが何かを操作するわけではないが、LLMが何かしらアプリを操作しているかのように動作させることができる\n","\n","Agentsで操作可能なアプリの例\n","- bash (シェル)\n","- Google Search\n","- IFTTT WebHooks (スマートホーム等)\n","- Python REPL\n","- Requests (他のAPIを叩く)\n","- Wikipedia API"],"metadata":{"id":"xnb4KsklUOeo"}},{"cell_type":"markdown","source":["実際にAgentsを利用してみよう\n"],"metadata":{"id":"qmfqEmF_VsdA"}},{"cell_type":"code","source":["!pip install --quiet langchain-experimental"],"metadata":{"id":"iGJkjbiBfq-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.chat_models import ChatOpenAI\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","tools = load_tools([\"terminal\"], llm=chat, allow_dangerous_tools=True)\n","agent_chain = initialize_agent(\n","    tools, chat, agent=\"zero-shot-react-description\")\n","\n","result = agent_chain.run(\"What is your current directory?\")\n","print(result)"],"metadata":{"id":"sbSTOTuKX7kJ","executionInfo":{"status":"ok","timestamp":1724706145439,"user_tz":-540,"elapsed":2013,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e309da8-a05b-4c17-a545-612fa1faa224"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: What is your current directory?\n","Thought:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mI should use the terminal to check my current directory.\n","Action: terminal\n","Action Input: pwd\u001b[0mExecuting command:\n"," pwd\n","\n","Observation: \u001b[36;1m\u001b[1;3m/content\n","\u001b[0m\n","Thought:\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: What is your current directory?\n","Thought:I should use the terminal to check my current directory.\n","Action: terminal\n","Action Input: pwd\n","Observation: /content\n","\n","Thought:\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:32: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mI now know my current directory is /content.\n","Final Answer: /content\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","/content\n"]}]},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"XbICnbceJ1nO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["単純に/content というディレクトリにいるという回答であるが、実際正解である\n","\n","ただ、ここで疑問が生じる **なぜ、ChatGPTはこちらのシェル環境のカレントディレクトリがわかったのだろうか？**\n","\n","これは、AgentsがMRKL(ミラクル)やReActなどを利用して動作しているためである\n","- MRKL(Multi-Round Knowledge Loop)\n","- ReAct (Reasoning/Acting) なお、Reactではない\n"],"metadata":{"id":"NCkT4uK8fHWE"}},{"cell_type":"markdown","source":["ログをみると、LangChainは次のようなプロンプトを生成している\n","```\n","Answer the following questions as best you can. You have access to the following tools:\n","\n","terminal: Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","```\n","\n","翻訳すると、\n","```\n","次の質問にできるだけ答えてください。あなたは以下のツールにアクセスできる：\n","\n","terminal: このLinuxマシンでシェルコマンドを実行する。\n","\n","以下の書式を使う：\n","\n","Question: あなたが答えなければならない入力問題\n","Thought: 何をすべきかを常に考える。\n","Action: 取るべき行動。[terminal]のどれかであるべき。\n","Action Input: アクションへの入力\n","Observation: 行動の結果\n","...（このThought/Action/Action Input/ObservationはN回繰り返すことができる）\n","Thought: 最終的な答えがわかった\n","Final Answer: 元の入力された質問に対する最終的な答え\n","\n","始める！\n","```\n","\n","となっており、これらの書式を用いて処理が進む\n","```\n","Question: What is your current directory?\n","```\n","という問いかけに対して、\n","```\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","```\n","とChatGPTが返答する\n","\n","そこで、AgentはAction Inputに記載されているコマンドを実行する\n","- その結果を Observationとして埋め込む\n","\n","さらに質問を続けるが、先のプロンプトに加えて、次の文章が加わっている\n","- つまり、これまでの動作をプロンプトに入力している\n","\n","```\n","Question: What is your current directory?\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","Observation: /content\n","\n","Thought:\n","```\n","これに対して\n","```\n","I now know the final answer\n","Final Answer: The current directory is /content.\n","```\n","と回答している\n","\n","Final Answerとして現在のディレクトリは/contentであることが示されており、Final Answerが返されたので、実行を終了している"],"metadata":{"id":"-3eUMCemaz3X"}},{"cell_type":"markdown","source":["では、どんどんやってみよう"],"metadata":{"id":"uJ0Z4_VyZfbU"}},{"cell_type":"code","source":["result = agent_chain.run(\"Make a new directory called 'testdir-by-agent'\")\n","print(result)"],"metadata":{"id":"1sX2ZcN2ZjT1","executionInfo":{"status":"ok","timestamp":1724706162084,"user_tz":-540,"elapsed":1610,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2414cb0-8e89-477f-92bc-2cde7f502266"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: Make a new directory called 'testdir-by-agent'\n","Thought:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mI should use the terminal to create a new directory.\n","Action: terminal\n","Action Input: mkdir testdir-by-agent\u001b[0mExecuting command:\n"," mkdir testdir-by-agent\n","\n","Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n","Thought:\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: Make a new directory called 'testdir-by-agent'\n","Thought:I should use the terminal to create a new directory.\n","Action: terminal\n","Action Input: mkdir testdir-by-agent\n","Observation: \n","Thought:\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:32: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mThe directory 'testdir-by-agent' should have been created successfully.\n","Final Answer: The directory 'testdir-by-agent' has been created.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The directory 'testdir-by-agent' has been created.\n"]}]},{"cell_type":"markdown","source":["ファイルも作ってみよう\n","\n","なお、途中で無料枠の場合はスロットを使い切ってしまうので、ワーニングメッセージと待ちが発生する"],"metadata":{"id":"hOQ0y-iOZ3W7"}},{"cell_type":"code","source":["result = agent_chain.run(\"Create new file called test.txt in the directory of testdir-by-agent and store the text of This is test in the file.\")\n","print(result)"],"metadata":{"id":"16ILSPFfZ486","executionInfo":{"status":"ok","timestamp":1724706172100,"user_tz":-540,"elapsed":2339,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"86ac789f-7ad7-4083-ed11-c12aff8aa541"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: Create new file called test.txt in the directory of testdir-by-agent and store the text of This is test in the file.\n","Thought:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mI need to create a new file and write the text \"This is test\" in it.\n","Action: terminal\n","Action Input: touch testdir-by-agent/test.txt && echo \"This is test\" > testdir-by-agent/test.txt\u001b[0mExecuting command:\n"," touch testdir-by-agent/test.txt && echo \"This is test\" > testdir-by-agent/test.txt\n","\n","Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n","Thought:\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n","\n","terminal - Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: Create new file called test.txt in the directory of testdir-by-agent and store the text of This is test in the file.\n","Thought:I need to create a new file and write the text \"This is test\" in it.\n","Action: terminal\n","Action Input: touch testdir-by-agent/test.txt && echo \"This is test\" > testdir-by-agent/test.txt\n","Observation: \n","Thought:\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_community/tools/shell/tool.py:32: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[32;1m\u001b[1;3mThe file should have been created successfully with the text \"This is test\" in it.\n","Final Answer: The file test.txt with the text \"This is test\" has been created in the directory testdir-by-agent.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The file test.txt with the text \"This is test\" has been created in the directory testdir-by-agent.\n"]}]},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"sVBNXV0IKnbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["testdir-by-agentの下にtest.txtがあり、その中身がThis is a testであることを確認しよう"],"metadata":{"id":"M_GU_jxdadC8"}},{"cell_type":"markdown","source":["### MRKL(Multi-Round Knowledge Loop)\n","\n","例えば、「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」という問いに対して、ChatGPTは答えることができるか？\n","\n","これを直接WebのChatGPTに問い合わせても答えることができない\n","\n","しかしながら、APIでは答えることができる\n","- つまり、APIでChatGPTをアクセスすると、Webとは異なる仕組みでアクセスできるということ\n","\n","では、その手順であるが、まず質問に答えるための手順を考える\n","\n","- 最初に問い。「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」（Question）\n","\n","- Googleで未知の情報を調べるために検索ワードを考える(Thought)\n","\n","- 現在の日本とフランスの首相の年齢を知る必要がある\n","  - 検索ワードは「現在の日本の首相の年齢」(Action Input)\n","  - さらに検索ワードは「現在のフランス首相の年齢」(Action Input)\n","\n","- 検索を実行「現在の日本の首相の年齢」（Action）\n","  - 結果は65歳でした（obsabation)\n","\n","- 検索を実行します「現在のフランス首相の年齢」（Action）\n","  - 結果は61歳でした（obsabation)\n","\n","- 日本の首相の年齢とフランスの首相の年齢の差分を計算する必要がある（Thought）\n","\n","- 計算（Action）\n","\n","- 結果は4でした(obsabation)\n","\n","- 答えは4歳です (final)\n","\n","このようにMRKLは、ChatGPTが情報をもとに次のアクションを考え、結果を評価し、次のアクションを考えるというプロセスを繰り返すことで回答精度を上げる方法論である\n","\n","考察（Thought）、観察（Observation）、行動（Action）のサイクルを繰り返すことで、回答精度が向上する\n","\n","よく言われる、ステップバイステップで考えるように指示すると正答率が上がるのと似ているが、Agent側で実際に実行して応答できるように工夫されている"],"metadata":{"id":"cQzbRtM5hCkG"}},{"cell_type":"markdown","source":["### Prompt Coding\n","\n","プロンプトコーディングはChatGPTの活用において必須となる技術である\n","\n","ChatGPTから精度の高い回答を得るために、人間に質問するのと同様に、質問力が重要であり、その質問の仕方に関する研究が進められている\n","\n","例えば、以下のように役割の指定や回答の形式を細かく設定することで、正答率を上げることができる\n","- 質問や回答が定型化されており、プログラムで文字列を処理することが容易になり、解析が可能となる\n","\n","```\n","あなたは、英語の先生です。これから私の英語を英語教師として文法の誤りを訂正して下ださい。\n","回答のフォーマットは以下のようにします。\n","あなたの英語：{入力分}\n","訂正後の英文:{英文例}\n","文法の解説:{解説1000文字以内}\n","```"],"metadata":{"id":"ya0rvvL2jaAT"}},{"cell_type":"markdown","source":["### 実際のプロンプト\n","\n","先の年齢差を問う問題に答えさせる場合、次のようなプロンプトが想定される\n","- 内容は、シェルを実行するAgentの問い合わせと酷似する\n","\n","```\n","Answer the following questions as best you can.\n","You have access to the following tools:\\n\\n\n","\n","Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n\n","Calculator: Useful for when you need to answer questions about math.\\n\\n\n","\n","Use the following format:\\n\\n\n","\n","Question: the input question you must answer\\n\n","Thought: you should always think about what to do\\n\n","Action: the action to take, should be one of [Search, Calculator]\\n\n","Action Input: the input to the action\\n\n","Observation: the result of the action\\n\n","... (this Thought/Action/Action Input/Observation can repeat N times)\\n\n","Thought: I now know the final answer\\n\n","Final Answer: the final answer to the original input question\\n\\n\n","\n","Begin!\\n\\n\n","\n","\n","Question: 現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？ 計算してください\\nThought:')\n","```\n","\n","最初の\"Use the following format:\\n\\n\" 以前について、\n","\n","ここで、toolsについて何がどのように利用できるかを伝えているが、重要な点は次の通りである\n","\n","- Searchというツール名\n","  - コロンの前にツール名が記載されている\n","- ユースケースを伝える\n","  - (時事問題に関する質問に答える必要があるときに便利です)\n","- 入力形式を指定する\n","  - (入力は、検索クエリである必要がある)\n","\n","「入力は検索クエリである必要がある」と伝えているため、半角スペース区切りの単語単位での検索クエリを作成するようになる\n","- ChatGPTは時事問題に関する内容は、Searchツールを使うようになる\n","\n","最初の\"Use the following format:\\n\\n\" 以降について\n","\n","進め方とフォーマットを伝えている\n","\n","- Question:質問内容を記載\n","- Thought:何をすべきかを常に考える必要がある\n","  - アクションを考えるように指示\n","- Action: 実行するアクションは、 [Search, Calculator]のいずれかである必要がある\n","  - アクション名はツール名と同じであり、ChatGPTからActionの指示が出る際には[Search,Calculator]のキーワードが出力される\n","- Action Input: アクションへの入力\n","  - Searchの場合は指示されたクエリ形式で入力する\n","- Observation:Actionの結果\n","  - アクションの結果を表示\n","\n","さらに、最後について\n","\n","- (this Thought/Action/Action Input/Observation can repeat N times)\n","  - N回繰り返すは、答えが出ない場合に打ち切る回数や、API利用料金を抑えるための制限回数として、Nを指定できるようにしている\n","- Thought:回答が判明したら下記に進みます\n","- Final Answer:最終的な回答をします"],"metadata":{"id":"3u5jaB1ikP5o"}},{"cell_type":"markdown","source":["実際に試行すると次のような結果を得ることができる\n","\n","```\n","> Entering new AgentExecutor chain...\n","I need to find out the age of the current Japanese and French Prime Ministers\n","Action: Search\n","Action Input: \"age of current Japanese Prime Minister\"params\n","\n","Observation: 65歳\n","Thought: Now I need to find out the age of the current French Prime Minister\n","Action: Search\n","Action Input: \"age of current French Prime Minister\"params\n","\n","Observation: 61歳\n","Thought: I now know the final answer\n","Final Answer: 4歳\n","```\n","\n"],"metadata":{"id":"3BoivuKFnmUn"}},{"cell_type":"markdown","source":["##  Chat API におけるプロンプトの構築"],"metadata":{"id":"bqrKxwnhlstV"}},{"cell_type":"markdown","source":["先のmemoryの例に加えて、\n","`import openai`\n","\n","および\n","\n","`langchain.verbose = True`\n","\n","を追加して、ログを詳細に取得する\n","\n","プロンプトに対して、\n","- Hi, I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC\n","\n","と入力する\n"],"metadata":{"id":"XNIICFODuMO_"}},{"cell_type":"code","source":["from langchain.chains import ConversationChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","import openai\n","\n","langchain.verbose = True\n","openai.log = \"debug\"\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","conversation = ConversationChain(\n","    llm=chat,\n","    memory=ConversationBufferMemory()\n",")\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    ai_message = conversation.predict(input=user_message)\n","    print(f\"AI: {ai_message}\")"],"metadata":{"id":"jX1cLM9FJfbe","executionInfo":{"status":"ok","timestamp":1724706198303,"user_tz":-540,"elapsed":26216,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"892e6093-1161-4b34-a961-20877bde2a70"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["You: Hi, I'm Keio Yukichi.\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, I'm Keio Yukichi.\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","AI: Hello Keio Yukichi! It's nice to meet you. How can I assist you today?\n","You: Do you know my name?\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, I'm Keio Yukichi.\n","AI: Hello Keio Yukichi! It's nice to meet you. How can I assist you today?\n","Human: Do you know my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","AI: Yes, you just told me your name is Keio Yukichi. It's a pleasure to have you here. How can I help you today?\n","You: EOC\n"]}]},{"cell_type":"markdown","source":["ログを参照することで、動作の詳細を獲得できる\n","\n","例えば、Memoryにより過去の履歴をプロンプトを与えることができるが、具体的には次のような動作をしている\n","\n","```\n","api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\nHuman: Hi, I\\'m Keio Yukichi.\\\\nAI: Hello Keio Yukichi! How can I assist you today?\\\\nHuman: Do you know my name?\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n","```\n","\n","このように、すべてuserメッセージとして混入している\n","\n","本来は、ChatGPTの言葉は、assistantとして入力するべきであろうことがわかる"],"metadata":{"id":"Ke23qQpWvIZM"}},{"cell_type":"markdown","source":["そこで、これを使い分けるには、次のようにする\n","\n","SystemMessage, HumanMessage、またAIMessageを用いて、それぞれの会話を仕分けできる"],"metadata":{"id":"Nv6G19MewSJN"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import (\n","    AIMessage,\n","    HumanMessage,\n","    SystemMessage\n",")\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","\n","messages = [\n","    SystemMessage(content=\"You are a helpful assistant.\"),\n","    HumanMessage(content=\"Hi! I'm Keio Yukichi!\"),\n","    AIMessage(content=\"Yes, You are Keio Yukichi.\")\n","]\n","\n","result = chat(messages)\n","print(result)"],"metadata":{"id":"X90oA5kIPGYG","executionInfo":{"status":"ok","timestamp":1724706199075,"user_tz":-540,"elapsed":773,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3daaa1e6-94e5-4ab2-f824-8160b4b169fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["content='How can I assist you today, Keio Yukichi?' response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 39, 'total_tokens': 51}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a06884e5-bb70-4a0c-a741-8a86b73e0e54-0'\n"]}]},{"cell_type":"code","source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"],"metadata":{"id":"wij-UYXMLZzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["これを踏まえて、先ほどのループ問い合わせプログラムを改善する\n","\n","```\n","    memory.chat_memory.add_user_message(user_message)\n","    ai_message = chat(memory.chat_memory.messages)\n","    memory.chat_memory.add_ai_message(ai_message.content)\n","```\n","とすることで、memoryに対してだれの発言かを仕分けして登録するようにする\n","\n","実際に実行して、次のようにプロンプトに入力する\n","- Hi. I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC\n","\n","ログを見てみると、Do you know my name? の問い合わせの後、\n","\n","`\"messages\": [{\"role\": \"user\", \"content\": \"Hi. I\\'m Keio Yukichi.\"}`とuserが入力した後、`{\"role\": \"assistant\", \"content\": \"Hello Keio Yukichi! How can I assist you today?\"}'とassistantが返答、さらに`{\"role\": \"user\", \"content\": \"Do you know my name?\"}`とuserが入力といった具合に、正しく仕分けされている\n"],"metadata":{"id":"GamQYH-iwucs"}},{"cell_type":"code","source":["langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","memory = ConversationBufferMemory()\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    memory.chat_memory.add_user_message(user_message)\n","    ai_message = chat(memory.chat_memory.messages)\n","    memory.chat_memory.add_ai_message(ai_message.content)\n","    print(f\"AI: {ai_message.content}\")\n"],"metadata":{"id":"gZAE9dogmna_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f057ebb-5cd5-4420-d331-eab83f324df0"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["You: Hi. I'm Keio Yukichi.\n","AI: Hello Keio Yukichi! How can I assist you today?\n","You: Do you know my name?\n","AI: Yes, you introduced yourself as Keio Yukichi at the beginning of our conversation. How can I help you today, Keio Yukichi?\n","You: EOC\n"]}]},{"cell_type":"markdown","source":["ここで一度実行を終了する\n","- 以降は「ランタイム」から「以降のセルを実行する」を選択して実行するとよい\n","- もしくは一つ一つクリックして実行すること"],"metadata":{"id":"CApGZbWFGkaZ"}},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"5j0TQZvcMZU5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Embedchain\n","\n"],"metadata":{"id":"8OnF6qsbDG1t"}},{"cell_type":"markdown","source":["Embedchainは、どのようなデータセットでも簡単にLLMボットを作成できるフレームワークである\n","\n","- CSVやExcel、テキストファイルに加えて、YouTube動画など、様々なフォーマットを扱うことができる\n","- バックエンドで、ChatGPT API、OpenAIのembeddingとLangChain、ベクトルデータベースにChromaを利用している\n","\n"],"metadata":{"id":"vqTZatQcJef3"}},{"cell_type":"markdown","source":["## ライブラリのインストール"],"metadata":{"id":"-VQE9nGwDaiG"}},{"cell_type":"markdown","source":["次のセルでlangchainをインストールするが、dependancyでエラーが生成されるかもしれない\n","- (2023/12) embedchainの都合により、バージョンを指定して導入する"],"metadata":{"id":"kOk3SJEGu1iF"}},{"cell_type":"code","source":["!pip install --quiet langchain==0.0.336"],"metadata":{"id":"lVxZFGSagE1C","executionInfo":{"status":"ok","timestamp":1724706321184,"user_tz":-540,"elapsed":22906,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"38095ab8-3944-482c-d303-26d44cb6399d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip uninstall -y --quiet tensorflow-probability"],"metadata":{"id":"FM6ubk9KD34G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet openai cohere tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724706364954,"user_tz":-540,"elapsed":37211,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"d05c3986-bd26-4a78-d610-81652f1c7189","id":"YXf72d02D34G"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m358.4/362.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install --quiet chromadb kaleido python-multipart"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724706410142,"user_tz":-540,"elapsed":45197,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a9b7114-4cf3-4f99-b9e8-9f8a22a5bb14","id":"TIS0VkUXD34H"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install --quiet --upgrade embedchain"],"metadata":{"id":"3iuoCtmFMbFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724706455083,"user_tz":-540,"elapsed":44951,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"bd73df41-475a-4a30-90a3-554556240a88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/210.9 kB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install --quiet pydantic"],"metadata":{"id":"jpuq7ieeMPAH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["必要なライブラリをインポートする"],"metadata":{"id":"DrrlU4QqDmxl"}},{"cell_type":"code","source":["import os\n","from embedchain import App"],"metadata":{"id":"ohPMkz4zDqxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["APIキーは、先に指定したキーを利用する点に注意する\n","\n","次のようにして、直接キーを指定してもよい\n","\n","```\n","os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n","\n","```"],"metadata":{"id":"JAO_SzlyDr5x"}},{"cell_type":"code","source":["!pip install python-dotenv"],"metadata":{"id":"v9lhoy9vLiQm","executionInfo":{"status":"ok","timestamp":1724706477318,"user_tz":-540,"elapsed":5718,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9489cd81-fbc1-4400-8343-ac3a95645d36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"]}]},{"cell_type":"markdown","source":["次のコードでTrueと出ればよいが、Falseと表示された場合は、戻って最初の方にある\"!echo ...\"のセルをもう一度実行するとよい"],"metadata":{"id":"YrhIHw2LHAtU"}},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"],"metadata":{"id":"nyRo07nqD08l","executionInfo":{"status":"ok","timestamp":1724706477318,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a661b32-a7bf-4a9b-c1a5-20924639799e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["embechainボットを起動する\n","- ここでは、ChatGPTを起動している\n","- Llama2App()とすると、Llama2が起動する\n","- その他、CustomAppなどを用いることで様々なLLMモデルに対応する"],"metadata":{"id":"k-SllFyqNIB1"}},{"cell_type":"code","source":["elon_bot = App()"],"metadata":{"id":"v6NHtXp6NCsR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次に、embedchainの `.add()` メソッドを使って異なる種類のデータソースを追加する\n","\n","- `\"pdf_file\"`でPDFファイルをURLなどで追加できる\n","- その他の対応フォーマットについては、https://docs.embedchain.ai/advanced/data_types を参照のこと\n"],"metadata":{"id":"WRKWFWSGD1tB"}},{"cell_type":"code","source":["!pip install --quiet youtube-transcript-api\n","!pip install --quiet pytube"],"metadata":{"id":"6hlPK-AlIGnA","executionInfo":{"status":"ok","timestamp":1724706537440,"user_tz":-540,"elapsed":8099,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3af46d63-30c1-44a6-b591-a7bfab072cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["elon_bot.add(\"web_page\", \"https://en.wikipedia.org/wiki/Elon_Musk\")\n","elon_bot.add(\"youtube_video\", \"https://www.youtube.com/watch?v=kwsTrVoCfSQ\")"],"metadata":{"id":"MhByl8oxEJFH","executionInfo":{"status":"ok","timestamp":1724706547638,"user_tz":-540,"elapsed":10201,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":159},"outputId":"a6584a9f-dc8b-40bc-84b1-2dd26ea5bb42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:embedchain.embedchain:Starting from version v0.0.40, Embedchain can automatically detect the data type. So, in the `add` method, the argument order has changed. You no longer need to specify 'web_page' for the `source` argument. So the code snippet will be `.add(\"https://en.wikipedia.org/wiki/Elon_Musk\", \"web_page\")`\n","WARNING:embedchain.embedchain:Embedchain is swapping the arguments for you. This functionality might be deprecated in the future, so please adjust your code.\n","Inserting batches in chromadb: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n","WARNING:embedchain.embedchain:Starting from version v0.0.40, Embedchain can automatically detect the data type. So, in the `add` method, the argument order has changed. You no longer need to specify 'youtube_video' for the `source` argument. So the code snippet will be `.add(\"https://www.youtube.com/watch?v=kwsTrVoCfSQ\", \"youtube_video\")`\n","WARNING:embedchain.embedchain:Embedchain is swapping the arguments for you. This functionality might be deprecated in the future, so please adjust your code.\n","Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'51272b8de040aeb88e1c1d3f88532503'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["ボットの準備ができたので、`.query()`メソッドを使ってボットに質問する\n","\n","なお、ここでは、queryを用いているが、2つのインタフェースがある\n","\n","- クエリー・インターフェース\n","  - このインターフェイスは質問に答えるボットであり、質問を受け、その答えを取得する\n","  - 過去のチャットに関するコンテキストは保持しない\n","  - `.query()`関数を呼び出して、任意のクエリの答えを取得できる\n","\n","- チャット・インターフェース\n","  - 過去の会話を記憶するチャット・インターフェイスであり、デフォルトでは過去5つの会話を記憶している\n","  - `.chat`関数を呼び出して問い合わせの答えを取得できる\n","\n","- ドライ・ラン\n","  - 追加、クエリー、チャットの各メソッドにあるオプションで、LLMに送信せず、生成されたプロンプトを表示することができまる\n","\n","\n","```\n","chatmsg = naval_chat_bot.query('Can you xxxxxx?', dry_run=True)\n","```\n","\n","- ストリーム応答\n","ChatGPT のようにレスポンスをストリームするために、クエリメソッドに追加するオプション\n","- チャンクを希望のフォーマットでレンダリングするには、ダウンストリームハンドラが必要となる\n","- OpenAIモデルとOpenSourceAppの両方をサポートする\n","\n","```\n","app = App()\n","query_config = QueryConfig(stream = True)\n","resp = app.query(\"What is ****** ?\", query_config)\n","\n","```\n","\n","- リセット\n","  - `reset`でデータベースをリセットし、すべての埋め込みを不可逆に削除する\n","\n","- カウント\n","  - `count`でデータベース内の埋め込み（チャンク）の数を数えます。"],"metadata":{"id":"aeTOQettEfWT"}},{"cell_type":"code","source":["elon_bot.query(\"How many companies does Elon Musk run?\")"],"metadata":{"id":"HZnx0V18EkzS","executionInfo":{"status":"ok","timestamp":1724706549083,"user_tz":-540,"elapsed":1456,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"c91efb55-044d-40a6-e5c4-4224aee1208d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:embedchain.embedchain:Starting from v0.1.125 the return type of query method will be changed to tuple containing `answer`.\n"]},{"output_type":"execute_result","data":{"text/plain":["'Elon Musk runs multiple companies, including SpaceX, Tesla, Neuralink, and The Boring Company.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# Gradioについて"],"metadata":{"id":"vy5mm7Trmjh4"}},{"cell_type":"markdown","source":["***ここからは、まとめて実行せず、つ一つ一つクリックして実行した方が良い***\n"],"metadata":{"id":"SY5aPDLRFzfE"}},{"cell_type":"markdown","source":["Webアプリを簡単に実装できるPythonライブラリ\n","\n","百聞は一見に如かずということで、早速実行してみよう"],"metadata":{"id":"LHpGhwMab4H1"}},{"cell_type":"markdown","source":["## gradio のインストール方法\n","\n","```\n","pip install gradio\n","```\n","\n","とし、例えば、\n","\n","```\n","import gradio as gr\n","```\n","とすることで利用可能となる\n","- (2023/12) ここでは互換性のために3.48.0を導入する"],"metadata":{"id":"jhssf_w9moNv"}},{"cell_type":"code","source":["!pip -q install gradio==3.48.0"],"metadata":{"id":"mWx4JDa-MGDa","executionInfo":{"status":"ok","timestamp":1724706564369,"user_tz":-540,"elapsed":15289,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df870dee-042c-4bfa-b8f5-94a0c496c6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["シンプルなWeb UIを作成して起動させる例を示す\n","- 名前を入力し、名前へのあいさつを出力するWeb UIである\n","\n","Colab上で実行すると、Colabのwebに統合される\n","- URLが出力されている通り、そのURLにアクセスできる環境にあれば、webのページとして表示される\n","- この例では、`https://localhost:7860/`などと表示されているが、クリックすると実は`https://wq0lbccui9-496ff2e9c6d22116-7860-colab.googleusercontent.com/`に転送されており、ネットワークセキュリティ上隔離されたColabの外からアクセスできるようになる\n","- Colab環境では、セキュリティ上の問題もあり、このようなグローバルアドレスが提供されない場合は、別途ボートフォーワーディングなどの知識が必要な場合がある\n","- また、ローカル上で他のWeb UIアプリを動作させているなどにより、ポートが競合する可能性がある\n","  - この場合、7860 が 7861 や 7862 といった番号に変わることがある\n","\n","\n","また、gradio clientを用いることで、作成したwebアプリケーションをweb APIのように利用することもできる\n","- 下に小さく「Use via API」と記載されているが、これをクリックし、記載の通りに実行すると動作がわかるであろう\n","·"],"metadata":{"id":"miQy3qBhcQNz"}},{"cell_type":"code","source":["import gradio as gr\n","\n","# あいさつの関数\n","def greet(name):\n","    return \"Hello \" + name + \"!\"\n","\n","# Interfaceの作成\n","demo = gr.Interface(\n","    fn=greet,\n","    inputs=\"text\",\n","    outputs=\"text\"\n",")\n","\n","# 起動\n","demo.launch()"],"metadata":{"id":"O8KX4wgicW2Z","executionInfo":{"status":"ok","timestamp":1724706570220,"user_tz":-540,"elapsed":5860,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"5f974f8e-e830-4edc-f0e1-b89a0e444223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://dea1ffdd63286b9afa.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://dea1ffdd63286b9afa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## 設計手順\n","\n","次の手順となる\n","- コールバック関数を定義\n","- レイアウトを定義\n","- WebUIの起動\n","\n","それぞれについて概要をみてみよう"],"metadata":{"id":"lslskTPQflov"}},{"cell_type":"markdown","source":["\n","### コールバック関数\n","\n","まず、コールバック関数として、画面レイアウトでボタンが押された時に呼び出したい関数を記述する  \n","\n","例えば、名前(text)、表示フラグ(boolean)、値(0から100の値)の3つを受け取る関数として次のような関数を想定する  \n","```\n","def my_func(my_name, is_disp, my_value):\n","    return f'### {my_name} ###', my_value * 100\n","```\n","\n"],"metadata":{"id":"ZKnVtHYXgNiu"}},{"cell_type":"markdown","source":["### Interface\n","\n","「Interface」は、関数をUIでラップするためのクラスであり、主なパラメータは次のとおり\n","- fn : ラップする関数\n","- inputs : 入力コンポーネント (\"text\"、\"image\"、\"audio\"など)\n","- outputs : 出力コンポーネント (\"text\"、\"image\"、\"label\"など)\n","\n","画面レイアウトの作成として、画面に表示したいUIパーツと、ボタンなどが押された時のアクション(呼ぶ出したいコールバック関数の名前)を記述する  \n","- 入力UIとして、my_nameは\"text\"であり、is_dispは\"checkbox\"であり、my_valueは、0から100の値をスライダーで入力させるとすると、gr.Slider(0, 100)となり、上からこの順に入力UIがレイアウトされる\n","- 出力UIとして、ここでは、文字と数字を想定する\n","\n","最終的に、次のような関数定義となる  \n","```\n","demo = gr.Interface(\n","    fn = my_func,\n","    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n","    outputs=[\"text\", \"number\"],\n",")\n","```\n","\n","なお、\"text\"に対して、より詳細な情報を与えることもできる\n","\n","```\n","demo = gr.Interface(\n","    fn=my_func,\n","    inputs=[\n","      gr.Textbox(\n","        lines=2,  # 行数\n","        placeholder=\"Name Here...\"  # プレースホルダ\n","      ),\n","      \"checkbox\", gr.Slider(0, 100)\n","    ]\n","    outputs=[\"text\", \"number\"],\n",")\n","\n","```"],"metadata":{"id":"2PxgV9NKnkjK"}},{"cell_type":"markdown","source":["\n","- 最後にWebUIを起動する  \n","シンプルに、  \n","```\n","demo.launch()\n","```\n","とするだけでよい\n","\n","inputs には「クリアボタン」と「送信」ボタンが、outputs には「フラグする」ボタンが自動で追加される(フラグボタンは出力をローカルファイルに保存する)\n","\n","簡易的なWebサーバが起動し、ブラウザ上でWeb UIが表示される\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/gradio1.jpg\" width=700>\n"],"metadata":{"id":"XjaSRRUCgmsO"}},{"cell_type":"markdown","source":["## より実践的な例\n","\n","他にも様々な機能があるため、調べてみるとよい\n","\n","```\n","import gradio as gr\n","\n","# コールバック関数の定義\n","def callback_func(val1,val2,val3):\n","    return str(int(val1) * int(val2)), f\"気温は {val3} 度です\"\n","\n","# 画面レイアウトの定義(Interfaceを使用)\n","app = gr.Interface(\n","    title=\"計算機\",\n","    fn=callback_func,\n","    inputs=[\n","        gr.Textbox(label=\"入力欄1\",lines=3, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Textbox(label=\"入力欄2\",lines=5, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Slider(label=\"温度\",minimum=0,maximum=100,step=1)\n","    ],\n","    outputs=[\n","        gr.Label(label=\"計算結果1\",lines=3),\n","        gr.Textbox(label=\"計算結果2\",lines=3)\n","    ]\n","    )\n","\n","# Web UIの起動\n","app.launch(inbrowser=True)\n","```\n","\n","\n"],"metadata":{"id":"fcPpqmEnt4qW"}},{"cell_type":"markdown","source":["### Blocks\n","\n","簡易的に使用する場合はInterfaceを使い、より複雑なレイアウトを作る場合はBlocksを使う\n","\n","- Interfaceのメリット\n","\n","  - ショートカット文字列の使用が可能(Blocksでは使用不可)\n","  - Interfaceでは基本的なボタンを自動生成\n","\n","- Blocksのメリット\n","\n","  - Blocksでのみ利用可能なレイアウトがある\n","  - レイアウトが複雑になった場合でもwith文で可読性の高いコードが書ける\n","\n","Blocksではレイアウトを指定でき、次のようなレイアウトがある\n","\n"],"metadata":{"id":"WXViJMW_oVc5"}},{"cell_type":"markdown","source":["#### コンポーネントを横や縦に並べる  \n","gr.Row()やgr.Column()を利用する\n","\n","コード中にコメントがあるので、切り替えてみるとよい"],"metadata":{"id":"cGy7qHvAtH4x"}},{"cell_type":"code","source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app:\n","  with gr.Row():\n","#  with gr.Column(scale=2):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\")\n","    # イベントハンドラー\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"],"metadata":{"id":"iQxV_PT9rq2P","executionInfo":{"status":"ok","timestamp":1724706572108,"user_tz":-540,"elapsed":1892,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"40411141-ffbc-43d4-dd13-b4d83c448461"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://35ebefa93d7b28996e.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://35ebefa93d7b28996e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#### タブで表示する  \n"],"metadata":{"id":"4Itelzr_sjVE"}},{"cell_type":"code","source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # 入力タブを定義\n","  with gr.Tab(\"入力タブ\"):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    btn = gr.Button(\"クリックしてね!\") # 出力タブを定義\n","  with gr.Tab(\"出力タブ\"):\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"],"metadata":{"id":"pyByV3fYsmGQ","executionInfo":{"status":"ok","timestamp":1724706573876,"user_tz":-540,"elapsed":1772,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"f812b5c5-bff5-418b-b6d8-3570266298d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://6a9bdd29c8181ceb32.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://6a9bdd29c8181ceb32.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["#### 丸角の子要素を設ける\n","\n","角が丸く、周囲にパディングがあるボックスである\n","- 以前はgr.Boxであったが、gr.Blocksに変更となった"],"metadata":{"id":"nkv4_Mj4s_qW"}},{"cell_type":"code","source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Box()関数でレイアウトを定義\n","  with gr.Group():\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"],"metadata":{"id":"V1IMxhLDtS9j","executionInfo":{"status":"ok","timestamp":1724706575746,"user_tz":-540,"elapsed":1873,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"4ed62015-3ba0-4e21-854c-3c2503995e19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://26f94c4910c32d7dd3.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://26f94c4910c32d7dd3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["#### 子要素を折りたたみ可能にする\n","\n","Accordionは、子要素を折りたたみ可能なセクションに配置する\n","\n","openパラメータにより初期状態で開いているか(True)、閉じているか(False)を指定でき、デフォルトはOpen(True)\n"],"metadata":{"id":"NnTwV4PrtfLK"}},{"cell_type":"code","source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Accordion()関数でレイアウトを定義\n","  with gr.Accordion(label=\"アプリを見る\", open=False):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"],"metadata":{"id":"wCdTBWZduNgD","executionInfo":{"status":"ok","timestamp":1724706577369,"user_tz":-540,"elapsed":1627,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"c342cd6f-6bb2-4ef4-88ce-d124cae85292"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://3ba7735f7a9bd60092.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://3ba7735f7a9bd60092.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["### チャットを実装する\n","\n","なんでもなく、\"How are you?\", \"I know you\", \"I'm very hungry\"のどれかをランダムに答えるアプリである"],"metadata":{"id":"HiEG9ZpVpKGJ"}},{"cell_type":"code","source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I know you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"],"metadata":{"id":"8hAUuk-oo47w","executionInfo":{"status":"ok","timestamp":1724706578923,"user_tz":-540,"elapsed":1557,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"b2874f4d-9430-48d5-8d2f-6e8d615ed0c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n","--------\n","Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://73ac94d3ec55fadd3d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://73ac94d3ec55fadd3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["(2023/12) 異常終了するため、ここで一度実行をとめる"],"metadata":{"id":"JRBnfYGzKeAL"}},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"GmUiESaiCiAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LLMチャットの実装\n","\n"],"metadata":{"id":"WKL_t_xT02BX"}},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"],"metadata":{"id":"zO27LB3bP-Sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y --quiet tensorflow-probability"],"metadata":{"id":"IErBgXXnKG1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet openai cohere tiktoken"],"metadata":{"id":"7cAsOhXoKG1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet chromadb kaleido python-multipart"],"metadata":{"id":"_pTuyW9NKG1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet langchain-experimental"],"metadata":{"id":"bJNxN6pbKG1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip -q install gradio==3.48.0"],"metadata":{"id":"BRDSFHfIKute"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import langchain\n","from langchain.chat_models import ChatOpenAI\n","\n","langchain.verbose = True\n","\n","def chat(message: str) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","    return llm.predict(message)\n"],"metadata":{"id":"BgGEEYyY1KxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["まずは、フレームワークとして、似非チャットを実装する\n","- ランダムに答えを返す"],"metadata":{"id":"KYCXt_wRU6bb"}},{"cell_type":"code","source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"],"metadata":{"id":"vinBiFGT1N6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["次に、過去の会話履歴を踏まえて回答するように chat関数を次のように更新する\n"],"metadata":{"id":"GoYskNFqH4ud"}},{"cell_type":"code","source":["import langchain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ChatMessageHistory\n","from langchain.schema import HumanMessage\n","\n","langchain.verbose = True\n","\n","def chat(message: str, history: ChatMessageHistory) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","    messages = history.messages\n","    messages.append(HumanMessage(content=message))\n","\n","    return llm(messages).content"],"metadata":{"id":"igs39-zMH-b4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["実際に試してみよう\n","\n","- 私の名前は出田です。\n","- 私の名前がわかりますか？\n","\n","と入力すると、\n","「はい、先ほど出田さんとおっしゃいましたよね。」\n","といった回答になる"],"metadata":{"id":"ayaQTKNLVQUu"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import AIMessage, HumanMessage\n","import openai\n","import gradio as gr\n","\n","llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n","\n","def predict(message, history):\n","    history_langchain_format = []\n","    for human, ai in history:\n","        history_langchain_format.append(HumanMessage(content=human))\n","        history_langchain_format.append(AIMessage(content=ai))\n","    history_langchain_format.append(HumanMessage(content=message))\n","    gpt_response = llm(history_langchain_format)\n","    return gpt_response.content\n","\n","gr.ChatInterface(predict).launch()"],"metadata":{"id":"JH7cYHuOSLio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PDFドキュメントの内容をLangChainを用いて問い合わせる\n","\n","ChatGPTには一度に扱えるテキストの量に限界があり、一度に入力させることはできない\n","\n","そこで、巨大PDFの内容からテキストを抽出し、分割、テキスト間の関連性もつベクトルデータをベクターストアに格納する\n","\n","\n"],"metadata":{"id":"lOIsJaGGdj1y"}},{"cell_type":"code","source":["!pip install --quiet openai chromadb langchain pypdf tiktoken"],"metadata":{"id":"vYYsMYHbeVsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["必要となるライブラリを読み込む"],"metadata":{"id":"YxsdRd6fi0b2"}},{"cell_type":"code","source":["import os\n","import platform\n","\n","import openai\n","import chromadb\n","import langchain\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.document_loaders import PyPDFLoader"],"metadata":{"id":"_LzkJuKBeee0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["API-KEYを読み込む\n","- エラーになる場合は、このテキストの最初にあるAPIキー設定箇所を再実行すること"],"metadata":{"id":"vfuM2_fii3k1"}},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"],"metadata":{"id":"VB17dhj0f_BW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["テスト用PDFとして、米国CLOUD法のホワイトペーパーを利用する"],"metadata":{"id":"KhbAaMY-jEr4"}},{"cell_type":"code","source":["if not os.path.exists('wp.pdf'):\n","  !wget https://www.justice.gov/criminal-oia/page/file/1153436/download -O wp.pdf"],"metadata":{"id":"YbX0gqVhelI5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PDFローダでテキスト化して読み込み、読分割する"],"metadata":{"id":"xgkvpakBjKHD"}},{"cell_type":"code","source":["loader = PyPDFLoader(\"wp.pdf\")\n","pages = loader.load_and_split()"],"metadata":{"id":"Cc2W5lgNej0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5ページ目の文章を確認してみよう"],"metadata":{"id":"eRBuWsaBjVS9"}},{"cell_type":"code","source":["pages[5].page_content"],"metadata":{"id":"kWOUsB8DfKuw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ChatGPTを準備する"],"metadata":{"id":"tNIT8POUjbHM"}},{"cell_type":"code","source":["openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"],"metadata":{"id":"y3blnhosfQ5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PDF文章をembeddingしてベクターストアに登録する\n","\n","このベクターストアを LLM に与えることで、テキスト間の関連性について表現したベクトルデータを外部から与え、LLMで利用できるようになる"],"metadata":{"id":"OMEsL8wCjcsg"}},{"cell_type":"code","source":["embeddings = OpenAIEmbeddings()\n","vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=\".\")\n","vectorstore.persist()"],"metadata":{"id":"OCIHq40PgeW5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PDF ドキュメントへ自然言語で問い合わせる\n","\n","ここでは、回答文の作成に関連した元テキスト群についても示すように指定する"],"metadata":{"id":"3MSvZvvXjh-W"}},{"cell_type":"code","source":["pdf_qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"],"metadata":{"id":"cdyjX_OIgjL9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["英語で US クラウド法とは何か？について問い合わせてみよう\n","- 回答は日本語を用いるように指示する"],"metadata":{"id":"v8FvaEGDj2EB"}},{"cell_type":"code","source":["query = \"What is US Cloud Act? Answer within 50 words in Japanese.\"\n","chat_history = []\n","\n","result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n","\n","result[\"answer\"]"],"metadata":{"id":"Y3zyM2nWidDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["上記の回答に関連した元のテキスト群について確認する"],"metadata":{"id":"NuhpAlr1kCPk"}},{"cell_type":"code","source":["result['source_documents']"],"metadata":{"id":"RIVIoNhCgx-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["US クラウド法によってどういう主体が影響を受けるか問い合わせる"],"metadata":{"id":"5ewuvirBkIov"}},{"cell_type":"code","source":["chat_history = [(query, result[\"answer\"]), (query, result[\"answer\"])]\n","query2 = \"CLOUD法によって影響を受ける主体にはどういったものがありますか？日本語で回答してください。\"\n","\n","result2 = pdf_qa({\"question\": query2, \"chat_history\": chat_history})\n","\n","result2[\"answer\"]"],"metadata":{"id":"LCIPxwSxg4Gz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ChatGPT による回答の正確性については保証されていないが、難解な資料を素早く理解するにはかなり有効であるといえよう"],"metadata":{"id":"Ue-g0X-0kNJ5"}},{"cell_type":"markdown","source":["# その他トピック\n","\n","ここ3ヵ月で次の通り、追いかけるのも大変\n","\n","## DALL-E3\n","画像生成AIの中でも高精度・高品質画像を作成可能なAI\n","\n","## SDXL Turbo\n","リアルタイムでプロンプトに反応して画像生成\n","https://clipdrop.co/ja/stable-diffusion-turbo\n","\n","## Adobe Firefly\n","ようやくAdobeも動きだした著作権・商用利用問題クリアの画像生成AI\n","\n","## GPTsとCopilot Studio\n","調査してみよう\n","\n","## Notion AIやCursorなどのLLM統合\n","- Notionは様々な機能を持つメモ帳\n","  - AI拡張は有料\n","- Cursorはプログラミング用統合環境\n","  - VSCodeオワコンといわれている\n","\n","## Claude2.1\n","- ChatGPT並み(かそれ以上)の制度を持つLLM\n","\n","## Google Gemini\n","\n","ChatGPTを超えた？といわれているLLM\n","- まだ利用できないためデモ画面での判断しかできない\n","\n","## Runway Gen-2\n","\n","画像から高精細かつ高品質な動画を生成\n","- 動かしたいものを指定したり、プロンプトで指示することができる\n"],"metadata":{"id":"OXsU26yn9VP4"}},{"cell_type":"markdown","source":["# 課題1(LangChain)\n","\n","上記のPDFファイル解析アプリケーションの例を、LangChainを用いたチャットボット形式に修正しなさい"],"metadata":{"id":"EXKfzAT0TmDV"}},{"cell_type":"markdown","source":["# 課題2(LangChainによるチューニング)\n","\n","履歴付きチャットボットを改造し、常に回答が子供の対応であるようにプロンプトエンジニアリングを行いなさい\n","\n","単純に、入力問い合わせに対して、「考え方や言葉遣いを6歳の子供のようにして答えなさい」といった言葉を付け加えなさい"],"metadata":{"id":"OEnQXDE_G8Ju"}},{"cell_type":"markdown","source":["# 課題3(Embedchain)\n","\n","Embedchainを用いて、何かしら専門的な内容に対して回答可能とするチャットボットを実装しなさい\n","\n","- ChatGPTへの質問結果も示し、実際に回答が改善されていることを示しなさい。\n","- インプットトークンが足りないというエラーが出た場合は、次の方法によりGPT4への切り替えなさい\n","  - gpt4を指定したconfigファイルを作成・配置する\n","  - [推奨] App()で初期化する際に `App.from_config(config_path=\"config_file.yaml\")`とする"],"metadata":{"id":"jg_-NJjO89GY"}},{"cell_type":"markdown","source":["# 課題4(GPTs)\n","\n","ChatGPTのGPTsを試しなさい\n","\n","この課題を解くには、ChatGPT4を利用する必要があるため、課金が伴う点に注意しなさい\n","\n","例えば、\n","\n","- 政府発行の文章や、博物館などの解説ページなどの情報を参照し、専門的な内容に回答するチャットを作成する\n","  - 会社の内部文章などが面白いが、普通に規定でできないであろう\n","  - 仮想的にそのような「公開されていない情報」(自分の日記など)について試すとよい\n","- 日本語で必ず回答するようにする\n","- 発言の最後に、専門用語についての解説を付与するようにする\n","\n","など\n","\n","また、GPTsにおけるAPI操作機能を用いて、気温や湿度などを住所から入手し、適切な服装などを指示するGPTを作成しなさい\n","\n","https://weather.tsukumijima.net/ などを利用するとよい\n","\n","さらに、HotPepper APIを用いて、お店を検索するGPTを作成しなさい\n","- これについて、HotPepper側の制限によりアクセスが拒否されている場合がある\n","- この場合は、Proxyを立てること"],"metadata":{"id":"xoemzd9MkM_7"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/keioNishi/lec-mlsys/blob/main/mlsys-text-M-ChatGPT-2-Application.ipynb","timestamp":1703525217224},{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}