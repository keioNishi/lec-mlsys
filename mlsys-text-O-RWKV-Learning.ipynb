{"cells":[{"cell_type":"markdown","metadata":{"id":"Ck0OgvaBxHVu"},"source":["# RWKV"]},{"cell_type":"markdown","metadata":{"id":"380RBGIhxBeR"},"source":["> 「変わることがなければ成長することもない  \n","> 成長することがなければ真に生きていない」  \n","> ビル・ゲイツ"]},{"cell_type":"markdown","metadata":{"id":"2_vdSl0ivzFj"},"source":["## RWKVの概要\n","\n","OpenAIが開発するChatGPTにおけるGPT4は強力であり、同様のモデルが様々登場しつつあるが、モデル動作に必要な計算資源も膨大であり、簡単に自宅やColabで動作させることはできないが、RWKVはこの問題を解決するのではないかと期待されている\n","\n","言語モデルは、LSTMなどRNNベースのモデルが利用されてきたが、Transformerの登場により状況が一変した\n","- Transformerにより並列処理が可能となり実行速度が向上した\n","- しかしながら、必要となる計算資源が膨大になった\n"]},{"cell_type":"markdown","metadata":{"id":"bbAsAeYpjaqV"},"source":["## RNNとTransformerの違いを再確認\n","\n","Recurrent Neural Networks (RNN)とTransformerの違いを再確認する\n","\n","ある文章があり、その単語のトークン列が$F[0], F[1], ... F[n]$とする\n","\n","- Transformer\n","  - Attention Weightを用いて$F[0] ... F[n-1]$の単語の依存関係から$F[n]$の単語を生成する\n","  - 文章全体の状態を保持して学習し、残差接続(residual connection)、Dropout、LayerNormなどを利用することで、層数を増やしても安定して学習を進めることができる\n","  - Self-Attentionは離れた位置の依存関係を学習できるが、シーケンス内のすべての要素と他のすべての要素との依存関係を計算するため、計算量とメモリ使用量がシーケンス長(トークン数)の2乗つまり、O(n^2)で大きくなるという欠点がある\n","  - 一方で計算を並列化することができるため、計算進度を高めることができる\n","\n","- RNN\n","  - $F[n-1]$の単語から$F[n]$の単語を生成というプロセスを繰り返して学習\n","  - 単一状態を保持し、これを繰り返し適用するため、離れた位置ほど単語の依存関係が失われていく\n","      - 計算を繰り返すため、勾配爆発や勾配消失を発生ひやすい\n","  - メモリと計算量は文章長に対して線形にスケールする\n","  - 並列化と拡張性の制限からtransformerと同等の性能を達成することが困難\n"]},{"cell_type":"markdown","metadata":{"id":"lmBeuRQwzrz0"},"source":["# RWKV(Receptance Weighted Key Value)とは?\n","\n","transformerの効率的な並列学習とRNNの効率的な推論の両方を兼ね備えたモデル\n","\n","名前は、利用する4つのパラメータ$R, W, K, V$に由来する\n"]},{"cell_type":"markdown","metadata":{"id":"IalKrapJKdP0"},"source":["## RWKVの特徴\n","\n","次のように纏めることができる\n","\n","- RNNベースのモデルを用いて、推論の高速化と省メモリ化を実現\n","  - 推論時のメモリ使用率は最も大きい14Bモデルでも3GB程度\n","- 数百億のパラメータまでスケールする(これが限界かどうかは不明であるが、スケール則は有効と思われる)、非Transformerアーキテクチャ\n","  - この規模はRNNベースのモデルでは達成できなかった\n","- 同一サイズのTransformerと同等の性能を発揮\n","- 学習時はTransformerと同様の動作であるため、並列化が容易\n","- 文章長が理論上無限となる(学習時の文章長に依存し、実際は1024程度)\n","- AttentionではなくRWKVモデルを利用する\n","\n","RWKVモデルは https://github.com/BlinkDL/ において公開されており、RWKV1からRWKV4までの4つのモデルが存在\n","\n","ここでは、RWKV2を用いるが、このモデルは推論にRNNモード、学習にTransformerモードを利用する"]},{"cell_type":"markdown","metadata":{"id":"lc7piu561e3Y"},"source":["## TransformerとRWKVの違い\n","\n","構造の違いを図を用いて表すと次の通り\n","\n","- RWKVは、特徴的なTime-mixing blockとChannel-mixing blockが存在\n","- Transformerと異なり、encoder-decoderモデルではない\n","- 全体の構造はTransformerと類似する\n","  - Time-mixing blockとmultihead attentionというブロックが同様の配置・連結された構造を持つが中身が異なる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer-rwkv.png\" width=700>\n"]},{"cell_type":"markdown","metadata":{"id":"_tJTWj6nPCF8"},"source":["# RWKVの詳細\n","\n","RWKVを特長づけるブロックとしてTime-mixing blockとChannel-mixing blockがある\n","\n","RWKVという名称の由来にもなっている4つのパラメータはTime-mixingブロックとChannel-mixingブロックで使用され、次の意味を持つ\n","- R:過去の情報の受容度を表現するReceptanceベクトル\n","- W:位置の重み減衰ベクトル。訓練可能なモデルパラメータ\n","- K:一般的なAttention Mechanismと同様のK(Key)ベクトル\n","- V:一般的なAttention Mechanismと同様のV(Value)ベクトル\n"]},{"cell_type":"markdown","metadata":{"id":"aF7YSS2NoEPl"},"source":["## TransformerからRWKVへ"]},{"cell_type":"markdown","metadata":{"id":"OOKLRNdFPosq"},"source":["### Transformer\n","\n","$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","ここでQ、K、Vはクエリ(Query)、キー(Key)、値(Value)を示し、キーの次元数$d_k$，シーケンス長(トークン数)$N$、次元数$d_v$である\n","\n","$F[t+1]$の予測に、$F[0]...F[t]$の文章と、現在の単語$x_t$と$F[t]$をそれぞれ比較して文脈全体の依存関係を考慮する\n","- 具体的には$Qx_t$と$K(F[0]...F[t])$の全ての単語の内積によりAttention Weightを求め、前の各状態$F[i]$と比較して類似度を求める\n","\n","すべてのQueryベクトルとKeyベクトルの内積を計算するため、計算量が$n^2$となる\n"]},{"cell_type":"markdown","metadata":{"id":"R9VIs20XppfQ"},"source":["### An Attention Free Transformer (AFT)\n","\n","Self-Attentionの代わりに普通の全結合層(Fully Connected layer)を利用したモデル\n","- 計算コストを大幅に削減\n","- 条件によって、Transformerと同等またはそれ以上のパフォーマンスを達成\n","\n","AFTは次のように求めることができる\n","\n","$$Attn^+(W,K,V)_t=\\frac{\\sum^t_{i=1}e^{\\omega t,i+k_i}v_i}{\\sum^t_{i=1}e^{\\omega t,i+k_i}}$$\n","\n","ここで$\\omega_{t,i} \\in R^{T\\times T}$は、学習した関係における位置バイアスを表す\n","\n","また、厳密ではないがRWKVを数的に表現すると、\n","\n","$$v_{i+1} = sigmoid(Rx[t])\\cdot \\sum^t_{i=0}{Attn^+(W,K,V)_t}$$\n","\n","となるため、一つ先($t+1$番目)の単語を予測する場合は、$t$番目の単語の予測に$sigmoid(R*F[t])$、$t～0$番目の単語の予測に$v_i$と$\\omega_{t,i+k_i}$の2つを用いる\n","\n","- なお、活性化関数は$sigmoid$である必要はないが、$sigmoid$の性能が高いという評価結果がある\n","  - この$sigmoid$の項は正規化せずreceptanceと呼ぶ\n","- MultiHeadAttentionは存在せず、各単語に$0～t$番目の単語との依存関係を求める必要ない\n","  - $e^{\\omega t,i+k_i}v_i$の項が依存関係を表現する\n","\n","この式により、RNNと同様再帰的に適用することで、$t-1$番目から、$t$番目を予測できる\n","- 一つ前の$v_{i-1}$に対して$e^\\omega$を掛け合わせ、$v_i$に関する項を足し合わせればよい\n","- この計算量の増大が、高々$O(n)$として表現できることから、計算コストを削減できる\n"]},{"cell_type":"markdown","metadata":{"id":"CMgqBL8zWUos"},"source":["## Time-mixing block\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ycnEUageMmZm"},"source":["### Time-mixing blockの動作内容\n","\n","RWKVの時刻$t$における各ベクトル・パラメータであるReceptanceベクトル($r_t$)、Key($k_t$)、Value($v_t$)は、次の式で表すことができる\n","\n","$$r_t=W_r\\cdot(\\mu_rx_t+(1−\\mu_r)x_{t−1}) \\\\\n","k_t=W_k\\cdot(\\mu_kx_t+(1−\\mu_k)x_{t−1}) \\\\\n","v_t=W_v\\cdot(\\mu_vx_t+(1−\\mu_v)x_{t−1})$$\n","\n","これらの値は時刻$t$での更新割合を制御するパラメータであり、0と1の間の値をとる\n","- $\\mu_r$​が大きいほど新しい入力$x_t$​の影響が強く、小さいほど過去の状態$x_{t−1}$​の影響が強くなる\n","\n","$mu_r$倍の新しい入力(x_t)と$1-mu_r$倍の和であることから、新しい入力と過去の状態との間で線形補間を行う式\n","- 従って、これを繰り返すことで再帰的に更新できる\n","\n","これらを使った演算の詳細は次の通りであり、An Attention Free Transformer (AFT）と類似している\n","\n","$$wk_vt=\\frac{\\sum_{i=1}^{t−1}e^{−(t−1−i)w+k_i}v_i+e^{u+k_t}v_t}{\\sum_{i=1}^{t−1}e^{−(t−1−i)w+k_i}+e^{u+k_t}}$$\n","$$o_t=W_o\\cdot(\\sigma(r_t)\\odot wk_vt)$$\n","\n","$wkv_t$​は、$Attn(Q,K,V)$と同様の役割を担っているが、$Q，K，V$の各要素はスカラーのため計算コストが小さい\n","\n","- 直感的には、時間$t$が増加するにつれてベクトル$o_t$​は長い履歴に依存することを示しているといえる\n","- ターゲットポジション$t$に対して、RWKVは位置間隔$[1, t]$での加重平均とレセプタンス$sigmoid(R)$と乗算している\n","  - 与えられたタイムステップ内で乗算され、異なるタイムステップで合計される\n","- 標準的なトランスフォーマーは全てのトークンのペア間でアテンションを計算するが、AFTは過去の時間ステップ全てにわたる加算の形でアテンションを計算するため、計算とメモリ利用効率が向上している"]},{"cell_type":"markdown","metadata":{"id":"rmPaCAVYhBGM"},"source":["### 時間減衰率(Time-Weighting)\n","\n","\n","RWKVはAFTに倣い、$\\omega_{t,i}$​をチャネルごとの時間減衰ベクトルとして定義している\n","\n","$$\\omega_{t,i} = -t(t-i)\\omega$$\n","\n","このモデルではTime-MixingおよびTime-Weightingというパラメータを導入している\n","- Time-Weightingを距離によるAttentionと呼ぶ\n","\n","Wはヘッド数、block_size(0～現在の時刻)、block_sizeの3次元で構成され、attに$\\mathbb{W}$を要素ごとかけた値をdropoutしている\n","\n","$\\mathbb{W}$は比較的小さいパラメータであり、このようなパラメ―タで離れた単語の依存関係を学習可能とする\n","\n","これは、異なる距離のトークン($W[:, block_size, block_size]$)が現在の単語attに与える影響は異なり、特に文章の後半は長い距離の$\\mathbb{W}$を用いる必要があるが、最初は履歴が小さく必要がない\n","\n","- SelfAttentionはそのような考慮がない\n","\n","実際には$\\mathbb{W}$は巡回行列でありバイアスを加算する必要があり、Time Weightingの導入によりPositionalEncodingが不要になる"]},{"cell_type":"markdown","metadata":{"id":"PVOwoFTJLpmf"},"source":["### Time-mixing blockの実装\n","\n","下記コードの関数`RWKV_TimeMix`の概要は次の通り\n","- 時刻$t$における入力の更新割合$µ_r$​はself.time_mixで表される\n","- また時刻$t−1$の入力xはnn.ZeroPad2dを用いて表されている\n","- これらを用いると入力$x$は、`x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)`と実装され、時間的にシフトした部分すなわち過去の情報と、シフトしていない部分すなわち現在の情報を適切な比率で混合した値へと変換している\n"]},{"cell_type":"markdown","metadata":{"id":"Q-s9U4cqWYJF"},"source":["## Channel-mixing block\n","\n","channel-mixing blockは次の式で表される\n","\n","$$r_t=W_r\\cdot(\\mu_rx_t+(1−\\mu_r)x_{t−1})$$\n","$$k_t=W_k\\cdot(\\mu_kx_t+(1−\\mu_k)x_{t−1})$$\n","$$o_t=\\sigma(r_t)\\odot(W_v\\cdot max(k_t,0)^2)$$\n","\n","Time-mixing blockは異なる時間ステップのトークン間の相互作用を管理するのに対して、Channel-mixing blockは同じ時間ステップ内の異なるチャンネル（または特徴）間の相互作用を管理する\n","\n","Channel-mixing blockは、全結合層や畳み込み層と同様の働きを持つ\n","- $\\sigma$はsquared ReLUを使用し、忘却ゲートの役割を果たす\n","\n","Channel-mixing blockの実装は、Time-mixing blockが理解できれば問題なく理解であろう"]},{"cell_type":"markdown","metadata":{"id":"gu8sUoklWba9"},"source":["# RWKVの特徴\n","\n","RWKVは学習時time-parallel mode (時間パラレルモード)が使用され、推論時（デコード時）はtime-sequential mode (時間シーケンシャルモード)が使用される\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/rwkv.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"NtnEww15S-YD"},"source":["## 時間パラレルモード\n","\n","学習時のモード\n","\n","時間パラレルモードは言葉の通り、時刻に関連する演算を並列して行う\n","\n","Time-mixing blockで説明したように新しい入力$x_t$と過去の状態との間で線形補間を行う\n","- 結果として、各タイムステップの計算が他のタイムステップの計算と独立に実行できる\n","- RNNとは異なり並列処理が可能"]},{"cell_type":"markdown","metadata":{"id":"eTK3Y6lpTJaA"},"source":["## 時間シーケンシャルモード\n","\n","推論時のモード\n","\n","学習時とは異なり、推論時にはRNNのような順次的なデコーディングを行う\n","\n","- このときRWKVはRNNと同様の構造を活用して動作する\n","  - これを時間シーケンシャルモードと呼ぶ\n","  - 各ステップの出力が次のステップの入力として用いられる\n","  - RNN同様、は出力トークンを一度に1つずつ生成し、あるトークンの生成は前のすべてのトークンの生成が完了した後に行わる\n","\n","RWKVはシーケンスの長さに関係なく一定の速度とメモリフットプリントを維持しする\n","- 長いシーケンスを効率的に処理できる\n","- 一方で、アテンションメカニズムを使用しているため、シーケンスの長さに比例してキャッシュの使用量が増加する\n","\n","以上からRWKVではTransfromerでは実現できなかった効率的なデコーディングが可能となる"]},{"cell_type":"markdown","metadata":{"id":"6JGsM1LVxoXO"},"source":["# RWKVの学習"]},{"cell_type":"markdown","metadata":{"id":"Op-Emz2dipPX"},"source":["実際に学習を行うことができる。ただし、学習にGoogle Colaboratory ProでA100を利用した場合でも10時間強程度必要となるため、基本的に放置になるのと同時に、それなりの課金が必要となる\n","\n","従って、ここでは、そのリンクのみ示すとして、実際に実行することは避ける\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9095,"status":"ok","timestamp":1724057234689,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"pM9PHL-Di7Ds","outputId":"4003ddf8-7f0c-416d-f8ee-b60d30543799"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3900,"status":"ok","timestamp":1724057238585,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"YHd2UZ8V8QMn","outputId":"0da8ffda-717a-4b07-c116-68b94dd5eb31"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-08-19 08:47:14--  https://keio.box.com/shared/static/71it8nriiu5h00630chy29x5ns3ucl74\n","Resolving keio.box.com (keio.box.com)... 74.112.186.157\n","Connecting to keio.box.com (keio.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/71it8nriiu5h00630chy29x5ns3ucl74 [following]\n","--2024-08-19 08:47:14--  https://keio.box.com/public/static/71it8nriiu5h00630chy29x5ns3ucl74\n","Reusing existing connection to keio.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://keio.app.box.com/public/static/71it8nriiu5h00630chy29x5ns3ucl74 [following]\n","--2024-08-19 08:47:14--  https://keio.app.box.com/public/static/71it8nriiu5h00630chy29x5ns3ucl74\n","Resolving keio.app.box.com (keio.app.box.com)... 74.112.186.157\n","Connecting to keio.app.box.com (keio.app.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!fLfkI5vQUr0USSjz_GkuhMFv0mXFsEyuNIRVIQSG0QjZnDHwjlzeRyj5yAm_Z32TEiqQ8WHhjxUDyHcCRWg6D9Zg3E434LA9YwaaspwstEJze-ipwBhxdC9weiWDgHW3rYpEDc4XZLe9CkO4eFNphDQHTHwDjEgvFZMdEZe6Cw1q9qvo8A5JucDVsj_xQ_xWFDMAXHzYMtJeTWxoNFT4djgLhyrrEY6jd0iojklHx59sFgNwnRYSmhz0sisyFS7bjkBKWggkACyGxSa9OESqrLc6KzuMkLJbTiI24WpF5Z8a9_DIhwyBtjIbm6o_X1UcZY60zYj2LPTaY1qhJxNsOEpIMfFJjM653hVYEFmPCIzbuBJyHYQc9mR7iCE_39YHshdmCHz3TyLO363dPXfGR71pOCw4pPPWlvDM-E_qLv8vDnVioGy0gjAk1s3tOjvtNcfV4XNiyM6mMvcF6zpnsYDk9pn0vmGvbpYL1ndm7o8PeSK1Z6bJX-nK8ksQmVrulp7YNITwg4P6xgpQEpmQkNRAO9hccOm0nv-M89BafRDiNUwSWqHR7_pkEIqlOo8Y2Byw4X3aUQiOt6mIpti_GZsCnjhjFDim6FVROApp0WxGH1sJI8bmf-KeabGAG8dx-0TK011BrHwFwVm2X1_U7qB1PU9fZxQENakPpTsLrSLX8U3tIY7oyTz65WMNK6bmlJfKaBd5Is8FIWdWIsEXUNuLMMboSTgLsIY3E804WziJs2SnUFE5-EBYo9F62Mfbz8aLsqgSdufGsx9Pzb3x5nnAe_s_hWp-WW_JEZlL2vuHJDeFEKS--BnENA3_XA7Z3Aqu75kq-O_TxT25UUCZvpF2WytNBpe4fXgH9Ck1YaQnFKiiQDrGTkmxZr3_3gqpI26QpXwvV7kk7JCrZMaCHBWN_dMDejB-DOTUZi_3Ffe7DlJTI1s7zIDyxrJ2_FOPQEoGnauzNlFA6KF0kA0MBfvkkzj0w-ojjfYtPYJTQyFfB_dp65fmxBR8l5PBF1LDcqm3ziFMfB0_QukxD_Vi7CnQDNUPDFRrD1_Wqa5Viw1l5RFZYh_-upXZ-Rtj2ghTOYqbjb3RG-J7BH0EmGRhG_Rte1gDewrlmW1LFeemJOYAyra718T6XNn2CMAScetPFRLK7liAKoa4iphhITbbqtueDbl3mzWkD7cny-RLPQ1psOMYkO3iRDGUFepFp3zQy7D73AAn681a4WLUcelCJJORSScXjo7XwHBIM938wIJItd5tZDp5lfqZUPVM6grK20hzq0Efmh2y8oU2Rap45s0jmn2ZgPQEL2FMULIuN0Hl4Z2nDgme2Tz1dLBqW6VZXesu_M88b8Qarmur8DqJq1w4YF1E7BYl-xmQMIg./download [following]\n","--2024-08-19 08:47:15--  https://public.boxcloud.com/d/1/b1!fLfkI5vQUr0USSjz_GkuhMFv0mXFsEyuNIRVIQSG0QjZnDHwjlzeRyj5yAm_Z32TEiqQ8WHhjxUDyHcCRWg6D9Zg3E434LA9YwaaspwstEJze-ipwBhxdC9weiWDgHW3rYpEDc4XZLe9CkO4eFNphDQHTHwDjEgvFZMdEZe6Cw1q9qvo8A5JucDVsj_xQ_xWFDMAXHzYMtJeTWxoNFT4djgLhyrrEY6jd0iojklHx59sFgNwnRYSmhz0sisyFS7bjkBKWggkACyGxSa9OESqrLc6KzuMkLJbTiI24WpF5Z8a9_DIhwyBtjIbm6o_X1UcZY60zYj2LPTaY1qhJxNsOEpIMfFJjM653hVYEFmPCIzbuBJyHYQc9mR7iCE_39YHshdmCHz3TyLO363dPXfGR71pOCw4pPPWlvDM-E_qLv8vDnVioGy0gjAk1s3tOjvtNcfV4XNiyM6mMvcF6zpnsYDk9pn0vmGvbpYL1ndm7o8PeSK1Z6bJX-nK8ksQmVrulp7YNITwg4P6xgpQEpmQkNRAO9hccOm0nv-M89BafRDiNUwSWqHR7_pkEIqlOo8Y2Byw4X3aUQiOt6mIpti_GZsCnjhjFDim6FVROApp0WxGH1sJI8bmf-KeabGAG8dx-0TK011BrHwFwVm2X1_U7qB1PU9fZxQENakPpTsLrSLX8U3tIY7oyTz65WMNK6bmlJfKaBd5Is8FIWdWIsEXUNuLMMboSTgLsIY3E804WziJs2SnUFE5-EBYo9F62Mfbz8aLsqgSdufGsx9Pzb3x5nnAe_s_hWp-WW_JEZlL2vuHJDeFEKS--BnENA3_XA7Z3Aqu75kq-O_TxT25UUCZvpF2WytNBpe4fXgH9Ck1YaQnFKiiQDrGTkmxZr3_3gqpI26QpXwvV7kk7JCrZMaCHBWN_dMDejB-DOTUZi_3Ffe7DlJTI1s7zIDyxrJ2_FOPQEoGnauzNlFA6KF0kA0MBfvkkzj0w-ojjfYtPYJTQyFfB_dp65fmxBR8l5PBF1LDcqm3ziFMfB0_QukxD_Vi7CnQDNUPDFRrD1_Wqa5Viw1l5RFZYh_-upXZ-Rtj2ghTOYqbjb3RG-J7BH0EmGRhG_Rte1gDewrlmW1LFeemJOYAyra718T6XNn2CMAScetPFRLK7liAKoa4iphhITbbqtueDbl3mzWkD7cny-RLPQ1psOMYkO3iRDGUFepFp3zQy7D73AAn681a4WLUcelCJJORSScXjo7XwHBIM938wIJItd5tZDp5lfqZUPVM6grK20hzq0Efmh2y8oU2Rap45s0jmn2ZgPQEL2FMULIuN0Hl4Z2nDgme2Tz1dLBqW6VZXesu_M88b8Qarmur8DqJq1w4YF1E7BYl-xmQMIg./download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n","Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1490 (1.5K) [application/octet-stream]\n","Saving to: ‘cuda.tgz’\n","\n","cuda.tgz            100%[===================>]   1.46K  --.-KB/s    in 0s      \n","\n","2024-08-19 08:47:15 (24.3 MB/s) - ‘cuda.tgz’ saved [1490/1490]\n","\n","--2024-08-19 08:47:15--  https://keio.box.com/shared/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0\n","Resolving keio.box.com (keio.box.com)... 74.112.186.157\n","Connecting to keio.box.com (keio.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /public/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0 [following]\n","--2024-08-19 08:47:15--  https://keio.box.com/public/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0\n","Reusing existing connection to keio.box.com:443.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://keio.app.box.com/public/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0 [following]\n","--2024-08-19 08:47:15--  https://keio.app.box.com/public/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0\n","Resolving keio.app.box.com (keio.app.box.com)... 74.112.186.157\n","Connecting to keio.app.box.com (keio.app.box.com)|74.112.186.157|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://public.boxcloud.com/d/1/b1!AY4v20mvs74f0ZXgWo9oXAEdE3bubMhC0XqCS6AAwqpRGgPhnEHzMOZmsaXK5hdB6CmbMCARe76gZHKgJbgWrHZ-KzNbcGpokA2ghOi8M4-AwJF3SA_reAu8jesfFpmsIx1o-quBskaDj_7YRqoZE94zVxrb76mZHrFYBnpm0y9GCjR8iff-jciyng6ASrGDq545mDif5aUX-Tg_WkFYQ-eSlnM-OGFzrgtZ2yx1hSlJ9sRMo6ldAPBllA63HZQUagkBS3DcK0IbyVqap0WZnIJOtgBaie3SzYCpW1uBk0mRZxa_JVyZm7BIoGagqNRpSwhHzbf7EVHlHzz9K41ITczdbvkO4ylNB9CIYxVaFVQ212quyap751bUa8QRQOn_lNkowvRhH0SIpZZPZ825i_zRN_Xh23qumc4cPROvAqNSU-pdxfdfXmdYxDj6N0N-sl7rg9Uiq0TVZrHQNQkQ4qWQE3Bol6r_wNsyTCpHyiRRdZh-2o9ShRxkOb8kviUqqnZK2Nv_l5FQ8KDKY55M8bDaYRThbiGY5IAS2AS8yvF56-YU90uArThUAsmbRAgfAVkKpjzQJtpQyqSzyc3rMElpEs5o9qO0A8eL5hfV9tAB3cLmMLmPSBW7mz2zCvhYYaTQSGTepJHxAxHtMsyXh7LaScTNlUe2B-QQsO0enm9zVfF55fGjm1r8NxuROMSZowX46kMLYYEpnza0Kt0JGF6ZeyG_8ikaxc6X6pf8CKbnJp9errCyjGH2W5Oj_H52oHM1th8mo8J4uc1tXQWvQ_ebDxUs1fPF3KT3RQ8kD0P_4tNjlbaVa8WkpQ_i2PniYF6-ViWqF4rcFM_DSdIWvGdD4-RKfPKR1w8RlCBnpmzrhyS056rJjyDJV7YVaE_AxakIhJfs3DJxiGRUx16qB0bHr6fklkBkB62X_5qoKkKPuT5RW1EPp222vr52Rq1oqtjcFdzkEMFXTOpWrntvKL42yL3d_DpygFWWLbbTLKMGXLVqwWMNmq_9xjZmCfLXTUslWexv9pXeQIqrVqeoatpzc1C85oBqccA472yrfjitDygm8kLxke8RbaXRBeW9-CehJTNzYdw_n74e5QprR64I9JorOzOP4N58q_TuKe1C-uBl6TG9YYoKgbXCJn5_iyNSaJHJnxkFGGv6uUlRNX5GIxQBzTmCWF9VifF6vnNlW-qoo2AR314p8o5rFs-xbC_siaOhDjYDKJw7lDl8UQtZsUXjMok4hpKvkYgSsyGc41vQ2Q5WV_5SuxTXbop0L5jAvzzpm7Pzsw0A0YapxPwiBGAaEwHRiMHmky_qZPUZFOPzSDL-aElfhKPBOPI0W-5yXbVIfbzrtxGA0Ss4Tcd9PRcZ9wA7GJKnZKTjJMw4uvg./download [following]\n","--2024-08-19 08:47:16--  https://public.boxcloud.com/d/1/b1!AY4v20mvs74f0ZXgWo9oXAEdE3bubMhC0XqCS6AAwqpRGgPhnEHzMOZmsaXK5hdB6CmbMCARe76gZHKgJbgWrHZ-KzNbcGpokA2ghOi8M4-AwJF3SA_reAu8jesfFpmsIx1o-quBskaDj_7YRqoZE94zVxrb76mZHrFYBnpm0y9GCjR8iff-jciyng6ASrGDq545mDif5aUX-Tg_WkFYQ-eSlnM-OGFzrgtZ2yx1hSlJ9sRMo6ldAPBllA63HZQUagkBS3DcK0IbyVqap0WZnIJOtgBaie3SzYCpW1uBk0mRZxa_JVyZm7BIoGagqNRpSwhHzbf7EVHlHzz9K41ITczdbvkO4ylNB9CIYxVaFVQ212quyap751bUa8QRQOn_lNkowvRhH0SIpZZPZ825i_zRN_Xh23qumc4cPROvAqNSU-pdxfdfXmdYxDj6N0N-sl7rg9Uiq0TVZrHQNQkQ4qWQE3Bol6r_wNsyTCpHyiRRdZh-2o9ShRxkOb8kviUqqnZK2Nv_l5FQ8KDKY55M8bDaYRThbiGY5IAS2AS8yvF56-YU90uArThUAsmbRAgfAVkKpjzQJtpQyqSzyc3rMElpEs5o9qO0A8eL5hfV9tAB3cLmMLmPSBW7mz2zCvhYYaTQSGTepJHxAxHtMsyXh7LaScTNlUe2B-QQsO0enm9zVfF55fGjm1r8NxuROMSZowX46kMLYYEpnza0Kt0JGF6ZeyG_8ikaxc6X6pf8CKbnJp9errCyjGH2W5Oj_H52oHM1th8mo8J4uc1tXQWvQ_ebDxUs1fPF3KT3RQ8kD0P_4tNjlbaVa8WkpQ_i2PniYF6-ViWqF4rcFM_DSdIWvGdD4-RKfPKR1w8RlCBnpmzrhyS056rJjyDJV7YVaE_AxakIhJfs3DJxiGRUx16qB0bHr6fklkBkB62X_5qoKkKPuT5RW1EPp222vr52Rq1oqtjcFdzkEMFXTOpWrntvKL42yL3d_DpygFWWLbbTLKMGXLVqwWMNmq_9xjZmCfLXTUslWexv9pXeQIqrVqeoatpzc1C85oBqccA472yrfjitDygm8kLxke8RbaXRBeW9-CehJTNzYdw_n74e5QprR64I9JorOzOP4N58q_TuKe1C-uBl6TG9YYoKgbXCJn5_iyNSaJHJnxkFGGv6uUlRNX5GIxQBzTmCWF9VifF6vnNlW-qoo2AR314p8o5rFs-xbC_siaOhDjYDKJw7lDl8UQtZsUXjMok4hpKvkYgSsyGc41vQ2Q5WV_5SuxTXbop0L5jAvzzpm7Pzsw0A0YapxPwiBGAaEwHRiMHmky_qZPUZFOPzSDL-aElfhKPBOPI0W-5yXbVIfbzrtxGA0Ss4Tcd9PRcZ9wA7GJKnZKTjJMw4uvg./download\n","Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n","Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 35193282 (34M) [application/octet-stream]\n","Saving to: ‘enwik8.tgz’\n","\n","enwik8.tgz          100%[===================>]  33.56M  59.8MB/s    in 0.6s    \n","\n","2024-08-19 08:47:17 (59.8 MB/s) - ‘enwik8.tgz’ saved [35193282/35193282]\n","\n"]}],"source":["import os\n","if not os.path.exists('cuda.tgz'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=10TVle71vI4B7bB-pPYLEaog4ut74sG6q\" -O cuda.tgz\n","  !wget https://keio.box.com/shared/static/71it8nriiu5h00630chy29x5ns3ucl74 -O cuda.tgz\n","  !tar xzf cuda.tgz\n","if not os.path.exists('enwik8.tgz'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=10TN_ZnQ3G84vhzoR-AiVfekt4SJMKVW5\" -O enwik8.tgz\n","  !wget https://keio.box.com/shared/static/9lk6sqkf1ol5z0glfmukvjtvs07zzmd0 -O enwik8.tgz\n","  !tar xzf enwik8.tgz\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3373,"status":"ok","timestamp":1724057241956,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"73DZ_3gBxIjM","outputId":"36fbb2df-f571-42f9-b167-06aee38ed40b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ninja\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m286.7/307.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ninja\n","Successfully installed ninja-1.11.1.1\n"]}],"source":["!pip install ninja"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xFupxeC1WU0"},"outputs":[],"source":["import json\n","import random\n","import time\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.cpp_extension import load\n","from torch.utils.data import Dataset\n","import math\n","import logging\n","import datetime\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cuda.matmul.allow_tf32 = True\n","\n","### Step 1: set training data ##########################################################################\n","\n","datafile = \"enwik8\"\n","datafile_encoding = 'utf-8'\n","# datafile_encoding = 'utf-16le'\n","\n","### Step 2: set model size #############################################################################\n","\n","ctx_len = 1024        # ===> increase T_MAX in model.py if your ctx_len > 1024\n","n_layer = 6\n","n_embd = 512\n","\n","# 'RWKV' (better for char-level English) or 'RWKV-ffnPre' (better in some cases)\n","model_type = 'RWKV'\n","\n","### Step 3: set batch size #############################################################################\n","\n","# ===> batch_size must be divisible by B_GROUP_FORWARD and B_GROUP_BACKWARD in model.py\n","# For example, if your batch_size = 20, you can set B_GROUP_FORWARD = 4, B_GROUP_BACKWARD = 2\n","# If you see \"CUDA out of memory\", reduce it. Use GPU-Z to find the highest value for your VRAM.\n","batch_size = 12\n","\n","### Step 4: set learning rate, training mini-epochs #######################################################\n","\n","lr_init = 6e-4\n","lr_final = 1e-5\n","# the mini-epoch is very short and of fixed length (ctx_len * epoch_length_fixed tokens)\n","n_epoch = 500\n","# 0 = never, 1 = every mini-epoch, 2 = every two mini-epochs, etc.\n","epoch_save_frequency = 30\n","epoch_save_path = 'trained-'\n","\n","epoch_length_fixed = 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19541,"status":"ok","timestamp":1724057265352,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"U2pG_j9D3Q_D","outputId":"20c73cd4-bdf1-48d8-ad3c-3e09ddec8a07"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/py310_cu121/timex...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/timex/build.ninja...\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n","If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n","  warnings.warn(\n","Building extension module timex...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module timex...\n"]}],"source":["########################################################################################################\n","# CUDA Kernel\n","########################################################################################################\n","\n","T_MAX = 1024          # increase this if your ctx_len > 1024\n","B_GROUP_FORWARD = 4   # set to 8 for best performance\n","B_GROUP_BACKWARD = 2  # set to 2 for best performance\n","\n","timex_cuda = load(name=\"timex\", sources=[\"cuda/timex_op.cpp\", \"cuda/timex_cuda.cu\"],\n","                  verbose=True, extra_cuda_cflags=['--use_fast_math', '--extra-device-vectorization', f'-DTmax={T_MAX}', f'-DBF={B_GROUP_FORWARD}', f'-DBB={B_GROUP_BACKWARD}'])\n","\n","\n","class TimeX(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, w, k, B, C, T, eps):\n","        ctx.B = B\n","        ctx.C = C\n","        ctx.T = T\n","        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n","        w = w.contiguous()\n","        k = k.contiguous()\n","        ctx.save_for_backward(w, k)\n","        wk = torch.empty((B, C, T), device='cuda',\n","                         memory_format=torch.contiguous_format)\n","        timex_cuda.forward(w, k, wk, eps, B, C, T)\n","        return wk\n","\n","    @staticmethod\n","    def backward(ctx, gwk):\n","        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n","        w, k = ctx.saved_tensors\n","        gw = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n","                         memory_format=torch.contiguous_format)\n","        gk = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n","                         memory_format=torch.contiguous_format)\n","        timex_cuda.backward(w, k, gwk.contiguous(), gw,\n","                            gk, ctx.B, ctx.C, ctx.T)\n","        return (gw.sum(dim=0), gk, None, None, None, None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2_rk5qJ1xEI"},"outputs":[],"source":["########################################################################################################\n","# RWKV: RWKV Time-mix + RWKV Channel-mix\n","########################################################################################################\n","\n","\n","RWKV_K_CLAMP = 60  # e^60 = 1e26\n","RWKV_K_EPS = 1e-16\n","RWKV_HEAD_QK_DIM = 256\n","\n","\n","def RWKV_Init(module, config):  # fancy initialization of all lin & emb layer in the module\n","    for m in module.modules():\n","        if not isinstance(m, (nn.Linear, nn.Embedding)):\n","            continue\n","        with torch.no_grad():\n","            name = '[unknown weight]'\n","            for name, parameter in module.named_parameters():  # find the name of the weight\n","                if id(m.weight) == id(parameter):\n","                    break\n","\n","            shape = m.weight.data.shape\n","            gain = 1.0\n","            scale = 1.0  # extra scale for gain\n","\n","            if isinstance(m, nn.Embedding):\n","                gain = math.sqrt(max(shape[0], shape[1]))\n","                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # token emb?\n","                    scale = 1e-4\n","                else:\n","                    scale = 0\n","\n","            if isinstance(m, nn.Linear):\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","                if shape[0] > shape[1]:\n","                    gain = math.sqrt(shape[0] / shape[1])\n","                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # final projection?\n","                    scale = 0.5\n","\n","            if hasattr(m, 'scale_init'):\n","                scale = m.scale_init\n","\n","            # print(str(shape[0]).ljust(5), str(shape[1]).ljust(5), f'{round(scale,2):g}'.ljust(4), name)\n","\n","            gain *= scale\n","            if scale == -999:\n","                nn.init.eye_(m.weight)\n","            elif gain == 0:\n","                # zero init is great for some RWKV matrices\n","                nn.init.zeros_(m.weight)\n","            elif gain > 0:\n","                nn.init.orthogonal_(m.weight, gain=gain)\n","            else:\n","                nn.init.normal_(m.weight, mean=0.0, std=-scale)\n","\n","class RWKV_TimeMix(nn.Module):\n","    def __init__(self, config, layer_id):\n","        super().__init__()\n","        self.layer_id = layer_id\n","        self.ctx_len = config.ctx_len\n","        self.n_embd = config.n_embd\n","\n","        attn_sz = config.n_embd\n","\n","        ############# fancy init of time_w curves ###################################\n","        f1_begin = 3.0\n","        f1_end = 1.2\n","        f2_begin = 0.65\n","        f2_end = 0.4\n","        with torch.no_grad():  # initial time_w curves for better convergence\n","            decay_speed = torch.ones(attn_sz, 1)\n","            first_sa_layer_id = 1\n","            for h in range(attn_sz):\n","                f1 = f1_begin + (layer_id-first_sa_layer_id) / \\\n","                    (config.n_layer-1-first_sa_layer_id) * (f1_end - f1_begin)\n","                f2 = f2_begin + (layer_id-first_sa_layer_id) / \\\n","                    (config.n_layer-1-first_sa_layer_id) * (f2_end - f2_begin)\n","                if layer_id == first_sa_layer_id:\n","                    f1 += 0.5\n","                if layer_id == config.n_layer-2:\n","                    f2 = 0.4\n","                if layer_id == config.n_layer-1:\n","                    f2 = 0.37\n","                decay_speed[h][0] = math.pow(f2, h / (attn_sz-1) * 7) * f1\n","        self.time_decay = nn.Parameter(torch.log(decay_speed)) # will use exp(self.time_decay) to ensure time_decay > 0\n","        self.time_curve = torch.tensor(\n","            [-(config.ctx_len - 2 - i) for i in range(config.ctx_len-1)]).unsqueeze(0)\n","        self.time_curve = self.time_curve.to('cuda')\n","        self.time_first = nn.Parameter(torch.ones(attn_sz, 1) * math.log(0.3))\n","        #############################################################################\n","\n","        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n","        with torch.no_grad():  # init to \"shift half of the channels\"\n","            ww = torch.ones(1, 1, config.n_embd)\n","            for i in range(config.n_embd // 2):\n","                ww[0, 0, i] = 0\n","        self.time_mix = nn.Parameter(ww)\n","\n","        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n","        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n","        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)\n","\n","        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n","\n","        self.key.scale_init = 0\n","        self.receptance.scale_init = 0\n","        self.output.scale_init = 0\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","\n","        x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)\n","\n","        k = self.key(x).transpose(-1, -2)\n","        v = self.value(x).transpose(-1, -2)\n","        r = self.receptance(x)\n","\n","        # RWKV_K_CLAMP can be removed if the CUDA kernel substracts the correct k_max for each k (I will do this later)\n","        k = torch.clamp(k, max=RWKV_K_CLAMP)\n","        k = torch.exp(k)\n","        kv = k * v\n","\n","        self.time_w = torch.cat(\n","            [torch.exp(self.time_decay) * self.time_curve, self.time_first], dim=-1)\n","        w = torch.exp(self.time_w)\n","\n","        wkv = TimeX.apply(w, kv, B, C, T, 0)\n","        # RWKV_K_EPS can be removed if the CUDA kernel sets 0/0 = 0 (I will do this later)\n","        wk = TimeX.apply(w, k, B, C, T, RWKV_K_EPS)\n","\n","        rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\n","        rwkv = self.output(rwkv)\n","        return rwkv\n","\n","\n","class RWKV_ChannelMix(nn.Module):\n","    def __init__(self, config, layer_id):\n","        super().__init__()\n","        self.layer_id = layer_id\n","\n","        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n","\n","        with torch.no_grad():  # init to \"shift half of the channels\"\n","            x = torch.ones(1, 1, config.n_embd)\n","            for i in range(config.n_embd // 2):\n","                x[0, 0, i] = 0\n","        self.time_mix = nn.Parameter(x)\n","\n","        hidden_sz = 4 * config.n_embd\n","        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n","        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n","        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n","\n","        self.value.scale_init = 0\n","        self.receptance.scale_init = 0\n","\n","    def forward(self, x):\n","        x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)\n","\n","        k = self.key(x)\n","        k = torch.square(torch.relu(k))\n","        kv = self.value(k)\n","\n","        rkv = torch.sigmoid(self.receptance(x)) * kv\n","        return rkv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJxMQ-E01Nkb"},"outputs":[],"source":["########################################################################################################\n","# The GPT Model with our blocks\n","########################################################################################################\n","\n","class GPTConfig:\n","    def __init__(self, vocab_size, ctx_len, **kwargs):\n","        self.vocab_size = vocab_size\n","        self.ctx_len = ctx_len\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","\n","class Block(nn.Module):\n","    def __init__(self, config, layer_id):\n","        super().__init__()\n","        self.config = config\n","        self.layer_id = layer_id\n","\n","        self.ln1 = nn.LayerNorm(config.n_embd)\n","        self.ln2 = nn.LayerNorm(config.n_embd)\n","\n","        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n","            self.ffnPre = RWKV_ChannelMix(config, layer_id+1000)\n","        else:\n","            self.att = RWKV_TimeMix(config, layer_id)\n","\n","        self.ffn = RWKV_ChannelMix(config, layer_id)\n","\n","    def forward(self, x):\n","        x = self.ln1(x)\n","        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n","            x = x + self.ffnPre(x)  # better in some cases\n","        else:\n","            x = x + self.att(x)\n","        x = self.ln2(x)\n","        x = x + self.ffn(x)\n","        return x\n","\n","class GPT(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.step = 0\n","        self.config = config\n","\n","        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n","\n","        self.blocks = nn.Sequential(*[Block(config, i)\n","                                    for i in range(config.n_layer)])\n","\n","        self.ln_out = nn.LayerNorm(config.n_embd)\n","        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","\n","        self.head_q = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n","        self.head_q.scale_init = 0\n","        self.head_k = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n","        self.head_k.scale_init = 0.1\n","        self.register_buffer(\"copy_mask\", torch.tril(\n","            torch.ones(config.ctx_len, config.ctx_len)))\n","\n","        self.ctx_len = config.ctx_len\n","\n","        RWKV_Init(self, config)\n","\n","        logger.info(\"number of parameters: %e\", sum(p.numel()\n","                    for p in self.parameters()))\n","\n","    def get_ctx_len(self):\n","        return self.ctx_len\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear)):\n","            module.weight.data.normal_(mean=0.0, std=0.01)\n","        if isinstance(module, (nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=1e-5)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def configure_optimizers(self, train_config):\n","        # separate out all parameters to those that will and won't experience regularizing weight decay\n","        decay = set()\n","        no_decay = set()\n","\n","        for mn, m in self.named_modules():  # here we disable weight_decay\n","            for pn, p in m.named_parameters():\n","                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n","                no_decay.add(fpn)\n","\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        inter_params = decay & no_decay\n","        union_params = decay | no_decay\n","        assert len(\n","            inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n","        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n","            % (str(param_dict.keys() - union_params), )\n","\n","        optim_groups = [\n","            {\"params\": [param_dict[pn]\n","                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n","        ]\n","\n","        optimizer = torch.optim.Adam(\n","            optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n","\n","        return optimizer\n","\n","    def forward(self, idx, targets=None):\n","        self.step += 1\n","        B, T = idx.size()\n","        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n","        x = self.emb(idx)\n","\n","        x = self.blocks(x)\n","\n","        x = self.ln_out(x)\n","\n","        q = self.head_q(x)[:, :T, :]\n","        k = self.head_k(x)[:, :T, :]\n","        c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n","        c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n","\n","        c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).float()\n","        x = self.head(x) + c\n","\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n","\n","        return x, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xp16yC-X2JcP"},"outputs":[],"source":["########################################################################################################\n","# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n","########################################################################################################\n","\n","class Dataset(Dataset):\n","    def __init__(self, data, ctx_len, epoch_length_fixed):\n","        print('building token list...', end=' ')\n","        unique = sorted(list(set(data)))\n","        # print()\n","        # for u in unique:\n","        #     print(u, end=' ')\n","        # print('\\n\\n')\n","\n","        xx = 0\n","        xxObj = {}\n","        for u in unique:\n","            xxObj[xx] = u\n","            xx += 1\n","        with open('vocab.json', \"w\", encoding=\"utf-16\") as vocab_file:\n","            vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n","\n","        data_size, vocab_size = len(data), len(unique)\n","        print('data has %d tokens, %d unique.' % (data_size, vocab_size))\n","        self.stoi = {ch: i for i, ch in enumerate(unique)}\n","        self.itos = {i: ch for i, ch in enumerate(unique)}\n","        self.ctx_len = ctx_len\n","        self.epoch_length_fixed = epoch_length_fixed\n","        self.vocab_size = vocab_size\n","        self.data = data\n","\n","    def __len__(self):\n","        return self.epoch_length_fixed\n","\n","    def __getitem__(self, idx):\n","        # cheat: pick a random spot in dataset\n","        i = np.random.randint(0, len(self.data) - (self.ctx_len + 1))\n","        chunk = self.data[i:i+self.ctx_len+1]\n","        dix = [self.stoi[s] for s in chunk]\n","        x = torch.tensor(dix[:-1], dtype=torch.long,\n","                         device=torch.device('cuda'))\n","        y = torch.tensor(dix[1:], dtype=torch.long,\n","                         device=torch.device('cuda'))\n","        return x, y\n","\n","\n","class TOKENIZER():\n","    def __init__(self, WORD_NAME, UNKNOWN_CHAR='\\ue083'):\n","        with open(WORD_NAME + '.json', \"r\", encoding=\"utf-16\") as result_file:\n","            self.word_table = json.load(result_file)\n","\n","        self.vocab_size = len(self.word_table)\n","\n","        self.stoi = {v: int(k) for k, v in self.word_table.items()}\n","        self.itos = {int(k): v for k, v in self.word_table.items()}\n","\n","        self.UNKNOWN_CHAR = self.stoi[UNKNOWN_CHAR]\n","\n","    def refine_context(self, context):\n","        context = context.strip().split('\\n')\n","        for c in range(len(context)):\n","            context[c] = context[c].strip().strip('\\u3000').strip('\\r')\n","        context = list(filter(lambda c: c != '', context))\n","        context = '\\n' + ('\\n'.join(context)).strip()\n","        if context == '':\n","            context = '\\n'\n","\n","        return context\n","\n","    def sample_logits(self, out, x, ctx_len, temperature=1.0, top_p_usual=None, top_p_newline=None):\n","        # out[self.UNKNOWN_CHAR] = -float('Inf')\n","\n","        lastChar = int(x[-1])\n","\n","        probs = F.softmax(torch.tensor(out), dim=-1)\n","\n","        if self.itos[lastChar] == '\\n':\n","            top_p = top_p_newline\n","        else:\n","            top_p = top_p_usual\n","\n","        sorted_probs, s_index = torch.sort(probs, descending=True)\n","\n","        # for j in range(30):\n","        #     pp = sorted_probs[j].item()\n","        #     if pp < 0.005:\n","        #         break\n","        #     ss = self.itos[int(s_index[j])].replace('\\n','_')\n","        #     print(f'{math.floor(pp*100):>3.0f}{ss}', end='')\n","        # print('')\n","\n","        cumulative_probs = torch.cumsum(sorted_probs, dim=-1).numpy()\n","        cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n","\n","        probs[probs < cutoff] = 0\n","        # print(\"[\" + str(round(cutoff,4)) + ' ' + str(round(to_float(sum(probs)),3)) + \"]\", end = \"\")\n","\n","        if temperature != 1.0:\n","            probs = probs.pow(1.0 / temperature)\n","\n","        return torch.multinomial(probs, num_samples=1)[0]\n","\n","\n","def to_float(x):\n","    return x.cpu().detach().numpy().flatten()[0].astype(float)\n","\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OQgBj-_19MY"},"outputs":[],"source":["########################################################################################################\n","# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n","########################################################################################################\n","\n","from torch.utils.data.dataloader import DataLoader\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch.nn import functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","from tqdm.auto import tqdm\n","import numpy as np\n","import logging\n","import os\n","import datetime\n","import sys\n","import math\n","\n","# import wandb  # comment this if you don't have wandb\n","# print('logging to wandb... (comment it if you don\\'t have wandb)')\n","\n","logger = logging.getLogger(__name__)\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cuda.matmul.allow_tf32 = True\n","\n","log_file = open(\"mylog.txt\", \"a\")\n","\n","\n","class TrainerConfig:\n","    max_epochs = 10\n","    batch_size = 64\n","    learning_rate = 4e-4\n","    betas = (0.9, 0.99)\n","    eps = 1e-8\n","    grad_norm_clip = 1.0\n","    lr_decay = True  # linear warmup followed by cosine decay\n","    warmup_tokens = 0\n","    final_tokens = 0\n","    epoch_save_frequency = 0\n","    epoch_save_path = 'trained-'\n","    num_workers = 0  # for DataLoader\n","\n","    def __init__(self, **kwargs):\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","\n","\n","class Trainer:\n","\n","    def __init__(self, model, train_dataset, test_dataset, config):\n","        self.model = model\n","        self.train_dataset = train_dataset\n","        self.test_dataset = test_dataset\n","        self.config = config\n","        self.avg_loss = -1\n","        self.steps = 0\n","\n","        if 'wandb' in sys.modules:\n","            cfg = model.config\n","            for k in config.__dict__:\n","                setattr(cfg, k, config.__dict__[k])  # combine cfg\n","            wandb.init(project=\"RWKV-LM\", name=self.get_run_name() + '-' +\n","                       datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'), config=cfg, save_code=False)\n","\n","        self.device = 'cpu'\n","        if torch.cuda.is_available():  # take over whatever gpus are on the system\n","            self.device = torch.cuda.current_device()\n","\n","    def get_run_name(self):\n","        raw_model = self.model.module if hasattr(\n","            self.model, \"module\") else self.model\n","        cfg = raw_model.config\n","        run_name = str(cfg.vocab_size) + '-' + str(cfg.ctx_len) + '-' + \\\n","            cfg.model_type + '-' + str(cfg.n_layer) + '-' + str(cfg.n_embd)\n","        return run_name\n","\n","    def train(self):\n","        model, config = self.model, self.config\n","        raw_model = model.module if hasattr(self.model, \"module\") else model\n","        optimizer = raw_model.configure_optimizers(config)\n","\n","        def run_epoch(split):\n","            is_train = split == 'train'\n","            model.train(is_train)\n","            data = self.train_dataset if is_train else self.test_dataset\n","\n","            if config.num_workers > 0:\n","                loader = DataLoader(data, shuffle=False, pin_memory=True,\n","                                    batch_size=config.batch_size,\n","                                    num_workers=config.num_workers)\n","            else:\n","                loader = DataLoader(data, shuffle=False,\n","                                    batch_size=config.batch_size,\n","                                    num_workers=config.num_workers)\n","\n","            pbar = tqdm(enumerate(loader), total=len(\n","                loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\n","\n","            for it, (x, y) in pbar:\n","                x = x.to(self.device)  # place data on the correct device\n","                y = y.to(self.device)\n","\n","                with torch.set_grad_enabled(is_train):\n","                    _, loss = model(x, y)  # forward the model\n","\n","                if is_train:  # backprop and update the parameters\n","                    model.zero_grad()\n","                    loss.backward()\n","\n","                    if config.grad_norm_clip > 0:\n","                        torch.nn.utils.clip_grad_norm_(\n","                            model.parameters(), config.grad_norm_clip)\n","\n","                    optimizer.step()\n","\n","                    if config.lr_decay:  # decay the learning rate based on our progress\n","                        # number of tokens processed this step (i.e. label is not -100)\n","                        self.tokens += (y >= 0).sum()\n","                        lr_final_factor = config.lr_final / config.learning_rate\n","                        if self.tokens < config.warmup_tokens:\n","                            # linear warmup\n","                            lr_mult = lr_final_factor + \\\n","                                (1 - lr_final_factor) * float(self.tokens) / \\\n","                                float(config.warmup_tokens)\n","                            progress = 0\n","                        else:\n","                            # cosine learning rate decay\n","                            progress = float(self.tokens - config.warmup_tokens) / float(\n","                                max(1, config.final_tokens - config.warmup_tokens))\n","                            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor /\n","                                                                     2) * math.cos(math.pi * progress)  # better 1.0 ~ 0.1\n","                        lr = config.learning_rate * lr_mult\n","                        for param_group in optimizer.param_groups:\n","                            param_group['lr'] = lr\n","                    else:\n","                        lr = config.learning_rate\n","\n","                    now_loss = loss.item()  # report progress\n","                    self.lr = lr\n","\n","                    if 'wandb' in sys.modules:\n","                        wandb.log({\"loss\": now_loss},\n","                                  step=self.steps * self.config.batch_size)\n","                    self.steps += 1\n","\n","                    if self.avg_loss < 0:\n","                        self.avg_loss = now_loss\n","                    else:\n","                        factor = 1 / (it + 1)\n","                        self.avg_loss = self.avg_loss * \\\n","                            (1.0 - factor) + now_loss * factor\n","                    pbar.set_description(\n","                        f\"mini-epoch {epoch+1} prog {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")\n","\n","        self.tokens = 0  # counter used for learning rate decay\n","        for epoch in range(config.max_epochs):\n","\n","            run_epoch('train')\n","\n","            log_file.write(\n","                f'{epoch+1} {self.avg_loss:.6f} {math.exp(self.avg_loss):.4f} {self.lr:.8f} {datetime.datetime.now()} \\n')\n","            log_file.flush()\n","\n","            if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\n","                # DataParallel wrappers keep raw model object in .module\n","                raw_model = self.model.module if hasattr(\n","                    self.model, \"module\") else self.model\n","                torch.save(raw_model.state_dict(),\n","                           self.config.epoch_save_path + str(epoch+1) + '.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4d4bec4d72844ed78a62e907afca7ee3"]},"id":"AadX-QSQtb7c","outputId":"0fc97520-3178-4414-d75b-f09f33e80b34"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading data... enwik8\n","building token list... data has 99621832 tokens, 6064 unique.\n","model RWKV epoch 500 batchsz 12 betas (0.9, 0.99) eps 4e-09 ctx 1024 layer 6 embd 512\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d4bec4d72844ed78a62e907afca7ee3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/834 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["########################################################################################################\n","\n","# import src.utils\n","# src.utils.set_seed(42) # remember to change seed if you load a model\n","\n","np.set_printoptions(precision=4, suppress=True, linewidth=200)\n","logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","                    datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\n","\n","grad_norm_clip = 1.0\n","warmup_tokens = 0\n","\n","betas = (0.9, 0.99)\n","eps = 4e-9\n","\n","num_workers = 0\n","\n","########################################################################################################\n","# Load data\n","########################################################################################################\n","\n","print('loading data... ' + datafile)\n","train_dataset = Dataset(open(\n","    datafile, \"r\", encoding=datafile_encoding).read(), ctx_len, epoch_length_fixed)\n","\n","########################################################################################################\n","# Train model\n","########################################################################################################\n","model = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=model_type,\n","                      n_layer=n_layer, n_embd=n_embd)).cuda()\n","\n","# # # load a trained model. remember to change random seed\n","# m2 = torch.load('trained-61.pth')\n","# model.load_state_dict(m2)\n","\n","print('model', model_type, 'epoch', n_epoch, 'batchsz', batch_size, 'betas',\n","      betas, 'eps', eps, 'ctx', ctx_len, 'layer', n_layer, 'embd', n_embd, )\n","tconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size,\n","                      learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps, grad_norm_clip=grad_norm_clip,\n","                      warmup_tokens=warmup_tokens, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=num_workers, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)\n","trainer = Trainer(model, train_dataset, None, tconf)\n","\n","trainer.train()\n","\n","savepthname = 'trained-' + str(n_epoch) + '-' + trainer.get_run_name() + \\\n","            '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S') + '.pth'\n","\n","torch.save(model.state_dict(), savepthname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgHzuF0G7VuW"},"outputs":[],"source":["torch.save(model.state_dict(), 'drive/MyDrive/' + savepthname)"]},{"cell_type":"markdown","metadata":{"id":"gCoSvZke71ch"},"source":["ここまでが上手くいけば、問題なく動作するはずである\n","\n","計算コストが大きく学習できなかった場合は、あらかじめ準備した学習結果をロードして続きを実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTUWjCdd9Aiw"},"outputs":[],"source":["\n","RWKV_K_CLAMP = 60\n","RWKV_K_EPS = 1e-16\n","RWKV_HEAD_QK_DIM = 256\n","\n","DEBUG_TIME = False   # True False - show trained time-coeffs\n","\n","\n","class RWKV_RNN():\n","    def __init__(self, MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len):\n","        self.RUN_DEVICE = RUN_DEVICE\n","        self.model_type = model_type\n","        self.n_layer = n_layer\n","        self.n_embd = n_embd\n","        self.ctx_len = ctx_len\n","\n","        self.w = types.SimpleNamespace()\n","\n","        w = torch.load(MODEL_NAME + '.pth',\n","                       map_location=torch.device(RUN_DEVICE))\n","        for x in w.keys():\n","            if '.time_' in x:\n","                w[x] = w[x].squeeze()\n","            if '.time_decay' in x:\n","                w[x] = torch.exp(-torch.exp(w[x]))\n","            if '.time_first' in x:\n","                w[x] = torch.exp(w[x])\n","            if DEBUG_TIME and '.time_' in x:\n","                print(x, w[x].squeeze().cpu().numpy())\n","\n","            xx = x.split('.')\n","            here = self.w\n","            for i in range(len(xx)):\n","                if xx[i].isdigit():\n","                    ii = int(xx[i])\n","                    if ii not in here:\n","                        here[ii] = types.SimpleNamespace()\n","                    here = here[ii]\n","                else:\n","                    if i == len(xx) - 1:\n","                        setattr(here, xx[i], w[x])\n","                    elif not hasattr(here, xx[i]):\n","                        if xx[i+1].isdigit():\n","                            setattr(here, xx[i], {})\n","                        else:\n","                            setattr(here, xx[i], types.SimpleNamespace())\n","                    here = getattr(here, xx[i])\n","\n","        self.clear()\n","\n","    def clear(self):\n","        self.xx = {}\n","        self.aa = {}\n","        self.bb = {}\n","        self.hk = None\n","\n","    def save(self, target):\n","        target.xx = copy.deepcopy(self.xx)\n","        target.aa = copy.deepcopy(self.aa)\n","        target.bb = copy.deepcopy(self.bb)\n","        target.hk = copy.deepcopy(self.hk)\n","\n","    def load(self, target):\n","        self.xx = copy.deepcopy(target.xx)\n","        self.aa = copy.deepcopy(target.aa)\n","        self.bb = copy.deepcopy(target.bb)\n","        self.hk = copy.deepcopy(target.hk)\n","\n","    def LN(self, xx, w):\n","        return F.layer_norm(xx, (self.n_embd,), weight=w.weight, bias=w.bias)\n","\n","    def FF(self, xx, w, name):\n","        if name not in self.xx:\n","            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n","        x = xx * w.time_mix + self.xx[name] * (1 - w.time_mix)\n","        self.xx[name] = xx\n","\n","        r = torch.sigmoid(w.receptance.weight @ x)\n","        k = torch.square(torch.relu(w.key.weight @ x))\n","        kv = w.value.weight @ k\n","\n","        return r * kv\n","\n","    def SA(self, xx, w, name):\n","        if name not in self.xx:\n","            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n","            self.aa[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n","            self.bb[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n","        x = xx * w.time_mix + self.xx[name] * (1 - w.time_mix)\n","        self.xx[name] = xx\n","\n","        r = torch.sigmoid(w.receptance.weight @ x)\n","\n","        k = torch.exp(torch.clamp(w.key.weight @ x, max=RWKV_K_CLAMP))\n","        v = w.value.weight @ x\n","        kv = k * v\n","\n","        a = self.aa[name] + w.time_first * kv\n","        b = self.bb[name] + w.time_first * k\n","        self.aa[name] = w.time_decay * self.aa[name] + kv\n","        self.bb[name] = w.time_decay * self.bb[name] + k\n","\n","        rwkv = r * a / (b + RWKV_K_EPS)\n","\n","        return w.output.weight @ rwkv\n","\n","    def run(self, ctx):\n","        w = self.w\n","        x = w.emb.weight[ctx[-1]]\n","\n","        for i in range(self.n_layer):\n","            x = self.LN(x, w.blocks[i].ln1)\n","            if i == 0 and self.model_type == 'RWKV-ffnPre':\n","                x = x + self.FF(x, w.blocks[i].ffnPre, f'ffnPre.{i}')\n","            else:\n","                x = x + self.SA(x, w.blocks[i].att, f'att.{i}')\n","            x = self.LN(x, w.blocks[i].ln2)\n","            x = x + self.FF(x, w.blocks[i].ffn, f'ffn.{i}')\n","\n","        x = self.LN(x, w.ln_out)\n","\n","        if self.hk == None:\n","            self.hk = (w.head_k.weight @ x).unsqueeze(0)\n","        else:\n","            self.hk = torch.cat(\n","                [self.hk, (w.head_k.weight @ x).unsqueeze(0)], dim=0)\n","        if self.hk.shape[0] > self.ctx_len:\n","            self.hk = self.hk[-self.ctx_len:, :]\n","\n","        q = w.head_q.weight @ x\n","\n","        x = w.head.weight @ x\n","        x = x.cpu().numpy().tolist()\n","\n","        c = (self.hk @ q) / RWKV_HEAD_QK_DIM\n","        for i in range(len(c)):\n","            x[ctx[i]] += c[i]\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"15TcJ5z_9uZW"},"source":["GPT-3などに比較すれば、これでもかなり学習時間や計算コストが小さいが、次のように、文章生成を行うことができる点に注目されたい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uTd4Hai8Umc"},"outputs":[],"source":["torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cuda.matmul.allow_tf32 = True\n","np.set_printoptions(precision=4, suppress=True, linewidth=200)\n","\n","### Step 1: set model ##################################################################################\n","\n","ctx_len = 1024\n","n_layer = 6\n","n_embd = 512\n","model_type = 'RWKV'           # 'RWKV' or 'RWKV-ffnPre'\n","\n","# your trained model\n","MODEL_NAME = 'trained-v1'\n","WORD_NAME = 'vocab'           # the .json vocab (generated by train.py\n","\n","# ########## Uncomment these to test my 27M params enwik8 model ##########\n","# MODEL_NAME = 'enwik8-ppl1.65-6064-1024-RWKV-6-512-2022-03-25-21-05-13'\n","# WORD_NAME = 'enwik8-vocab'\n","# EVAL_DATA = 'enwik8'  # uncomment this for EVAL MODE (no text generation)\n","# ########################################################################\n","\n","# --> set UNKNOWN_CHAR to the rarest token in your vocab.json <--\n","# --> all unknown tokens in your context will be denoted by it <--\n","UNKNOWN_CHAR = ' '   # here we just set it to [space] for simplicity\n","\n","RUN_DEVICE = 'cpu'   # 'cpu' (already very fast) or 'cuda'\n","DEBUG_DEBUG = False  # True False - show softmax output\n","\n","### Step 2: set context ################################################################################\n","\n","context = \"\\nIn the\"       # ==> this is your prompt\n","\n","NUM_TRIALS = 10\n","LENGTH_PER_TRIAL = 500\n","\n","TEMPERATURE = 1.0\n","top_p = 0.7\n","top_p_newline = 0.9\n","\n","########################################################################################################\n","\n","print(f'Loading {MODEL_NAME}...')\n","model = RWKV_RNN(MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len)\n","tokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\n","\n","########################################################################################################\n","\n","if 'EVAL_DATA' in vars() or 'EVAL_DATA' in globals():\n","    print('Evaluating on ' + EVAL_DATA + ' ...')\n","\n","    data = open(EVAL_DATA, \"r\", encoding='utf-8').read()\n","\n","    loss_table = np.zeros(ctx_len)\n","\n","    N_SAMPLE = 1000\n","\n","    for iii in range(N_SAMPLE):\n","        pos = np.random.randint(0, len(data) - ctx_len-1)\n","        context = data[pos:pos+ctx_len+1]\n","        ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n","\n","        model.clear()\n","        for i in range(1, ctx_len+1):\n","            x = ctx[:i]\n","            out = model.run(x)\n","            prob = F.softmax(torch.tensor(out), dim=-1)\n","            loss_table[i-1] += -math.log(prob[ctx[i]])\n","\n","        print(f'Tested {iii+1} samples: avg_loss over ctx_len =',\n","              np.mean(loss_table) / (iii+1))\n","\n","    exit(0)\n","\n","########################################################################################################\n","\n","context = tokenizer.refine_context(context)\n","print('\\nYour prompt has ' + str(len(context)) + ' tokens.')\n","print('\\n--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. This will be much faster in future versions. <--\\n')\n","\n","for TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n","    t_begin = time.time_ns()\n","\n","    src_len = len(context)\n","    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n","    print(('-' * 30) + context, end='')\n","\n","    model.clear()\n","    if TRIAL == 0:\n","        init_state = types.SimpleNamespace()\n","        for i in range(src_len):\n","            x = ctx[:i+1]\n","            if i == src_len - 1:\n","                init_state.out = model.run(x)\n","            else:\n","                model.run(x)\n","        model.save(init_state)\n","    else:\n","        model.load(init_state)\n","\n","    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n","        x = ctx[:i+1]\n","        x = x[-ctx_len:]\n","\n","        if i == src_len:\n","            out = copy.deepcopy(init_state.out)\n","        else:\n","            out = model.run(x)\n","        if DEBUG_DEBUG:\n","            print('model', np.array(x), '==>', np.array(\n","                out), np.max(out), np.min(out))\n","\n","        char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n","                                       top_p_usual=top_p, top_p_newline=top_p_newline)\n","        char = char.item()\n","        print(tokenizer.itos[int(char)], end='', flush=True)\n","        ctx += [char]\n","    t_end = time.time_ns()\n","    print(\"\\n----------\", round((t_end - t_begin) / (10 ** 9), 2), end='s ')"]},{"cell_type":"markdown","metadata":{"id":"8sniUCV2eXg2"},"source":["課題1\n","\n","RWKVはなぜ期待されているのか、自分の言葉で纏めなさい"]},{"cell_type":"markdown","metadata":{"id":"BTNzcUIcedUt"},"source":["課題2\n","\n","その他、注目する技術は次々と登場している\n","\n","何でもよいので、最新の機械学習に関連する技術について調査を行い、簡単に纏めなさい"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}