{"cells":[{"cell_type":"markdown","metadata":{"id":"Ck0OgvaBxHVu"},"source":["# RWKV"]},{"cell_type":"markdown","metadata":{"id":"380RBGIhxBeR"},"source":["> 「変わることがなければ成長することもない  \n","> 成長することがなければ真に生きていない」  \n","> ビル・ゲイツ"]},{"cell_type":"markdown","metadata":{"id":"2_vdSl0ivzFj"},"source":["## RWKVの概要\n","\n","OpenAIが開発するChatGPTにおけるGPT4は強力であり、同様のモデルが様々登場しつつあるが、モデル動作に必要な計算資源も膨大であり、簡単に自宅やColabで動作させることはできないが、RWKVはこの問題を解決するのではないかと期待されている\n","\n","言語モデルは、LSTMなどRNNベースのモデルが利用されてきたが、Transformerの登場により状況が一変した\n","- Transformerにより並列処理が可能となり実行速度が向上した\n","- しかしながら、必要となる計算資源が膨大になった\n"]},{"cell_type":"markdown","metadata":{"id":"bbAsAeYpjaqV"},"source":["## RNNとTransformerの違いを再確認\n","\n","Recurrent Neural Networks (RNN)とTransformerの違いを再確認する\n","\n","ある文章があり、その単語のトークン列が$F[0], F[1], ... F[n]$とする\n","\n","- Transformer\n","  - Attention Weightを用いて$F[0] ... F[n-1]$の単語の依存関係から$F[n]$の単語を生成する\n","  - 文章全体の状態を保持して学習し、残差接続(residual connection)、Dropout、LayerNormなどを利用することで、層数を増やしても安定して学習を進めることができる\n","  - Self-Attentionは離れた位置の依存関係を学習できるが、シーケンス内のすべての要素と他のすべての要素との依存関係を計算するため、計算量とメモリ使用量がシーケンス長(トークン数)の2乗つまり、O(n^2)で大きくなるという欠点がある\n","  - 一方で計算を並列化することができるため、計算進度を高めることができる\n","\n","- RNN\n","  - $F[n-1]$の単語から$F[n]$の単語を生成というプロセスを繰り返して学習\n","  - 単一状態を保持し、これを繰り返し適用するため、離れた位置ほど単語の依存関係が失われていく\n","      - 計算を繰り返すため、勾配爆発や勾配消失を発生ひやすい\n","  - メモリと計算量は文章長に対して線形にスケールする\n","  - 並列化と拡張性の制限からtransformerと同等の性能を達成することが困難\n"]},{"cell_type":"markdown","metadata":{"id":"lmBeuRQwzrz0"},"source":["# RWKV(Receptance Weighted Key Value)とは?\n","\n","transformerの効率的な並列学習とRNNの効率的な推論の両方を兼ね備えたモデル\n","\n","名前は、利用する4つのパラメータ$R, W, K, V$に由来する\n"]},{"cell_type":"markdown","metadata":{"id":"IalKrapJKdP0"},"source":["## RWKVの特徴\n","\n","次のように纏めることができる\n","\n","- RNNベースのモデルを用いて、推論の高速化と省メモリ化を実現\n","  - 推論時のメモリ使用率は最も大きい14Bモデルでも3GB程度\n","- 数百億のパラメータまでスケールする(これが限界かどうかは不明であるが、スケール則は有効と思われる)、非Transformerアーキテクチャ\n","  - この規模はRNNベースのモデルでは達成できなかった\n","- 同一サイズのTransformerと同等の性能を発揮\n","- 学習時はTransformerと同様の動作であるため、並列化が容易\n","- 文章長が理論上無限となる(学習時の文章長に依存し、実際は1024程度)\n","- AttentionではなくRWKVモデルを利用する\n","\n","RWKVモデルは https://github.com/BlinkDL/ において公開されており、RWKV1からRWKV4までの4つのモデルが存在\n","\n","ここでは、RWKV2を用いるが、このモデルは推論にRNNモード、学習にTransformerモードを利用する"]},{"cell_type":"markdown","metadata":{"id":"lc7piu561e3Y"},"source":["## TransformerとRWKVの違い\n","\n","構造の違いを図を用いて表すと次の通り\n","\n","- RWKVは、特徴的なTime-mixing blockとChannel-mixing blockが存在\n","- Transformerと異なり、encoder-decoderモデルではない\n","- 全体の構造はTransformerと類似する\n","  - Time-mixing blockとmultihead attentionというブロックが同様の配置・連結された構造を持つが中身が異なる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer-rwkv.png\" width=700>\n"]},{"cell_type":"markdown","metadata":{"id":"_tJTWj6nPCF8"},"source":["# RWKVの詳細\n","\n","RWKVを特長づけるブロックとしてTime-mixing blockとChannel-mixing blockがある\n","\n","RWKVという名称の由来にもなっている4つのパラメータはTime-mixingブロックとChannel-mixingブロックで使用され、次の意味を持つ\n","- R:過去の情報の受容度を表現するReceptanceベクトル\n","- W:位置の重み減衰ベクトル。訓練可能なモデルパラメータ\n","- K:一般的なAttention Mechanismと同様のK(Key)ベクトル\n","- V:一般的なAttention Mechanismと同様のV(Value)ベクトル\n"]},{"cell_type":"markdown","metadata":{"id":"aF7YSS2NoEPl"},"source":["## TransformerからRWKVへ"]},{"cell_type":"markdown","metadata":{"id":"OOKLRNdFPosq"},"source":["### Transformer\n","\n","$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","ここでQ、K、Vはクエリ(Query)、キー(Key)、値(Value)を示し、キーの次元数$d_k$，シーケンス長(トークン数)$N$、次元数$d_v$である\n","\n","$F[t+1]$の予測に、$F[0]...F[t]$の文章と、現在の単語$x_t$と$F[t]$をそれぞれ比較して文脈全体の依存関係を考慮する\n","- 具体的には$Qx_t$と$K(F[0]...F[t])$の全ての単語の内積によりAttention Weightを求め、前の各状態$F[i]$と比較して類似度を求める\n","\n","すべてのQueryベクトルとKeyベクトルの内積を計算するため、計算量が$n^2$となる\n"]},{"cell_type":"markdown","metadata":{"id":"R9VIs20XppfQ"},"source":["### An Attention Free Transformer (AFT)\n","\n","Self-Attentionの代わりに普通の全結合層(Fully Connected layer)を利用したモデル\n","- 計算コストを大幅に削減\n","- 条件によって、Transformerと同等またはそれ以上のパフォーマンスを達成\n","\n","AFTは次のように求めることができる\n","\n","$$Attn^+(W,K,V)_t=\\frac{\\sum^t_{i=1}e^{\\omega t,i+k_i}v_i}{\\sum^t_{i=1}e^{\\omega t,i+k_i}}$$\n","\n","ここで$\\omega_{t,i} \\in R^{T\\times T}$は、学習した関係における位置バイアスを表す\n","\n","また、厳密ではないがRWKVを数的に表現すると、\n","\n","$$v_{i+1} = sigmoid(Rx[t])\\cdot \\sum^t_{i=0}{Attn^+(W,K,V)_t}$$\n","\n","となるため、一つ先($t+1$番目)の単語を予測する場合は、$t$番目の単語の予測に$sigmoid(R*F[t])$、$t～0$番目の単語の予測に$v_i$と$\\omega_{t,i+k_i}$の2つを用いる\n","\n","- なお、活性化関数は$sigmoid$である必要はないが、$sigmoid$の性能が高いという評価結果がある\n","  - この$sigmoid$の項は正規化せずreceptanceと呼ぶ\n","- MultiHeadAttentionは存在せず、各単語に$0～t$番目の単語との依存関係を求める必要ない\n","  - $e^{\\omega t,i+k_i}v_i$の項が依存関係を表現する\n","\n","この式により、RNNと同様再帰的に適用することで、$t-1$番目から、$t$番目を予測できる\n","- 一つ前の$v_{i-1}$に対して$e^\\omega$を掛け合わせ、$v_i$に関する項を足し合わせればよい\n","- この計算量の増大が、高々$O(n)$として表現できることから、計算コストを削減できる\n"]},{"cell_type":"markdown","metadata":{"id":"CMgqBL8zWUos"},"source":["## Time-mixing block\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ycnEUageMmZm"},"source":["### Time-mixing blockの動作内容\n","\n","RWKVの時刻$t$における各ベクトル・パラメータであるReceptanceベクトル($r_t$)、Key($k_t$)、Value($v_t$)は、次の式で表すことができる\n","\n","$$r_t=W_r\\cdot(\\mu_rx_t+(1−\\mu_r)x_{t−1}) \\\\\n","k_t=W_k\\cdot(\\mu_kx_t+(1−\\mu_k)x_{t−1}) \\\\\n","v_t=W_v\\cdot(\\mu_vx_t+(1−\\mu_v)x_{t−1})$$\n","\n","これらの値は時刻$t$での更新割合を制御するパラメータであり、0と1の間の値をとる\n","- $\\mu_r$​が大きいほど新しい入力$x_t$​の影響が強く、小さいほど過去の状態$x_{t−1}$​の影響が強くなる\n","\n","$mu_r$倍の新しい入力(x_t)と$1-mu_r$倍の和であることから、新しい入力と過去の状態との間で線形補間を行う式\n","- 従って、これを繰り返すことで再帰的に更新できる\n","\n","これらを使った演算の詳細は次の通りであり、An Attention Free Transformer (AFT）と類似している\n","\n","$$wk_vt=\\frac{\\sum_{i=1}^{t−1}e^{−(t−1−i)w+k_i}v_i+e^{u+k_t}v_t}{\\sum_{i=1}^{t−1}e^{−(t−1−i)w+k_i}+e^{u+k_t}}$$\n","$$o_t=W_o\\cdot(\\sigma(r_t)\\odot wk_vt)$$\n","\n","$wkv_t$​は、$Attn(Q,K,V)$と同様の役割を担っているが、$Q，K，V$の各要素はスカラーのため計算コストが小さい\n","\n","- 直感的には、時間$t$が増加するにつれてベクトル$o_t$​は長い履歴に依存することを示しているといえる\n","- ターゲットポジション$t$に対して、RWKVは位置間隔$[1, t]$での加重平均とレセプタンス$sigmoid(R)$と乗算している\n","  - 与えられたタイムステップ内で乗算され、異なるタイムステップで合計される\n","- 標準的なトランスフォーマーは全てのトークンのペア間でアテンションを計算するが、AFTは過去の時間ステップ全てにわたる加算の形でアテンションを計算するため、計算とメモリ利用効率が向上している"]},{"cell_type":"markdown","metadata":{"id":"rmPaCAVYhBGM"},"source":["### 時間減衰率(Time-Weighting)\n","\n","\n","RWKVはAFTに倣い、$\\omega_{t,i}$​をチャネルごとの時間減衰ベクトルとして定義している\n","\n","$$\\omega_{t,i} = -t(t-i)\\omega$$\n","\n","このモデルではTime-MixingおよびTime-Weightingというパラメータを導入している\n","- Time-Weightingを距離によるAttentionと呼ぶ\n","\n","Wはヘッド数、block_size(0～現在の時刻)、block_sizeの3次元で構成され、attに$\\mathbb{W}$を要素ごとかけた値をdropoutしている\n","\n","$\\mathbb{W}$は比較的小さいパラメータであり、このようなパラメ―タで離れた単語の依存関係を学習可能とする\n","\n","これは、異なる距離のトークン($W[:, block_size, block_size]$)が現在の単語attに与える影響は異なり、特に文章の後半は長い距離の$\\mathbb{W}$を用いる必要があるが、最初は履歴が小さく必要がない\n","\n","- SelfAttentionはそのような考慮がない\n","\n","実際には$\\mathbb{W}$は巡回行列でありバイアスを加算する必要があり、Time Weightingの導入によりPositionalEncodingが不要になる"]},{"cell_type":"markdown","metadata":{"id":"PVOwoFTJLpmf"},"source":["### Time-mixing blockの実装\n","\n","下記コードの関数`RWKV_TimeMix`の概要は次の通り\n","- 時刻$t$における入力の更新割合$µ_r$​はself.time_mixで表される\n","- また時刻$t−1$の入力xはnn.ZeroPad2dを用いて表されている\n","- これらを用いると入力$x$は、`x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)`と実装され、時間的にシフトした部分すなわち過去の情報と、シフトしていない部分すなわち現在の情報を適切な比率で混合した値へと変換している\n"]},{"cell_type":"markdown","metadata":{"id":"Q-s9U4cqWYJF"},"source":["## Channel-mixing block\n","\n","channel-mixing blockは次の式で表される\n","\n","$$r_t=W_r\\cdot(\\mu_rx_t+(1−\\mu_r)x_{t−1})$$\n","$$k_t=W_k\\cdot(\\mu_kx_t+(1−\\mu_k)x_{t−1})$$\n","$$o_t=\\sigma(r_t)\\odot(W_v\\cdot max(k_t,0)^2)$$\n","\n","Time-mixing blockは異なる時間ステップのトークン間の相互作用を管理するのに対して、Channel-mixing blockは同じ時間ステップ内の異なるチャンネル（または特徴）間の相互作用を管理する\n","\n","Channel-mixing blockは、全結合層や畳み込み層と同様の働きを持つ\n","- $\\sigma$はsquared ReLUを使用し、忘却ゲートの役割を果たす\n","\n","Channel-mixing blockの実装は、Time-mixing blockが理解できれば問題なく理解であろう"]},{"cell_type":"markdown","metadata":{"id":"gu8sUoklWba9"},"source":["# RWKVの特徴\n","\n","RWKVは学習時time-parallel mode (時間パラレルモード)が使用され、推論時（デコード時）はtime-sequential mode (時間シーケンシャルモード)が使用される\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/rwkv.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"NtnEww15S-YD"},"source":["## 時間パラレルモード\n","\n","学習時のモード\n","\n","時間パラレルモードは言葉の通り、時刻に関連する演算を並列して行う\n","\n","Time-mixing blockで説明したように新しい入力$x_t$と過去の状態との間で線形補間を行う\n","- 結果として、各タイムステップの計算が他のタイムステップの計算と独立に実行できる\n","- RNNとは異なり並列処理が可能"]},{"cell_type":"markdown","metadata":{"id":"eTK3Y6lpTJaA"},"source":["## 時間シーケンシャルモード\n","\n","推論時のモード\n","\n","学習時とは異なり、推論時にはRNNのような順次的なデコーディングを行う\n","\n","- このときRWKVはRNNと同様の構造を活用して動作する\n","  - これを時間シーケンシャルモードと呼ぶ\n","  - 各ステップの出力が次のステップの入力として用いられる\n","  - RNN同様、は出力トークンを一度に1つずつ生成し、あるトークンの生成は前のすべてのトークンの生成が完了した後に行わる\n","\n","RWKVはシーケンスの長さに関係なく一定の速度とメモリフットプリントを維持しする\n","- 長いシーケンスを効率的に処理できる\n","- 一方で、アテンションメカニズムを使用しているため、シーケンスの長さに比例してキャッシュの使用量が増加する\n","\n","以上からRWKVではTransfromerでは実現できなかった効率的なデコーディングが可能となる"]},{"cell_type":"markdown","metadata":{"id":"E8bZFEPLrlWr"},"source":["# ChatRWKV-RWKV4-Worldを試す\n","\n","transformerレベルの性能を持つRNNである、[RWKV](https://github.com/BlinkDL/RWKV-LM)を利用して、実際にRWKVを試す\n","- ここでは、[ChatRWKV](https://github.com/BlinkDL/RWKV-LM) RWKVの推論用モデルを利用する"]},{"cell_type":"markdown","source":["利用にあたって、学習済みモデルやドライブのマウント場所など、必要な設定を行う"],"metadata":{"id":"tTGHHbPgGF9x"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"4HUIQ3-rcw2O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724315998216,"user_tz":-540,"elapsed":23755,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e17bff99-7d6d-4b17-af1e-2ff784ae0797"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Saving models to /content/drive/MyDrive/rwkv-4-world-model\n"]}],"source":["save_models_to_drive = True # ドライブをマウントしてモデルを保存、利用する\n","drive_mount = '/content/drive' # ドライブをマウントする場所を指定する\n","model_dir = 'rwkv-4-world-model' # モデルを保存する場所を指定する\n","\n","import os\n","if save_models_to_drive:\n","    from google.colab import drive\n","    drive.mount(drive_mount)\n","    model_dir_path = f\"{drive_mount}/MyDrive/{model_dir}\" if save_models_to_drive else f\"/content/{model_dir}\"\n","else:\n","    model_dir_path = \"/content\"\n","\n","os.makedirs(f\"{model_dir_path}\", exist_ok=True)\n","\n","print(f\"Saving models to {model_dir_path}\")"]},{"cell_type":"code","source":["!git clone https://github.com/BlinkDL/ChatRWKV"],"metadata":{"id":"upx44PXgchfJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724316001323,"user_tz":-540,"elapsed":3112,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"592d8ea9-2d63-49cb-e37f-7a635159c46c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ChatRWKV'...\n","remote: Enumerating objects: 1977, done.\u001b[K\n","remote: Counting objects: 100% (884/884), done.\u001b[K\n","remote: Compressing objects: 100% (320/320), done.\u001b[K\n","remote: Total 1977 (delta 703), reused 632 (delta 547), pack-reused 1093 (from 1)\u001b[K\n","Receiving objects: 100% (1977/1977), 30.55 MiB | 14.01 MiB/s, done.\n","Resolving deltas: 100% (1120/1120), done.\n"]}]},{"cell_type":"code","source":["!pip install ninja tokenizers rwkv prompt_toolkit"],"metadata":{"id":"VozXngWX_JeP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724316005430,"user_tz":-540,"elapsed":4109,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"9640344b-8efd-4e64-b487-89b11e67ccfb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ninja\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n","Collecting rwkv\n","  Downloading rwkv-0.8.26-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: prompt_toolkit in /usr/local/lib/python3.10/dist-packages (3.0.47)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n","Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rwkv-0.8.26-py3-none-any.whl (406 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.2/406.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ninja, rwkv\n","Successfully installed ninja-1.11.1.1 rwkv-0.8.26\n"]}]},{"cell_type":"markdown","source":["## モデルの選択とダウンロード\n","\n"],"metadata":{"id":"AB039RVaGZUT"}},{"cell_type":"markdown","source":["最初に `model_dir` から `model_file` を検索する。\n","- 有効なパスでない場合、huggingfaceから `RWKV-4-World` モデルのダウンロードを試みる\n","\n","ここでは、日本語を利用するが、その他どのようなモデルがあるかは[repo](https://huggingface.co/BlinkDL/rwkv-4-world/)を参照すること\n","- RWKV-4-World-JPNtuned-7B: `RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth`\n","- RWKV-4-World-CHNtuned-7B: `RWKV-4-World-CHNtuned-7B-v1-20230709-ctx4096.pth`\n","\n","ダウンロードには、20分弱必要となる"],"metadata":{"id":"qpVCnsmvICd7"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"M-Mrn6tTeEub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724316968591,"user_tz":-540,"elapsed":963166,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f8eb3aeb-d33a-4c81-e4d9-56c9fb5c4a08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 'RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth' from https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth this may take a while\n","Using /content/drive/MyDrive/rwkv-4-world-model/RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth as base\n"]}],"source":["import urllib\n","\n","model_file = \"RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth\" # モデルを指定する(ここでは日本語モデルとする)\n","\n","model_path = f\"{model_dir_path}/{model_file}\"\n","if not os.path.exists(model_path):\n","    model_repo = f\"https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main\"\n","    model_url = f\"{model_repo}/{urllib.parse.quote_plus(model_file)}\"\n","    try:\n","        print(f\"Downloading '{model_file}' from {model_url} this may take a while\")\n","        urllib.request.urlretrieve(model_url, model_path)\n","        print(f\"Using {model_path} as base\")\n","    except Exception as e:\n","        print(f\"Model '{model_file}' doesn't exist\")\n","        raise Exception\n","else:\n","    print(f\"Using {model_path} as base\")"]},{"cell_type":"markdown","source":["## モデルのロード"],"metadata":{"id":"ncbQF6liIaSx"}},{"cell_type":"markdown","source":["Strategyの例:\n","- `cpu fp32`\n","- `cuda:0 fp16 -> cuda:1 fp16`\n","- `cuda fp16i8 *10 -> cuda fp16`\n","- `cuda fp16i8`\n","- `cuda fp16i8 -> cpu fp32 *10`\n","- `cuda fp16i8 *10+`\n","\n","RWKV_JIT_ONの例:\n","- `1` でJITコンパイラ有効(有効にするべき)\n","- `0` でJITコンパイラ無効\n","\n","JITコンパイラを利用するため、 torch 1.13+ が必要\n","\n","RWKV_CUDA_ONの例:\n","- `1` でCUDA有効(GPU使用を推奨)\n","- `0` でCUDA無効\n","\n","このモデルを利用するには、ハイメモリが必要となる"],"metadata":{"id":"54DJNDYCIkGa"}},{"cell_type":"code","source":["import os, copy, types, gc, sys\n","import numpy as np\n","import torch\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cuda.matmul.allow_tf32 = True\n","\n","strategy = 'cuda fp16'\n","RWKV_JIT_ON = '1'\n","RWKV_CUDA_ON = '1'\n","os.environ[\"RWKV_JIT_ON\"] = '1'\n","os.environ[\"RWKV_CUDA_ON\"] = '1'"],"metadata":{"id":"WxMnXXJYI_RZ","executionInfo":{"status":"ok","timestamp":1724316972119,"user_tz":-540,"elapsed":3531,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## モデルのセットアップ\n"],"metadata":{"id":"jpRtBktdJgrB"}},{"cell_type":"markdown","source":["CHAT_LANGは日本語でよい\n","\n","PROMPT_FILEは、次の違いがある\n","- -1.pyとする場合は、UserとBotでQ&A形式のプロンプトとして運用する\n","- -2.pyとする場合は、BobとAliceのチャットのような、会話プロンプトとして運用する\n","\n","ここでは、会話プロンプトとしている\n","\n","パラメータ説明は次の通り\n","- GEN_TEMP : 温度\n","- GEN_TOP_P : top-P\n","- GEN_alpha_presence : 同じトークンの繰り返しを減らすため出力トークンに課すペナルティ (-2〜2)\n","- GEN_penalty_decay : 同じテキスト出力をへらすため出力トークンに課すペナルティ (-2〜2)\n","- GEN_penalty_decay : 新しいトークンがこれまでのテキストに表示されているかどうかに基づいてペナルティを課し、モデルが新しいトピックについて話す可能性を高める\n","\n","詳細は、[API](https://platform.openai.com/docs/api-reference/completions/create)仕様を参照のこと\n","\n","また、CHUNK_LENは、VRAMを節約するために入力をチャンクに分割する際のチャンクサイズを指定する\n","\n","より良いチャットとQAのためには、tempの値、およびtop-pの値を減らし、繰り返しのペナルティを増やすとよい\n","- この点について、https://platform.openai.com/docs/api-reference/parameter-details の解説を参照するとよい\n","- Q&A の精度を高る、特に多様性を減らすために、top_p を0.5、0.2、0.1 などといった小さな値にするとよいであろう"],"metadata":{"id":"tjtNPfRQJwvm"}},{"cell_type":"code","source":["CHAT_LANG = 'Japanese'\n","PROMPT_FILE = f'/content/ChatRWKV/v2/prompt/default/{CHAT_LANG}-2.py'\n","\n","CHAT_LEN_SHORT = 40\n","CHAT_LEN_LONG = 150\n","FREE_GEN_LEN = 256\n","\n","GEN_TEMP = 1.2\n","GEN_TOP_P = 0.5\n","GEN_alpha_presence = 0.4\n","GEN_alpha_frequency = 0.4\n","GEN_penalty_decay = 0.996\n","AVOID_REPEAT = '，：？！'\n","\n","CHUNK_LEN = 256\n","\n","print(f'\\n{CHAT_LANG} - {strategy} - {PROMPT_FILE}')\n","from rwkv.model import RWKV\n","from rwkv.utils import PIPELINE\n","\n","def load_prompt(PROMPT_FILE):\n","    variables = {}\n","    with open(PROMPT_FILE, 'rb') as file:\n","        exec(compile(file.read(), PROMPT_FILE, 'exec'), variables)\n","    user, bot, interface, init_prompt = variables['user'], variables['bot'], variables['interface'], variables['init_prompt']\n","    init_prompt = init_prompt.strip().split('\\n')\n","    for c in range(len(init_prompt)):\n","        init_prompt[c] = init_prompt[c].strip().strip('\\u3000').strip('\\r')\n","    init_prompt = '\\n' + ('\\n'.join(init_prompt)).strip() + '\\n\\n'\n","    return user, bot, interface, init_prompt\n","\n","# Load Model\n","\n","print(f'Loading model - {model_path}')\n","model = RWKV(model=model_path, strategy=strategy)\n","\n","pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\")\n","END_OF_TEXT = 0\n","END_OF_LINE = 11\n","\n","# pipeline = PIPELINE(model, \"cl100k_base\")\n","# END_OF_TEXT = 100257\n","# END_OF_LINE = 198\n","\n","model_tokens = []\n","model_state = None\n","\n","AVOID_REPEAT_TOKENS = []\n","for i in AVOID_REPEAT:\n","    dd = pipeline.encode(i)\n","    assert len(dd) == 1\n","    AVOID_REPEAT_TOKENS += dd\n","\n","########################################################################################################\n","\n","def run_rnn(tokens, newline_adj = 0):\n","    global model_tokens, model_state\n","\n","    tokens = [int(x) for x in tokens]\n","    model_tokens += tokens\n","    # print(f'### model ###\\n{tokens}\\n[{pipeline.decode(model_tokens)}]')\n","\n","    while len(tokens) > 0:\n","        out, model_state = model.forward(tokens[:CHUNK_LEN], model_state)\n","        tokens = tokens[CHUNK_LEN:]\n","\n","    out[END_OF_LINE] += newline_adj # adjust \\n probability\n","\n","    if model_tokens[-1] in AVOID_REPEAT_TOKENS:\n","        out[model_tokens[-1]] = -999999999\n","    return out\n","\n","all_state = {}\n","def save_all_stat(srv, name, last_out):\n","    n = f'{name}_{srv}'\n","    all_state[n] = {}\n","    all_state[n]['out'] = last_out\n","    all_state[n]['rnn'] = copy.deepcopy(model_state)\n","    all_state[n]['token'] = copy.deepcopy(model_tokens)\n","\n","def load_all_stat(srv, name):\n","    global model_tokens, model_state\n","    n = f'{name}_{srv}'\n","    model_state = copy.deepcopy(all_state[n]['rnn'])\n","    model_tokens = copy.deepcopy(all_state[n]['token'])\n","    return all_state[n]['out']\n","\n","# Model only saw '\\n\\n' as [187, 187] before, but the tokenizer outputs [535] for it at the end\n","def fix_tokens(tokens):\n","        return tokens"],"metadata":{"id":"L1wBpmJjQXSR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724317037196,"user_tz":-540,"elapsed":65081,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"dc204519-9218-4120-b5f2-935141fc119f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Japanese - cuda fp16 - /content/ChatRWKV/v2/prompt/default/Japanese-2.py\n"]},{"output_type":"stream","name":"stderr","text":["Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n","Creating extension directory /root/.cache/torch_extensions/py310_cu121/wkv_cuda...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/wkv_cuda/build.ninja...\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n","If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n","  warnings.warn(\n","Building extension module wkv_cuda...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","Loading extension module wkv_cuda...\n"]},{"output_type":"stream","name":"stdout","text":["Loading model - /content/drive/MyDrive/rwkv-4-world-model/RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth\n","RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n","\n","Loading /content/drive/MyDrive/rwkv-4-world-model/RWKV-4-World-JPNtuned-7B-v1-20230718-ctx4096.pth ...\n","Model detected: v4.0\n","Strategy: (total 32+1=33 layers)\n","* cuda [float16, float16], store 33 layers\n","0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n","emb.weight                        f16      cpu  65536  4096       \n","blocks.0.ln1.weight               f16   cuda:0   4096             \n","blocks.0.ln1.bias                 f16   cuda:0   4096             \n","blocks.0.ln2.weight               f16   cuda:0   4096             \n","blocks.0.ln2.bias                 f16   cuda:0   4096             \n","blocks.0.att.time_decay           f32   cuda:0   4096             \n","blocks.0.att.time_first           f32   cuda:0   4096             \n","blocks.0.att.time_mix_k           f16   cuda:0   4096             \n","blocks.0.att.time_mix_v           f16   cuda:0   4096             \n","blocks.0.att.time_mix_r           f16   cuda:0   4096             \n","blocks.0.att.key.weight           f16   cuda:0   4096  4096       \n","blocks.0.att.value.weight         f16   cuda:0   4096  4096       \n","blocks.0.att.receptance.weight    f16   cuda:0   4096  4096       \n","blocks.0.att.output.weight        f16   cuda:0   4096  4096       \n","blocks.0.ffn.time_mix_k           f16   cuda:0   4096             \n","blocks.0.ffn.time_mix_r           f16   cuda:0   4096             \n","blocks.0.ffn.key.weight           f16   cuda:0   4096 16384       \n","blocks.0.ffn.receptance.weight    f16   cuda:0   4096  4096       \n","blocks.0.ffn.value.weight         f16   cuda:0  16384  4096       \n","............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n","blocks.31.ln1.weight              f16   cuda:0   4096             \n","blocks.31.ln1.bias                f16   cuda:0   4096             \n","blocks.31.ln2.weight              f16   cuda:0   4096             \n","blocks.31.ln2.bias                f16   cuda:0   4096             \n","blocks.31.att.time_decay          f32   cuda:0   4096             \n","blocks.31.att.time_first          f32   cuda:0   4096             \n","blocks.31.att.time_mix_k          f16   cuda:0   4096             \n","blocks.31.att.time_mix_v          f16   cuda:0   4096             \n","blocks.31.att.time_mix_r          f16   cuda:0   4096             \n","blocks.31.att.key.weight          f16   cuda:0   4096  4096       \n","blocks.31.att.value.weight        f16   cuda:0   4096  4096       \n","blocks.31.att.receptance.weight   f16   cuda:0   4096  4096       \n","blocks.31.att.output.weight       f16   cuda:0   4096  4096       \n","blocks.31.ffn.time_mix_k          f16   cuda:0   4096             \n","blocks.31.ffn.time_mix_r          f16   cuda:0   4096             \n","blocks.31.ffn.key.weight          f16   cuda:0   4096 16384       \n","blocks.31.ffn.receptance.weight   f16   cuda:0   4096  4096       \n","blocks.31.ffn.value.weight        f16   cuda:0  16384  4096       \n","ln_out.weight                     f16   cuda:0   4096             \n","ln_out.bias                       f16   cuda:0   4096             \n","head.weight                       f16   cuda:0   4096 65536       \n"]}]},{"cell_type":"markdown","source":["## デモの出力"],"metadata":{"id":"WLXK6wXZNInK"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"p0lOYL6PFvpN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724317039782,"user_tz":-540,"elapsed":2590,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"d4ece0c8-a5fe-4501-82cd-93a0d6af483e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Run prompt...\n","\n","以下は、Aliceという女の子とその友人Bobの間で行われた会話です。 Aliceはとても賢く、想像力があり、友好的です。 AliceはBobに反対することはなく、AliceはBobに質問するのは苦手です。 AliceはBobに自分のことや自分の意見をたくさん伝えるのが好きです。 AliceはいつもBobに親切で役に立つ、有益なアドバイスをしてくれます。\n","\n","Bob: こんにちはAlice、調子はどうですか？\n","Alice: こんにちは！元気ですよ。あたなはどうですか？\n","\n","Bob: 元気ですよ。君に会えて嬉しいよ。見て、この店ではお茶とジュースが売っているよ。\n","Alice: 本当ですね。中に入りましょう。大好きなモカラテを飲んでみたいです！\n","\n","Bob: モカラテって何ですか？\n","Alice: モカラテはエスプレッソ、ミルク、チョコレート、泡立てたミルクから作られた飲み物です。香りはとても甘いです。\n","\n","Bob: それは美味しそうですね。今度飲んでみます。しばらく私とおしゃべりしてくれますか？\n","Alice: もちろん！ご質問やアドバイスがあれば、喜んでお答えします。専門的な知識には自信がありますよ。どうぞよろしくお願いいたします！\n","\n"]}],"source":["# Run inference\n","print(f'\\nRun prompt...')\n","\n","user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n","out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n","save_all_stat('', 'chat_init', out)\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","srv_list = ['dummy_server']\n","for s in srv_list:\n","    save_all_stat(s, 'chat', out)\n","\n","def reply_msg(msg):\n","    print(f'{bot}{interface} {msg}\\n')\n","\n","def on_message(message):\n","    global model_tokens, model_state, user, bot, interface, init_prompt\n","\n","    srv = 'dummy_server'\n","\n","    msg = message.replace('\\\\n','\\n').strip()\n","\n","    x_temp = GEN_TEMP\n","    x_top_p = GEN_TOP_P\n","    if (\"-temp=\" in msg):\n","        x_temp = float(msg.split(\"-temp=\")[1].split(\" \")[0])\n","        msg = msg.replace(\"-temp=\"+f'{x_temp:g}', \"\")\n","        # print(f\"temp: {x_temp}\")\n","    if (\"-top_p=\" in msg):\n","        x_top_p = float(msg.split(\"-top_p=\")[1].split(\" \")[0])\n","        msg = msg.replace(\"-top_p=\"+f'{x_top_p:g}', \"\")\n","        # print(f\"top_p: {x_top_p}\")\n","    if x_temp <= 0.2:\n","        x_temp = 0.2\n","    if x_temp >= 5:\n","        x_temp = 5\n","    if x_top_p <= 0:\n","        x_top_p = 0\n","    msg = msg.strip()\n","\n","    if msg == '+reset':\n","        out = load_all_stat('', 'chat_init')\n","        save_all_stat(srv, 'chat', out)\n","        reply_msg(\"Chat reset.\")\n","        return\n","\n","    # use '+prompt {path}' to load a new prompt\n","    elif msg[:8].lower() == '+prompt ':\n","        print(\"Loading prompt...\")\n","        try:\n","            PROMPT_FILE = msg[8:].strip()\n","            user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n","            out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n","            save_all_stat(srv, 'chat', out)\n","            print(\"Prompt set up.\")\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","        except:\n","            print(\"Path error.\")\n","\n","    elif msg[:5].lower() == '+gen ' or msg[:3].lower() == '+i ' or msg[:4].lower() == '+qa ' or msg[:4].lower() == '+qq ' or msg.lower() == '+++' or msg.lower() == '++':\n","\n","        if msg[:5].lower() == '+gen ':\n","            new = '\\n' + msg[5:].strip()\n","            # print(f'### prompt ###\\n[{new}]')\n","            model_state = None\n","            model_tokens = []\n","            out = run_rnn(pipeline.encode(new))\n","            save_all_stat(srv, 'gen_0', out)\n","\n","        elif msg[:3].lower() == '+i ':\n","            msg = msg[3:].strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n","            new = f'''\n","Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","# Instruction:\n","{msg}\n","\n","# Response:\n","'''\n","            # print(f'### prompt ###\\n[{new}]')\n","            model_state = None\n","            model_tokens = []\n","            out = run_rnn(pipeline.encode(new))\n","            save_all_stat(srv, 'gen_0', out)\n","\n","        elif msg[:4].lower() == '+qq ':\n","            new = '\\nQ: ' + msg[4:].strip() + '\\nA:'\n","            # print(f'### prompt ###\\n[{new}]')\n","            model_state = None\n","            model_tokens = []\n","            out = run_rnn(pipeline.encode(new))\n","            save_all_stat(srv, 'gen_0', out)\n","\n","        elif msg[:4].lower() == '+qa ':\n","            out = load_all_stat('', 'chat_init')\n","\n","            real_msg = msg[4:].strip()\n","            new = f\"{user}{interface} {real_msg}\\n\\n{bot}{interface}\"\n","            # print(f'### qa ###\\n[{new}]')\n","\n","            out = run_rnn(pipeline.encode(new))\n","            save_all_stat(srv, 'gen_0', out)\n","\n","        elif msg.lower() == '+++':\n","            try:\n","                out = load_all_stat(srv, 'gen_1')\n","                save_all_stat(srv, 'gen_0', out)\n","            except:\n","                return\n","\n","        elif msg.lower() == '++':\n","            try:\n","                out = load_all_stat(srv, 'gen_0')\n","            except:\n","                return\n","\n","        begin = len(model_tokens)\n","        out_last = begin\n","        occurrence = {}\n","        for i in range(FREE_GEN_LEN+100):\n","            for n in occurrence:\n","                out[n] -= (GEN_alpha_presence + occurrence[n] * GEN_alpha_frequency)\n","            token = pipeline.sample_logits(\n","                out,\n","                temperature=x_temp,\n","                top_p=x_top_p,\n","            )\n","            if token == END_OF_TEXT:\n","                break\n","            for xxx in occurrence:\n","                occurrence[xxx] *= GEN_penalty_decay\n","            if token not in occurrence:\n","                occurrence[token] = 1\n","            else:\n","                occurrence[token] += 1\n","\n","            if msg[:4].lower() == '+qa ':# or msg[:4].lower() == '+qq ':\n","                out = run_rnn([token], newline_adj=-2)\n","            else:\n","                out = run_rnn([token])\n","\n","            xxx = pipeline.decode(model_tokens[out_last:])\n","            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n","                print(xxx, end='', flush=True)\n","                out_last = begin + i + 1\n","                if i >= FREE_GEN_LEN:\n","                    break\n","        print('\\n')\n","        # send_msg = pipeline.decode(model_tokens[begin:]).strip()\n","        # print(f'### send ###\\n[{send_msg}]')\n","        # reply_msg(send_msg)\n","        save_all_stat(srv, 'gen_1', out)\n","\n","    else:\n","        if msg.lower() == '+':\n","            try:\n","                out = load_all_stat(srv, 'chat_pre')\n","            except:\n","                return\n","        else:\n","            out = load_all_stat(srv, 'chat')\n","            msg = msg.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n","            new = f\"{user}{interface} {msg}\\n\\n{bot}{interface}\"\n","            # print(f'### add ###\\n[{new}]')\n","            out = run_rnn(pipeline.encode(new), newline_adj=-999999999)\n","            save_all_stat(srv, 'chat_pre', out)\n","\n","        begin = len(model_tokens)\n","        out_last = begin\n","        print(f'{bot}{interface}', end='', flush=True)\n","        occurrence = {}\n","        for i in range(999):\n","            if i <= 0:\n","                newline_adj = -999999999\n","            elif i <= CHAT_LEN_SHORT:\n","                newline_adj = (i - CHAT_LEN_SHORT) / 10\n","            elif i <= CHAT_LEN_LONG:\n","                newline_adj = 0\n","            else:\n","                newline_adj = min(3, (i - CHAT_LEN_LONG) * 0.25) # MUST END THE GENERATION\n","\n","            for n in occurrence:\n","                out[n] -= (GEN_alpha_presence + occurrence[n] * GEN_alpha_frequency)\n","            token = pipeline.sample_logits(\n","                out,\n","                temperature=x_temp,\n","                top_p=x_top_p,\n","            )\n","            # if token == END_OF_TEXT:\n","            #     break\n","            for xxx in occurrence:\n","                occurrence[xxx] *= GEN_penalty_decay\n","            if token not in occurrence:\n","                occurrence[token] = 1\n","            else:\n","                occurrence[token] += 1\n","\n","            out = run_rnn([token], newline_adj=newline_adj)\n","            out[END_OF_TEXT] = -999999999  # disable <|endoftext|>\n","\n","            xxx = pipeline.decode(model_tokens[out_last:])\n","            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n","                print(xxx, end='', flush=True)\n","                out_last = begin + i + 1\n","\n","            send_msg = pipeline.decode(model_tokens[begin:])\n","            if '\\n\\n' in send_msg:\n","                send_msg = send_msg.strip()\n","                break\n","\n","            # send_msg = pipeline.decode(model_tokens[begin:]).strip()\n","            # if send_msg.endswith(f'{user}{interface}'): # warning: needs to fix state too !!!\n","            #     send_msg = send_msg[:-len(f'{user}{interface}')].strip()\n","            #     break\n","            # if send_msg.endswith(f'{bot}{interface}'):\n","            #     send_msg = send_msg[:-len(f'{bot}{interface}')].strip()\n","            #     break\n","\n","        # print(f'{model_tokens}')\n","        # print(f'[{pipeline.decode(model_tokens)}]')\n","\n","        # print(f'### send ###\\n[{send_msg}]')\n","        # reply_msg(send_msg)\n","        save_all_stat(srv, 'chat', out)\n","\n","########################################################################################################\n","\n","if CHAT_LANG == 'English':\n","    HELP_MSG = '''Commands:\n","say something --> chat with bot. use \\\\n for new line.\n","+ --> alternate chat reply\n","+reset --> reset chat\n","\n","+gen YOUR PROMPT --> free single-round generation with any prompt. use \\\\n for new line.\n","+i YOUR INSTRUCT --> free single-round generation with any instruct. use \\\\n for new line.\n","+++ --> continue last free generation (only for +gen / +i)\n","++ --> retry last free generation (only for +gen / +i)\n","\n","Now talk with the bot and enjoy. Remember to +reset periodically to clean up the bot's memory. Use RWKV-4 14B (especially https://huggingface.co/BlinkDL/rwkv-4-raven) for best results.\n","'''\n","elif CHAT_LANG == 'Chinese':\n","    HELP_MSG = f'''指令:\n","直接输入内容 --> 和机器人聊天（建议问机器人问题），用\\\\n代表换行，必须用 Raven 模型\n","+ --> 让机器人换个回答\n","+reset --> 重置对话，请经常使用 +reset 重置机器人记忆\n","\n","+i 某某指令 --> 问独立的问题（忽略聊天上下文），用\\\\n代表换行，必须用 Raven 模型\n","+gen 某某内容 --> 续写内容（忽略聊天上下文），用\\\\n代表换行，写小说用 testNovel 模型\n","+++ --> 继续 +gen / +i 的回答\n","++ --> 换个 +gen / +i 的回答\n","\n","作者：彭博 请关注我的知乎: https://zhuanlan.zhihu.com/p/603840957\n","如果喜欢，请看我们的优质护眼灯: https://withablink.taobao.com\n","\n","中文 Novel 模型，可以试这些续写例子（不适合 Raven 模型）：\n","+gen “区区\n","+gen 以下是不朽的科幻史诗长篇巨著，描写细腻，刻画了数百位个性鲜明的英雄和宏大的星际文明战争。\\\\n第一章\n","+gen 这是一个修真世界，详细世界设定如下：\\\\n1.\n","'''\n","elif CHAT_LANG == 'Japanese':\n","    HELP_MSG = f'''コマンド:\n","直接入力 --> ボットとチャットする．改行には\\\\nを使用してください．\n","+ --> ボットに前回のチャットの内容を変更させる．\n","+reset --> 対話のリセット．メモリをリセットするために，+resetを定期的に実行してください．\n","\n","+i インストラクトの入力 --> チャットの文脈を無視して独立した質問を行う．改行には\\\\nを使用してください．\n","+gen プロンプトの生成 --> チャットの文脈を無視して入力したプロンプトに続く文章を出力する．改行には\\\\nを使用してください．\n","+++ --> +gen / +i の出力の回答を続ける．\n","++ --> +gen / +i の出力の再生成を行う.\n","\n","ボットとの会話を楽しんでください。また、定期的に+resetして、ボットのメモリをリセットすることを忘れないようにしてください。\n","'''\n","\n","print(f'{pipeline.decode(model_tokens)}'.replace(f'\\n\\n{bot}',f'\\n{bot}'), end='')\n","\n","########################################################################################################\n","\n","\n"]},{"cell_type":"markdown","source":["## チャット"],"metadata":{"id":"FXsmk5MGNXSg"}},{"cell_type":"markdown","source":["このセルを実行するとチャットが開始される\n","- 入力欄にメッセージを入力するとそれに対する返答を得ることができる\n","\n","次のコマンドが用意されている:\n","- `+`：代替のチャット応答を取得する\n","- `+reset`：チャットをリセットする\n","- `+gen YOUR PROMPT`：任意のプロンプトでの無料(記録しない)の単一ラウンド生成に使用する\n","- `+i YOUR INSTRUCT`：任意の指示での無料の単一ラウンド生成に使用する\n","- `+++`：最後の無料生成を続行するために使用する（`+gen` / `+i`のみ）\n","- `++`：最後の無料生成を再試行するために使用する（`+gen` / `+i`のみ）\n","\n","RWKVは過去の入力内容を保持し続けるため、定期的にボットのメモリをクリーンアップするために、`+reset`を実行するのを忘れないようにすること\n"],"metadata":{"id":"G69UwVfdNbzx"}},{"cell_type":"code","source":["from prompt_toolkit import prompt\n","while True:\n","    msg = input(\"Bob: \")\n","    if len(msg.strip()) > 0:\n","        on_message(msg)\n","    else:\n","        print('Error: please say something')"],"metadata":{"id":"AtdWuOEQN0S4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e52321a-6a6a-4d00-eb15-c51b69e7ecf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bob: こんにちは\n","Alice: こんにちはBob、調子はどうですか？\n","\n","Bob: 新しいDNNモデルであるRWKVについて学んでいます。なにに注意すればよいですか？\n","Alice: こんにちはBob、調子はとても良いです。RWKVについて学んでいますか？\n","\n","Bob: はい。RWKVについて学んでいます。なにに注意すればよいですか？\n","Alice: はい。RWKVについて学んでいますか？\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6JGsM1LVxoXO"},"source":["# RWKVの学習"]},{"cell_type":"markdown","metadata":{"id":"Op-Emz2dipPX"},"source":["実際にRWKVを用いた学習を行うことができる。ただし、学習にGoogle Colaboratory ProでA100を利用した場合でも10時間強程度必要であり、T4などでは24時間を超えるため学習が終了しない。高価なA100を利用し、基本的に放置になるのと同時に、それなりの課金が必要となる\n","\n","従って、授業内容としては適さないと判断しており、ここでは、そのリンクのみ示すとして、実際に実行することは避ける\n","- また、このリンクについて、動作を完全に保証するものではない点も留意願いたい\n","- 学習を行うノートブックは、mlsys-text-O-RWKV-Learning.ipynbである\n"]},{"cell_type":"markdown","metadata":{"id":"8sniUCV2eXg2"},"source":["課題1\n","\n","RWKVはなぜ期待されているのか、自分の言葉で纏めなさい"]},{"cell_type":"markdown","metadata":{"id":"BTNzcUIcedUt"},"source":["課題2\n","\n","その他、注目する技術は次々と登場している\n","\n","何でもよいので、最新の機械学習に関連する技術について調査を行い、簡単に纏めなさい"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}