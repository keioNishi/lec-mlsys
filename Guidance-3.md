# どうして学習できるのか？

授業でも扱うが、まず、シンプルなネットワークについて考える
- 学習とは、教師データを用いてネットワークの重み調整を行うことである
- 教師データは、入力とその正解のセットで構成される

つまり学習とは、図の重み($w_i$)の値を調整することである

<img src="http://class.west.sd.keio.ac.jp/dataai/text/guidance2.png">

シンプルなネットワークでは次の方針が取られる
1. 重みとバイアスを初期化する（これは適当で構わない）
1. データ（バッチ）をニューラルネットワークに入力し出力を得る
1. この出力と正解の誤差を損失関数で計算
1. 誤差をへらすように重みを修正
1. 正しいといえる重みになるまで、出力を得るところからやり直す

これを勾配降下法と呼ぶ
- 近づけるときにどれだけ寄せるかの指標が「学習率」
  - 一発で正解値を出すのは複雑すぎてほぼ無理
  - そこで次第に正しい値に近づける方針を取る
 
<img src="http://class.west.sd.keio.ac.jp/dataai/text/guidance27.png" width=500>

## そしてとんでもない世界へ

例えば、シンプルな画像認識では、どの程度多層なのか？どの程度計算が大変なのかをみてみる

シンプルな物体認識DNNとして知られるTiny-YoLoのネットワークは次の通り

<img src="http://class.west.sd.keio.ac.jp/dataai/text/guidance28.png">

- 例えば、(ここでは厳密さは省き、詳細は改めて説明する)416x3は、カラー画像を416x416ピクセルx3の情報とし、これを208x208x16個のノードへ畳み込み、次に104x104x32のノードと畳み込みといった具合に、全23層を情報が伝わるように構成されている
- 大量にノードが存在しており、これらの重みを全て安定させ定常状態になるようにする必要がある
- 「まじかーーーーー」と思いますよね。現実はもっと、もっと多いです

こんなものを安定的に学習させる技術ができてしまった
- これがDNNの発展を推し進めた一番の理由

## ここまでのまとめ

DLブームは本物

- 何が凄いのか？
  - 端的には、パラメータを山のように増やしても過学習がなぜか起きないこと
  - これまでの常識がすべて覆った
- 説明可能とかくだらないことに拘るのは年寄り
  - 人間は説明不可能、なら人間を研究するな
- この領域は日進月歩
  - 物体識別精度の記録は日々更新
  - 今学んでいる内容も将来メジャーとは言えない
- 自由度は高い
  - 一方で、テンプレ的な、こうあるべきネットワークや、なんじゃこりゃネットワークも存在
  - 「先人の知恵を学ぶ」ことに主眼を置く

一方で
- 世界の頭脳と金がモデル提案にまい進している現時点で勝負を挑むなら相当覚悟した方が良い
- たかが画像分類でも既におぞましいパラメータ数、とんでもない精度(人間を簡単に超えている)

<img src="http://class.west.sd.keio.ac.jp/dataai/text/guidance29.png">


