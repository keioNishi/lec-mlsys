{"cells":[{"cell_type":"markdown","metadata":{"id":"bHZ54p4iIfem"},"source":["---\n",">「不用意にもらす言葉こそ、ほんとうらしいものをふくんでいるのだ。 」\n",">\n","> 太宰治\n","---"]},{"cell_type":"markdown","metadata":{"id":"hTExuC4mcoQo"},"source":["# はじめに\n","\n","ここでは、LangChainを主として、ChatGPTなどのLLMモデルを便利に使うための仕組みについて学ぶ\n","\n","なお、ChatGPTを対象とする場合、ChatGPTには、GPTsという独自のChatGPTをNoCodeで作成するツールが準備されている\n","- 従って、以下で説明する内容の多くは、この機能を用いて実現できる\n","- ただし、仕様がすぐに変更されるなど、混乱している状況にあるため、各自で触って試してみるとよい\n","- GPTsの開発状況により状況が大きく変わる可能性がある点に注意する事"]},{"cell_type":"markdown","metadata":{"id":"L9HXMz2Zvn8D"},"source":["# LangChain\n","LangChainは日々更新されている\n","- これは、この授業テキスト全般に言えることであるが、特に後半は更新が頻繁に行われている\n","- アップデートにより実行できない場合もあるが、その場合は速やかに申し出ること\n","\n","その前に、OpenAPIのChat APIについて学ぶ\n","\n","なお、このノートブックは、GPUを使わないCPUランタイムを利用している\n","\n","***注意***\n","\n","ChatGPTのAPI利用について、無償料金枠が2024年時点でなくなっている\n","\n","利用において、次のような文章を含むエラーが出力された場合は、しばらく待って再度実行する必要がある\n","\n","```\n","WARNING:RateLimitError: Rate limit reached for default-gpt-3.5-turbo on requests per min. Limit: 3 / min. Please try again in 20s.\n","```\n","\n","もし、まとめて実行する場合、途中で実行を待ってスロットを使いつくさないようにする必要があるため、次の設定を行うとよい\n","- ***制約がかかった場合は、openai_wait = Trueにすること***\n","- 利用にはまずAPIキーを発行してください"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WMatVzE3Xfaq","executionInfo":{"status":"ok","timestamp":1768067835580,"user_tz":-540,"elapsed":8,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["import os\n","import time\n","#openai_wait = True\n","openai_wait = False"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10416,"status":"ok","timestamp":1768067847158,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"1vnn8kmhfNuN","outputId":"388add90-32c3-4110-8c5b-20837aaba2b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Collecting nltk\n","  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.9.1\n","    Uninstalling nltk-3.9.1:\n","      Successfully uninstalled nltk-3.9.1\n","Successfully installed nltk-3.9.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}],"source":["!pip install --upgrade nltk\n","import nltk"]},{"cell_type":"markdown","metadata":{"id":"-dyt3hia9vY1"},"source":["# Chat APIを利用する\n"]},{"cell_type":"markdown","metadata":{"id":"3ojZHInfAtB9"},"source":["## API Keyの発行\n","\n","openai.comにアクセスし、DASHBOARDメニューにあるAPI keysでAPI keyを発行する\n","- セキュリティのため、プロジェクトごとに異なるキーを利用すること\n","- ここでは、dataai-keyという鍵をつくるとよい\n","- 発行された鍵をコピーしておくこと"]},{"cell_type":"markdown","metadata":{"id":"oCgLsK6z_wCo"},"source":["## API Keyを使えるようにする\n","\n","発行したKeyを次のコードにペーストして利用する\n","\n","次のセルにある、```%env OPENAI_API_KEY=``` に続けて、APIキーを記録して実行すること"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1768067847187,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"r0cuW-PeOD_s","outputId":"a26369eb-ab54-4c11-a632-a7918d9da67e"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: OPENAI_API_KEY=ここにキーを登録\n","env: OPENAI_API_KEY=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\n","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\n"]}],"source":["%env OPENAI_API_KEY=ここにキーを登録\n","%env OPENAI_API_KEY=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\n","print(os.environ['OPENAI_API_KEY'])"]},{"cell_type":"markdown","metadata":{"id":"pZ8jrQtABH08"},"source":["APIはOpenAI Chat APIに統一されている\n","\n","モデルは、コストの安いgpt-3.5-turboを利用する\n","- ここでは性能よりも使い方を学ぶためコスト重視とする\n","- model=\"gpt-4\"などとすることで、GPT-4を利用することができるが、利用料が高くなるので注意すること"]},{"cell_type":"markdown","metadata":{"id":"McSntSfqJWVP"},"source":["## APIを使う\n","\n","まず、ChatGPTに挨拶して、APIが使えているかどうかを確認する\n","\n","\"Hello. Am I using the API correctly?\"と聞いて回答を実際に得る\n","\n","必要なライブラリは、単純に次の2つ\n","- インターネットを利用して、シンプルにRESTでリクエストを投げてレスポンスを得るためのrequests\n","- JSONフォーマットを扱うためのjson"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Rr8d5MVyeTIO","executionInfo":{"status":"ok","timestamp":1768067847203,"user_tz":-540,"elapsed":16,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[],"source":["import requests\n","import os\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1333,"status":"ok","timestamp":1768055530624,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"bOzgI4KGOIcj","outputId":"c3641a94-af76-4d9e-c7be-7fd11ce5d8dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"id\": \"chatcmpl-CwUIbT7ZvHBYjYkV7pjJOSevForPD\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1768055529,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Hello! I'm an AI assistant and I'm here to help you with any questions you may have about using APIs. Can you provide more information about the specific API you are using and what you are trying to achieve? This will help me better assist you with your query.\",\n","        \"refusal\": null,\n","        \"annotations\": []\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 16,\n","    \"completion_tokens\": 55,\n","    \"total_tokens\": 71,\n","    \"prompt_tokens_details\": {\n","      \"cached_tokens\": 0,\n","      \"audio_tokens\": 0\n","    },\n","    \"completion_tokens_details\": {\n","      \"reasoning_tokens\": 0,\n","      \"audio_tokens\": 0,\n","      \"accepted_prediction_tokens\": 0,\n","      \"rejected_prediction_tokens\": 0\n","    }\n","  },\n","  \"service_tier\": \"default\",\n","  \"system_fingerprint\": null\n","}\n"]}],"source":["url = \"https://api.openai.com/v1/chat/completions\"\n","headers = {\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","}\n","data = {\n","    \"model\": \"gpt-3.5-turbo\",\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"Hello. Am I using the API correctly?\"}\n","    ],\n","    \"temperature\": 0,\n","}\n","\n","response = requests.post(url=url, headers=headers, json=data)\n","print(json.dumps(response.json(), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"hw4z_Ng7KUco"},"source":["返事は\"content\"にあるように、\n","\n","\"Hello! I'm an AI assistant and I'm here to help you with any questions you may have about using APIs. Can you provide more details about the API you are using and what specific issues you are facing?\"\n","\n","などとなるが、意味としては、\n","\n","「こんにちは！私はAIアシスタントです。APIの利用に関するご質問なら何でもお手伝いします。お使いのAPIについて、また直面している具体的な問題を詳しく教えていただけますか？」\n","\n","となる\n","\n","なお、パラメータについて、\n","- temperatureは0に近いほど、同じ回答を出力するようになる\n","- max_tokensは返答として最大何文字返すかを指定する\n","  - ChatGPTのAPIは、利用文字数(トークン数)により課金されるため、コスト低減を考えるのであれば重要である\n","- nは同一質問に対する返答数を指定する  \n","  この場合temparatureを大きめの値にしなければ、同じ回答が並ぶことになる\n","\n","これで、ひとまず使えるようになったであろう"]},{"cell_type":"markdown","source":["## Chat Completions APIのパラメータ\n","\n","OpenAI の `/v1/chat/completions` エンドポイントで指定できる主なフィールドと意味を整理する\n","\n","## 必須パラメータ\n","\n","| パラメータ | 型 | 説明 |\n","|-----------|----|------|\n","| **model** | string | 使用するモデル名 (`gpt-3.5-turbo` など)。必須。 |\n","| **messages** | array | 対話履歴。各要素は `{role, content}` を持つ。必須。 |\n","\n","---\n","\n","## 任意パラメータ一覧\n","\n","| パラメータ | 型 | 説明 | デフォルト |\n","|-----------|----|------|-------------|\n","| **temperature** | number (0〜2) | 出力のランダム性。0 に近いほど決定的。 | **1** |\n","| **top_p** | number (0〜1) | nucleus sampling。temperature との併用推奨なし。 | **1** |\n","| **n** | integer | 同じプロンプトに対して返す候補の数。 | **1** |\n","| **stream** | boolean | 応答をストリーミングで受け取る。 | **false** |\n","| **stop** | string / array | 応答を強制停止させる文字列。複数可。 | 省略 |\n","| **max_tokens** | integer | 生成する最大トークン数上限。 | **デフォルト値は非公開** |\n","| **presence_penalty** | number (-2〜2) | 未登場語を促進。テーマを広げる。 | **0** |\n","| **frequency_penalty** | number (-2〜2) | 同語句の繰り返しを抑える。 | **0** |\n","| **logit_bias** | dict | 特定トークンの確率を調整（高度用途）。 | 省略 |\n","| **user** | string | 利用者識別用。安全性/分析目的。 | 省略 |\n","\n","なお以下の点に注意すること\n","- max_tokensの明確なデフォルト固定値は公開されていない\n","- 未指定時はモデルごとのコンテキスト上限に基づき自動調整される\n","  - gpt-3.5-turbo のコンテキスト長が 4096 トークンの場合  \n","  プロンプトが 1000 トークン → 生成可能は約 3000 トークン前後\n","- 課金対象は入力トークン（プロンプト + 過去の messages）と出力トークン（生成部分）の合計\n","  - 過去の messages が増えるほど高くなる\n","  - max_tokens を低く設定すると節約になる\n","  - n を増やすと **n倍近くコスト増**\n","\n","全項目入りリクエストの例を挙げておく\n","\n","```\n","python\n","import os\n","import requests\n","import json\n","url = \"https://api.openai.com/v1/chat/completions\"\n","headers = {\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n","}\n","data = {\n","    \"model\": \"gpt-3.5-turbo\",\n","    \"messages\": [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": \"Explain gravity in 2 sentences.\"}\n","    ],\n","    \"temperature\": 0.2,\n","    \"top_p\": 1,\n","    \"n\": 1,\n","    \"max_tokens\": 100,\n","    \"stream\": False,\n","    \"stop\": None,\n","    \"presence_penalty\": 0,\n","    \"frequency_penalty\": 0,\n","    \"logit_bias\": {},\n","    \"user\": \"example-user-id\"\n","}\n","response = requests.post(url, headers=headers, data=json.dumps(data))\n","print(response.json())\n","```"],"metadata":{"id":"oWrBK3tBAWcm"}},{"cell_type":"markdown","metadata":{"id":"vI5oDCjZEcJb"},"source":["### エラートラブル(1)\n","\n","次のように表示される場合は、APIを利用するための無償枠や課金分を既に使い切ったか、時間が過ぎたためExpireしたことを意味する\n","- 課金するか、別アカウントを利用する\n","- 新しいアカウントに対して、5ドル分の無料お試し枠が設定される\n","\n","```\n","{\n","  \"error\": {\n","    \"message\": \"You exceeded your current quota, please check your plan and billing details.\",\n","    \"type\": \"insufficient_quota\",\n","    \"param\": null,\n","    \"code\": \"insufficient_quota\"\n","  }\n","}\n","```"]},{"cell_type":"markdown","metadata":{"id":"12TmNkeaPkMV"},"source":["### エラートラブル(2)\n","\n","頻繁にアクセスすると、エラーが発生する\n","\n","これは、単位時間あたりのアクセス数が制限されているためである\n","- しばらく待ってリトライすること"]},{"cell_type":"markdown","metadata":{"id":"o6grQLKrN9xo"},"source":["### roleについて\n","\n","\"role\"は、次の3つがある\n","- userはChatGPTのユーザで、皆さんのこと\n","- assistantがChatGPTによる回答を指し、ChatGPTのこと\n","- systemはassistantのふるまいを制御するために利用\n","\n","但し、gpt-3.5-turboでは、systemは利用しない\n"]},{"cell_type":"markdown","metadata":{"id":"1LggZY0MRJ7W"},"source":["## API利用において重要なこと"]},{"cell_type":"markdown","metadata":{"id":"hMArjv5yOllm"},"source":["### ブラウザ版との違い\n","\n","API利用と、ブラウザ利用における最も大きな違いは、API利用では過去の会話のやり取りを一切考慮しないという点である\n","\n","したがって、APIを利用する場合で過去の会話を参照したい場合は、過去の会話そのものを全てmessageに記載する必要がある\n"]},{"cell_type":"markdown","metadata":{"id":"xTfjkD7QOrIy"},"source":["### 料金の確認\n","\n","https://openai.com/pricing にアクセスすると、\n","\n","| Model\t| Input\t| Output |\n","|:---|:---|:---|\n","| GPT-3.5-Turbo 4K context | \\$0.0015 / 1K tokens | \\$0.002 / 1K tokens |\n","\n","と記載されている\n","\n","現在いくらつかったかは、\n","\n","https://platform.openai.com/account/usage\n","\n","にアクセスして確認するとよい\n"]},{"cell_type":"markdown","metadata":{"id":"YZ6vL8YXOvFN"},"source":["### トークン数\n","\n","課金対象にもなっているトークン数について、トークンは基本的に単語のことであり、トークン数は入力した単語の数を意味する\n","- ChatGPTを含む多くのLMにおいて、膨大な単語を効率よく学習するため、一つの単語を複数のトークンに分割して処理している\n","  - 例えば、\"humburger\"は、\"hum\", \"bur\", \"ger\"の3つに分解される\n","- 日本語と英語ではトークン数のカウント方法が異なる\n","  - 日本語は内部で英語に変換されて処理されているため日本語は似た内容の文章において課金上不利となる\n","  - 日本語は1トークンはおよそ4文字、英語ではおよそ0.75語に相当するなど不利な状況があり、一般に直接英語を利用した方がお得といわれている所以\n","  - 実際2倍程度の開きがあったが、近年緩和されつつある\n","\n","このトークン数は、各モデルの入力や出力サイズの制限にも利用される\n","\n","直接文字数を計数したい場合は、https://platform.openai.com/tokenizer にアクセスして、文章を入力するとよい\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yw6W-1k9kQ7T"},"source":["# GPT APIを用いたアプリケーション実装\n","\n","***ここでは、料理名を入力することで、材料と手順、調理時間、お勧めのサイドメニューなどを表示するというアプリを作成することを念頭に説明する***"]},{"cell_type":"markdown","metadata":{"id":"toaMHvttly4S"},"source":["## 全体の構成\n","\n","### APIまでの通信手順\n","\n","以下の手順を踏む\n","- スマートフォンやPC、Webのアプリを利用して、レシピ名をサーバプログラムに送信する\n","- サーバプログラムは、プロンプトエンジニアリングを行い、APIキーを付与してOpenAIのAPIを叩く\n","\n","レシピ生成アプリが直接OpenAI APIを叩くような構成は、APIキーが漏えいするため普通は行わない\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi1.jpg\" width=700>"]},{"cell_type":"markdown","metadata":{"id":"AFbLFE6gqLkD"},"source":["# LangChain"]},{"cell_type":"markdown","metadata":{"id":"XpZ3ySBfqQGu"},"source":["## LangChainとは？\n","\n","LLMを使ったアプリケーション開発フレームワーク\n","- PythonとJavaScript/TypeScriptの2つがある\n","- フリーで利用できる\n","\n","詳細は公式のドキュメントを参照すること\n","- 過去のバージョンのマニュアルを見る場合は、githubにあるlangchainに行き、docsのreleasesから目的のバージョンを選択するとよい"]},{"cell_type":"markdown","source":["## LangChainを使うメリット\n","\n","1. **抽象化**: HTTP通信やJSONの処理を意識せずに済む\n","1. **モデル切り替え**: OpenAI → Azure → Anthropic など容易に変更可能\n","1. **機能拡張**: プロンプトテンプレート、チェーン、メモリ、RAGなどを活用可能\n","1. **エコシステム**: 多数のツール・ローダー・ベクトルストアとの連携可能"],"metadata":{"id":"71XpvCYfGYJ1"}},{"cell_type":"markdown","metadata":{"id":"gUfbxuHqrE-W"},"source":["## Module\n","\n","LangChainにはmoduleと呼ばれる構成要素がある\n","\n","| モジュール | 役割 | 2025年の状況 |\n","|-----------|------|-------------|\n","| **Models** | LLMやチャットモデルへのアクセス | `langchain_openai` 等からインポート |\n","| **Prompts** | プロンプトテンプレートの管理 | `langchain_core.prompts` からインポート |\n","| **Chains** | 複数の処理を連結 | **LCEL記法を推奨**（LLMChain等は非推奨）|\n","| **Indexes** | ドキュメントの読み込みとベクトル化 | `langchain_community` 等からインポート |\n","| **Memory** | 会話履歴の管理 | メッセージリスト直接管理を推奨 |\n","| **Agents** | ツールを使った自律的な処理 | **LangGraph** を推奨 |"]},{"cell_type":"markdown","metadata":{"id":"Czl2ZbdordE3"},"source":["### Models module\n","\n","LongChainで利用する機械学習モデルである\n","\n","- Chat Models: Open AIのChatAPIのためのモジュール\n","- Text Embedding Models: テキストをベクトル化するモデル\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6sB2-PQgsrMO"},"source":["先に示した簡単な対話プログラムを再構成する\n","- 先ほどよりもシンプルに記述できることがわかるであろう\n","- 内部でjsonに変換され通信が行なわれている"]},{"cell_type":"markdown","metadata":{"id":"QiLIDe5732PJ"},"source":["openaiをインストールする\n","\n","ライブラリの開発速度が速く、バージョン競合が簡単に発生するため注意すること\n","- できれば、自分で解決する能力を身に着けるとよい\n","- (2023/12) openai、cohere、tiktokenは同時に導入しないと警告が表示される\n","- (2023/12) tensorflow-probabilityと一部ライブラリがコンフリクトするため、uninstallする\n","- (2025/01) Colabにプリインストールされているopentelemetryとchromadbの依存関係が競合するため、先にopentelemetry関連のバージョンを固定\n","\n","次のようにインストールすることで対処する"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"GrJA882U5Blz","executionInfo":{"status":"ok","timestamp":1768067883981,"user_tz":-540,"elapsed":10918,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b47cadb5-a768-439f-aaaa-4ce1da62ff1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip uninstall -y --quiet tensorflow-probability tensorflow-metadata tensorflow-datasets\n","!pip install --quiet \"opentelemetry-api==1.37.0\" \"opentelemetry-sdk==1.37.0\" \\\n","    \"opentelemetry-exporter-otlp-proto-common==1.37.0\" \"opentelemetry-proto==1.37.0\" \\\n","    \"opentelemetry-exporter-otlp-proto-http==1.37.0\" 2>/dev/null"]},{"cell_type":"markdown","source":["なお、LangChainは複数のパッケージに分割されている\n","\n","| パッケージ | 用途 | 主なクラス |\n","|-----------|------|----------|\n","| `langchain` | コア機能 | - |\n","| `langchain-core` | 基本クラス・インターフェース | PromptTemplate, StrOutputParser |\n","| `langchain-community` | コミュニティ貢献のローダー・ツール | DirectoryLoader, ChatMessageHistory |\n","| `langchain-openai` | OpenAI統合 | ChatOpenAI, OpenAIEmbeddings |\n","| `langchain-chroma` | Chromaベクトルストア統合 | Chroma |\n","| `langchain-text-splitters` | テキスト分割機能 | RecursiveCharacterTextSplitter |\n"],"metadata":{"id":"dw-dIr9CHQbJ"}},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":13428,"status":"ok","timestamp":1768067898434,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"lWZ181WMBpLM"},"outputs":[],"source":["# 基本パッケージのインストール\n","!pip install --quiet openai cohere tiktoken"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17622,"status":"ok","timestamp":1768067916081,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"LxnKpkivFMKx","outputId":"6b14e890-38ae-4bac-e496-13b6d669e6f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# chromadbと関連パッケージのインストール\n","!pip install --quiet chromadb kaleido python-multipart"]},{"cell_type":"markdown","source":["LangChain関連パッケージのインストール（新しいLCEL対応版）"],"metadata":{"id":"FM0iruOGD-I5"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"A8IbLgpEGqJB","executionInfo":{"status":"ok","timestamp":1768067936196,"user_tz":-540,"elapsed":20086,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"26b53cf1-c580-48b6-d7ff-a0f2c77b2405"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","embedchain 0.1.128 requires chromadb<0.6.0,>=0.5.10, but you have chromadb 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install --quiet langchain langchain-core langchain-community langchain-openai langchain-chroma langchain-text-splitters"]},{"cell_type":"markdown","metadata":{"id":"0HX01Spxvna6"},"source":["## LangChain の書き方\n","\n","LangChain 0.1.x 以降、APIの書き方が大きく変わった\n","\n","### `invoke()` メソッドを使う\n","\n","```python\n","# 旧: predict() や run() は非推奨（deprecatedの警告が出る）\n","result = llm.predict(\"質問\")  # ← 使わない\n","result = llm.run(\"質問\")      # ← 使わない\n","# 新: invoke() を使う\n","result = llm.invoke(\"質問\")   # ← これを使う\n","```\n","\n","### 戻り値は `AIMessage` オブジェクト\n","\n","```python\n","result = llm.invoke(\"質問\")\n","print(result)          # → AIMessage(content='回答テキスト', ...)\n","print(result.content)  # → '回答テキスト' （これで文字列を取得）\n","```\n","`result` をそのまま文字列として使うとエラーになる\n","\n","### インポート先が `langchain_openai` に変更\n","\n","```python\n","# 旧: langchain.chat_models からのインポート（非推奨）\n","from langchain.chat_models import ChatOpenAI\n","# 新: langchain_openai パッケージからインポート\n","from langchain_openai import ChatOpenAI\n","```\n","\n","次のように実行する\n","\n","```python\n","from langchain_openai import ChatOpenAI\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","result = llm.invoke(\"自己紹介してください。\")\n","print(result.content)  # ← .content で回答テキストを取得\n","```\n","\n","- `ChatOpenAI()`: OpenAIのチャットモデルを初期化\n","  - `model_name`: 使用するモデル（\"gpt-3.5-turbo\", \"gpt-4\" など）\n","  - `temperature`: 0に近いほど決定的な回答、1に近いほど多様な回答\n","- `invoke()`: モデルを呼び出して応答を得る\n","- `result.content`: AIMessageオブジェクトから回答テキストを取得"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"gJHDUhwwgKc6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768067937747,"user_tz":-540,"elapsed":1530,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e5535861-dc0e-4e4a-aa26-5e1a22af1d69"},"outputs":[{"output_type":"stream","name":"stdout","text":["はじめまして、私はAIアシスタントです。自然言語処理技術を用いて、様々な質問に回答したり、会話を行ったりすることができます。お手伝いが必要なことがあれば、遠慮なくお知らせください。どうぞよろしくお願いいたします。\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}],"source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","result = llm.invoke(\"自己紹介してください。\")\n","print(result.content)"]},{"cell_type":"markdown","metadata":{"id":"eVvTlTgDtQgX"},"source":["## Prompts module\n","\n","まず、Promptsも含めた、主なテンプレートの種類について概説する\n","\n","テンプレートは、モデルへの入力を組み立てるmoduleであり、次の要素がある\n","\n","| クラス | 用途 |\n","|--------|------|\n","| `PromptTemplate` | 単純な文字列テンプレート |\n","| `ChatPromptTemplate` | チャット形式（system/human/ai）|\n","| `MessagesPlaceholder` | 会話履歴を挿入する場所を指定 |\n","\n","ここでは、Prompt Templatesについて説明する\n","- ChatGPTへのプロンプトについてテンプレートつまり例文を作成することができる\n","\n","Prompts のインポート先は次のように刷新されている\n","\n","```python\n","# 基本的なプロンプト\n","from langchain_core.prompts import PromptTemplate\n","# チャット用プロンプト\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","# 出力パーサー\n","from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0giJqxftt69A"},"source":["次のコードでは、`{command}` が `ls` に置き換わる\n","- Promptの長さを考慮して埋め込む\n","- 出力する形式を指定して埋め込む\n","\n","などが可能であり、単純なPythonコードによる埋め込みよりも高度な処理が可能である\n","\n","1. **変数は `{変数名}` で指定**\n","   - template内の `{command}` が置換対象\n","   \n","2. **`input_variables` はリスト形式**\n","   - 単一変数でも `[\"command\"]` とリストで書く\n","   - 複数変数なら `[\"var1\", \"var2\"]`\n","\n","3. **`format()` で値を埋め込む**\n","   - キーワード引数で `変数名=値` を指定\n","   - 戻り値は埋め込み後の文字列"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sZIx1e8iwJ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055904903,"user_tz":-540,"elapsed":12,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"d18ebed7-293c-4ab6-aba8-b38575221ff0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","次のコマンドの概要を説明してください。\n","コマンド: ls\n","\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","コマンド: {command}\n","\"\"\"\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","result = prompt.format(command=\"ls\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"cI2F8zKJuV-M"},"source":["## Chains module\n","\n","Models, Templates, Chainsなどのmoduleを連結する\n","\n","LangChain 0.1.17以降、従来の `LLMChain` は**非推奨（deprecated）**となっており、**LCEL記法** を使用する。\n","\n","### LCEL記法\n","\n","**パイプ演算子 `|`** を使ってコンポーネントを連結する\n","- データが左から右へ流れるイメージ\n","\n","```python\n","chain = prompt | chat | StrOutputParser()\n","```\n","\n","### パイプラインの流れ\n","\n","```\n","{\"command\": \"ls\"}\n","      ↓\n","   prompt        → PromptTemplateが変数を埋め込んだ文字列を生成\n","      ↓\n","    chat         → ChatOpenAIがAPIを呼び出してAIMessageを返す\n","      ↓\n","StrOutputParser  → AIMessageから.contentを抽出して文字列を返す\n","      ↓\n","  \"結果の文字列\"\n","```\n","\n","### StrOutputParser の役割\n","\n","```python\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# StrOutputParser がない場合\n","chain = prompt | chat\n","result = chain.invoke({\"command\": \"ls\"})\n","print(type(result))  # → <class 'AIMessage'>\n","print(result.content)  # ← .content が必要\n","\n","# StrOutputParser がある場合\n","chain = prompt | chat | StrOutputParser()\n","result = chain.invoke({\"command\": \"ls\"})\n","print(type(result))  # → <class 'str'>  ← 直接文字列が得られる\n","```\n","\n","なお、LangChainの挙動の詳細を確認するため、\n","`langchain.verbose = True`\n","としている\n","- いろいろと意味のない文章や宣伝も表示されるが、不要な場合は、Falseとするとよい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QyYR_Iixyuy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055954085,"user_tz":-540,"elapsed":1725,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"5a966743-df7c-45e8-a082-6d4558988c8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== 詳細ログ ===\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n","\u001b[0m{\n","  \"command\": \"ls\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n","\u001b[0m{\n","  \"command\": \"ls\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n","\u001b[0m[outputs]\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: \\n次のコマンドの概要を説明してください。\\n\\nコマンド: ls\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.67s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"「ls」コマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。ファイルやディレクトリの名前や属性、更新日時などが表示され、現在のディレクトリ内で何が存在しているかを確認するのに便利です。\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"「ls」コマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。ファイルやディレクトリの名前や属性、更新日時などが表示され、現在のディレクトリ内で何が存在しているかを確認するのに便利です。\",\n","            \"additional_kwargs\": {\n","              \"refusal\": null\n","            },\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 158,\n","                \"prompt_tokens\": 32,\n","                \"total_tokens\": 190,\n","                \"completion_tokens_details\": {\n","                  \"accepted_prediction_tokens\": 0,\n","                  \"audio_tokens\": 0,\n","                  \"reasoning_tokens\": 0,\n","                  \"rejected_prediction_tokens\": 0\n","                },\n","                \"prompt_tokens_details\": {\n","                  \"audio_tokens\": 0,\n","                  \"cached_tokens\": 0\n","                }\n","              },\n","              \"model_provider\": \"openai\",\n","              \"model_name\": \"gpt-3.5-turbo-0125\",\n","              \"system_fingerprint\": null,\n","              \"id\": \"chatcmpl-CwUPQldkf48vCAyRUa6vyFgHAlyxV\",\n","              \"service_tier\": \"default\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"lc_run--019ba858-d1f3-7141-ae4e-8775a98cac3e-0\",\n","            \"usage_metadata\": {\n","              \"input_tokens\": 32,\n","              \"output_tokens\": 158,\n","              \"total_tokens\": 190,\n","              \"input_token_details\": {\n","                \"audio\": 0,\n","                \"cache_read\": 0\n","              },\n","              \"output_token_details\": {\n","                \"audio\": 0,\n","                \"reasoning\": 0\n","              }\n","            },\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 158,\n","      \"prompt_tokens\": 32,\n","      \"total_tokens\": 190,\n","      \"completion_tokens_details\": {\n","        \"accepted_prediction_tokens\": 0,\n","        \"audio_tokens\": 0,\n","        \"reasoning_tokens\": 0,\n","        \"rejected_prediction_tokens\": 0\n","      },\n","      \"prompt_tokens_details\": {\n","        \"audio_tokens\": 0,\n","        \"cached_tokens\": 0\n","      }\n","    },\n","    \"model_provider\": \"openai\",\n","    \"model_name\": \"gpt-3.5-turbo-0125\",\n","    \"system_fingerprint\": null,\n","    \"id\": \"chatcmpl-CwUPQldkf48vCAyRUa6vyFgHAlyxV\",\n","    \"service_tier\": \"default\"\n","  },\n","  \"run\": null,\n","  \"type\": \"LLMResult\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n","\u001b[0m{\n","  \"output\": \"「ls」コマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。ファイルやディレクトリの名前や属性、更新日時などが表示され、現在のディレクトリ内で何が存在しているかを確認するのに便利です。\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.68s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"「ls」コマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。ファイルやディレクトリの名前や属性、更新日時などが表示され、現在のディレクトリ内で何が存在しているかを確認するのに便利です。\"\n","}\n","\n","=== 結果 ===\n","「ls」コマンドは、リスト（List）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するためのコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。ファイルやディレクトリの名前や属性、更新日時などが表示され、現在のディレクトリ内で何が存在しているかを確認するのに便利です。\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","#  詳細な実行ログを出力させるために利用、普通はコメントアウトしておく\n","from langchain_core.tracers.stdout import ConsoleCallbackHandler\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Prompt を用意\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","\n","コマンド: {command}\n","\"\"\"\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","\n","# LCEL Chain を作成（パイプ演算子で接続）\n","chain = prompt | chat | StrOutputParser()\n","\n","# 実行\n","# config={'callbacks': [...]} を指定して、この実行のログをコンソールに出力させる\n","print(\"=== 詳細ログ ===\")\n","result = chain.invoke({\"command\": \"ls\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n","print(\"\\n=== 結果 ===\")\n","#result = chain.invoke({\"command\": \"ls\"})\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"B-A400ZJu2ws"},"source":["`chain.invoke()` を用いて、構築したchainが順に実行される\n","- 最初に文字の埋め込み(prompt)が行われる\n","- 次にchat modelにより実際に通信が行われる\n","\n","invoke() の使い方\n","- 引数は必ず辞書形式\n","\n","```python\n","# 間違い: 文字列をそのまま渡す\n","result = chain.invoke(\"ls\")\n","# 正しい: 辞書で渡す（キーはinput_variablesで指定した変数名）\n","result = chain.invoke({\"command\": \"ls\"})\n","```\n","**エラー例**: `KeyError: 'command'` → 辞書のキー名が間違っている\n","\n","- 複数の変数がある場合\n","\n","```python\n","prompt = PromptTemplate(\n","    input_variables=[\"name\", \"age\"],\n","    template=\"{name}さんは{age}歳です\"\n",")\n","chain = prompt | chat | StrOutputParser()\n","# 全ての変数を辞書で渡す\n","result = chain.invoke({\"name\": \"田中\", \"age\": \"30\"})\n","```"]},{"cell_type":"markdown","metadata":{"id":"nSMyGRodvnbL"},"source":["### LCEL による複数ステップの連結\n","\n","LCEL記法では、`RunnableLambda` を使って中間変換を行い、チェーンを連結する。\n","\n","```python\n","from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n","# 1つ目のチェーン\n","chain1 = prompt1 | chat | StrOutputParser()\n","# 2つ目のチェーン  \n","chain2 = prompt2 | chat | StrOutputParser()\n","# 連結（RunnableLambdaで出力を次の入力形式に変換）\n","full_chain = chain1 | RunnableLambda(lambda x: {\"input\": x}) | chain2\n","```\n","RunnablePassthrough と RunnableLambda\n","\n","| クラス | 役割 |\n","|--------|------|\n","| `RunnablePassthrough()` | 入力をそのまま次に渡す |\n","| `RunnableLambda(func)` | 関数を適用して変換する |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfIPskvyXo_3"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"DwtbSXduwJO-"},"source":["### Chainの例\n","\n","例えば\n","- 簡単な算数の問題を問い合わせ、それに対して回答を得る場合、詳細な手順を聞くようにすると解答の精度が向上する\n","- しかしながら、欲しいのは最終的な答えであって、途中経過は不要であるとする\n","- すると、まず、詳細な手順を含む答えを得てから、その答えを要約して最後の答えだけ得るようにするとよい\n","\n","つまり、ChatGPTを2回利用して最終的に欲しい回答を獲得することを考える\n","- なお、この例では1回でも十分である\n","\n","以下のコードのデータの流れは次の通り\n","\n","```\n","\"リンゴの問題文\"\n","      ↓ RunnablePassthrough\n","{\"question\": \"リンゴの問題文\"}\n","      ↓ cot_chain\n","\"ステップ1...ステップ2...答えは10個\"\n","      ↓ RunnableLambda\n","{\"input\": \"ステップ1...ステップ2...答えは10個\"}\n","      ↓ summarize_chain\n","\"10個\"\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BETIBsg104QZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055595796,"user_tz":-540,"elapsed":2151,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"48c40a06-35b5-4247-fe44-b8892b9ca6a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["最終的には10個のリンゴが残ります。\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# 1 つ目の Prompt を用意\n","cot_template = \"\"\"\n","以下の質問に回答してください。\n","\n","### 質問 ###\n","{question}\n","### 質問終了 ###\n","\n","ステップバイステップで考えましょう。\n","\"\"\"\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=cot_template,\n",")\n","\n","# 2 つ目の Prompt を用意\n","summarize_template = \"\"\"\n","入力を結論だけ抜き出して記述してください。\n","\n","### 入力 ###\n","{input}\n","### 入力終了 ###\n","\"\"\"\n","summarize_prompt = PromptTemplate(\n","    input_variables=[\"input\"],\n","    template=summarize_template,\n",")\n","\n","# 1つ目のチェーン: ステップバイステップで考える\n","cot_chain = cot_prompt | chat | StrOutputParser()\n","# 2つ目のチェーン: 結論だけ抜き出す\n","summarize_chain = summarize_prompt | chat | StrOutputParser()\n","\n","# 全体のチェーン: cot_chain の出力を summarize_chain の input に渡す\n","full_chain = (\n","    {\"question\": RunnablePassthrough()} # 入力をquestionキーに\n","    | cot_chain                         # → 詳細な回答\n","    | RunnableLambda(lambda x: {\"input\": x}) # 出力をinputキーに変換\n","    | summarize_chain                   # → 結論のみ\n",")\n","\n","# 実行\n","result = full_chain.invoke(\"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\")\n","print(result)"]},{"cell_type":"markdown","source":["なお、独自のpromptsやchains moduleを作成することも可能である\n","\n","詳細は、LangChainのマニュアル https://python.langchain.com/docs/get_started/introduction を参照されたい"],"metadata":{"id":"zTdhV8cnKKH1"}},{"cell_type":"markdown","metadata":{"id":"V-oQdKENxWo_"},"source":["LCEL でカスタム処理を組み込むには、RunnableLambda を使う\n","\n","```python\n","from langchain_core.runnables import RunnableLambda\n","\n","# カスタム処理を関数として定義\n","def my_processor(input_data):\n","    # 何らかの処理\n","    return {\"processed\": input_data.upper()}\n","\n","# RunnableLambda でラップしてチェーンに組み込み\n","chain = (\n","    RunnableLambda(my_processor)\n","    | prompt\n","    | chat\n","    | StrOutputParser()\n",")\n","```\n","よく使う Runnable クラスは次の通り\n","\n","| クラス | 役割 |\n","|--------|------|\n","| `RunnableLambda(func)` | 任意の関数をラップ |\n","| `RunnablePassthrough()` | 入力をそのまま通過 |\n","| `RunnableParallel({...})` | 複数の処理を並列実行 |"]},{"cell_type":"markdown","metadata":{"id":"CXZ034oXyCvv"},"source":["## Output Parsers\n","\n","出力形式を指定するプロンプトの作成とPythonオブジェクトとのマッピングを提供する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi2.jpg\" width=700>\n","\n","#### PydanticOutputParser の使い方\n","\n","- 手順\n","\n","1. **Pydanticモデルを定義**: 期待する出力構造を定義\n","2. **Parserを作成**: `PydanticOutputParser(pydantic_object=モデル)`\n","3. **フォーマット指示を取得**: `parser.get_format_instructions()`\n","4. **プロンプトに埋め込み**: `partial_variables` で指示を埋め込む\n","5. **チェーンを実行**: `chain.invoke()`\n","6. **出力をパース**: `parser.parse(output.content)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkm0Ys9k4t3h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055598215,"user_tz":-540,"elapsed":2417,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"b3b86db1-2447-4a62-b321-d761c65125c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== output ===\n","```json\n","{\n","    \"ingredients\": [\n","        \"カレールー\",\n","        \"肉（牛肉、豚肉、鶏肉など）\",\n","        \"じゃがいも\",\n","        \"にんじん\",\n","        \"玉ねぎ\",\n","        \"油\",\n","        \"水\"\n","    ],\n","    \"steps\": [\n","        \"1. じゃがいも、にんじん、玉ねぎを適当な大きさに切る。\",\n","        \"2. 鍋に油を熱し、肉を炒める。\",\n","        \"3. 野菜を加えて炒める。\",\n","        \"4. 水を加えて煮込む。\",\n","        \"5. カレールーを加えて溶かし、とろみがつくまで煮込む。\",\n","        \"6. 器に盛り付けて完成。\"\n","    ],\n","    \"time\": [\n","        \"準備時間: 15分\",\n","        \"調理時間: 45分\"\n","    ],\n","    \"sides\": [\n","        \"ご飯\",\n","        \"フライドポテト\",\n","        \"サラダ\"\n","    ]\n","}\n","```\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import PydanticOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from pydantic import BaseModel, Field\n","from typing import List\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","class Recipe(BaseModel):\n","    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n","    steps: List[str] = Field(description=\"steps to make the dish\")\n","    time: List[str] = Field(description=\"time to make the dish\")\n","    sides: List[str] = Field(description=\"side menu of the dish\")\n","\n","template = \"\"\"料理のレシピを教えてください。\n","\n","{format_instructions}\n","\n","料理名: {dish}\n","\"\"\"\n","# Parserの生成\n","parser = PydanticOutputParser(pydantic_object=Recipe)\n","\n","# フォーマット指示をプロンプトに埋め込む\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"dish\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","# LCEL Chain を作成\n","chain = prompt | chat\n","\n","# 実行\n","output = chain.invoke({\"dish\": \"カレー\"})\n","print(\"=== output ===\")\n","print(output.content)"]},{"cell_type":"markdown","metadata":{"id":"YEJA39jxzwdW"},"source":["最後に、recipe型にマッピングするために、parser.parse()を利用する\n","\n","```python\n","recipe = parser.parse(output.content)\n","```\n","\n","- 処理の流れ\n","\n","1. `output` は `AIMessage` オブジェクト\n","2. `output.content` でJSON形式の文字列を取得\n","3. `parser.parse()` で文字列を `Recipe` オブジェクトに変換\n","\n","- 変換後のアクセス方法\n","\n","```python\n","print(recipe.ingredients)  # ['玉ねぎ', '人参', ...]\n","print(recipe.steps)        # ['野菜を切る', '炒める', ...]\n","print(recipe.time)         # ['30分']\n","print(recipe.sides)        # ['サラダ', 'らっきょう']\n","```\n","\n","- **`OutputParserException`**: LLMの出力がJSON形式でない場合に発生\n","  - 対策: プロンプトを改善するか、`temperature=0` で安定させる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW2oh6_QzrGk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055598239,"user_tz":-540,"elapsed":23,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"caae79d6-2db1-4ea6-ef33-ae8b23257a80"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== recipe object ===\n","ingredients=['カレールー', '肉（牛肉、豚肉、鶏肉など）', 'じゃがいも', 'にんじん', '玉ねぎ', '油', '水'] steps=['1. じゃがいも、にんじん、玉ねぎを適当な大きさに切る。', '2. 鍋に油を熱し、肉を炒める。', '3. 野菜を加えて炒める。', '4. 水を加えて煮込む。', '5. カレールーを加えて溶かし、とろみがつくまで煮込む。', '6. 器に盛り付けて完成。'] time=['準備時間: 15分', '調理時間: 45分'] sides=['ご飯', 'フライドポテト', 'サラダ']\n"]}],"source":["recipe = parser.parse(output.content)\n","print(\"=== recipe object ===\")\n","print(recipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkf3FX9080tZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055598292,"user_tz":-540,"elapsed":52,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"ae4b3d95-c45f-49ca-da4a-cfc9764d61a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['カレールー', '肉（牛肉、豚肉、鶏肉など）', 'じゃがいも', 'にんじん', '玉ねぎ', '油', '水']"]},"metadata":{},"execution_count":17}],"source":["recipe.ingredients"]},{"cell_type":"code","source":["print(recipe.ingredients)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhHpGqJCLkXp","executionInfo":{"status":"ok","timestamp":1768055598326,"user_tz":-540,"elapsed":41,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c9787644-c9af-410b-c950-efaa4cc885d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['カレールー', '肉（牛肉、豚肉、鶏肉など）', 'じゃがいも', 'にんじん', '玉ねぎ', '油', '水']\n"]}]},{"cell_type":"code","source":["print(recipe.steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWyhHv9OLslN","executionInfo":{"status":"ok","timestamp":1768055598386,"user_tz":-540,"elapsed":59,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"08b4c175-5898-445d-ceb0-9170f31d2aa9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['1. じゃがいも、にんじん、玉ねぎを適当な大きさに切る。', '2. 鍋に油を熱し、肉を炒める。', '3. 野菜を加えて炒める。', '4. 水を加えて煮込む。', '5. カレールーを加えて溶かし、とろみがつくまで煮込む。', '6. 器に盛り付けて完成。']\n"]}]},{"cell_type":"code","source":["print(recipe.time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7546gJBLt0Y","executionInfo":{"status":"ok","timestamp":1768055598410,"user_tz":-540,"elapsed":23,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"ebad7138-678b-40f3-8b18-055934c0043b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['準備時間: 15分', '調理時間: 45分']\n"]}]},{"cell_type":"code","source":["print(recipe.sides)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CFQcWXsLu4v","executionInfo":{"status":"ok","timestamp":1768055598430,"user_tz":-540,"elapsed":25,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"acbcc037-db61-4e64-a11c-f09ba40f35ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['ご飯', 'フライドポテト', 'サラダ']\n"]}]},{"cell_type":"markdown","metadata":{"id":"prFUg4G3QfbA"},"source":["## Indexes"]},{"cell_type":"markdown","metadata":{"id":"p3Gp3u-hJHR_"},"source":["ChatGPTは学習に用いたデータセットの範疇でのみ答えを出すため、新しい知識や概念については、正しく解答することが難しい\n","\n","例えば、次のような質問に対しては、お手上げになっている\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain1.jpg\" width=500>\n","\n","ここで、質問に対する回答を得るうえで必要となる情報を渡してから処理させると、おおよそ正しい回答を得ることができるようになる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain2.jpg\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"R59skBRVYMJp"},"source":["コンテキストとして、情報を加えることで正しい回答を与えることができており、これは強力な方法であるが、実際に用いる場合は注意が必要である\n","\n","- ChatGPTには、入力文字数に制限があるため、長い文章を入力する必要がある場合はともかく、様々な情報を大量に与えておいて、そこから適切な回答を得るという利用は困難である\n","- 文字数、つまり入力トークン数も課金に関係するため、高コストとなる\n","\n","まず、全ての情報を与えておいて、何かしら回答を得るということは非現実的であるが、自動化という点では有効な手段である\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ob50Tp4IZaSW"},"source":["### Vector Store\n","\n","そこで、Vector Storeを活用する\n","- 文章をベクトル化してVector Storeに保存、入力と近しいベクトルの文章をVector Storeから検索してcontextに含める手法\n","- 全文章ではなく、「大事と思われる部分文章群についてのみ」情報を与える\n","  - これには、embeddingと呼ばれる内部ベクトル表現への変換を行い、そのベクトルの近接性を用いて、どの文章が重要かを判断している\n","  - 単語をベクトル化するトークンではない点に注意すること\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain3.jpg\" width=700>\n","\n","Vector Store のパッケージは次の通り\n","\n","| Vector Store | パッケージ | 特徴 |\n","|-------------|-----------|------|\n","| Chroma | `langchain-chroma` | ローカル実行、開発向け |\n","| FAISS | `langchain-community` | 高速、Meta製 |\n","| Pinecone | `langchain-pinecone` | クラウド、本番向け |\n","\n","- 基本的な使い方\n","\n","```python\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_chroma import Chroma\n","# 1. Embeddingsモデルを作成\n","embeddings = OpenAIEmbeddings()\n","# 2. Vector Storeを作成\n","vectorstore = Chroma.from_documents(documents, embeddings)\n","# 3. Retrieverとして使用\n","retriever = vectorstore.as_retriever()\n","docs = retriever.invoke(\"検索クエリ\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"PVg3gQRLcb2X"},"source":["実際にindexを扱う\n","\n","ここでは、LangChainのドキュメントを参考にして、質問できるようにする\n","- 最新のドキュメントはgitで公開されているため、LangChainのgitをcloneする"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VY_0R_VGcHv","outputId":"d95e47c9-6c8a-4942-822b-5c06d75ed593","executionInfo":{"status":"ok","timestamp":1768055598654,"user_tz":-540,"elapsed":225,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'langchain' already exists and is not an empty directory.\n","/content/langchain\n","/content\n","LangChainドキュメントのダウンロード完了\n"]}],"source":["# LangChainのドキュメントをダウンロード\n","# （リポジトリが大きいため、shallow cloneを使用）\n","!git clone --depth 1 --filter=blob:none --sparse https://github.com/langchain-ai/langchain.git\n","%cd langchain\n","!git sparse-checkout set docs\n","%cd ..\n","print(\"LangChainドキュメントのダウンロード完了\")"]},{"cell_type":"markdown","metadata":{"id":"UdmH-q4VcpFn"},"source":["langchainのディレクトリに入る"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhcQzqUqR1t3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768055598795,"user_tz":-540,"elapsed":119,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"99f8b505-a19c-4821-8847-04a66f18502b"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 96\n","drwxr-xr-x 2 root root 4096 Jan 10 13:55 .\n","drwxr-xr-x 3 root root 4096 Jan 10 13:55 ..\n","-rw-r--r-- 1 root root 9164 Jan 10 13:55 chat.ipynb\n","-rw-r--r-- 1 root root 6223 Jan 10 13:55 document_loaders.ipynb\n","-rw-r--r-- 1 root root 7342 Jan 10 13:55 llms.ipynb\n","-rw-r--r-- 1 root root  970 Jan 10 13:55 provider.ipynb\n","-rw-r--r-- 1 root root 6778 Jan 10 13:55 retrievers.ipynb\n","-rw-r--r-- 1 root root 6343 Jan 10 13:55 stores.ipynb\n","-rw-r--r-- 1 root root 7280 Jan 10 13:55 text_embedding.ipynb\n","-rw-r--r-- 1 root root 5529 Jan 10 13:55 toolkits.ipynb\n","-rw-r--r-- 1 root root 8355 Jan 10 13:55 tools.ipynb\n","-rw-r--r-- 1 root root 8532 Jan 10 13:55 vectorstores.ipynb\n"]}],"source":["# カレントディレクトリの確認\n","!ls -la langchain/libs/cli/langchain_cli/integration_template/docs/ | head -20"]},{"cell_type":"markdown","metadata":{"id":"gK1vJgKmgpG1"},"source":["以降の動作確認で必要となるライブラリを導入する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znXiN1W1Rd8Q"},"outputs":[],"source":["# 追加で必要なパッケージ（chromadb, langchainは既にインストール済み）\n","!pip install --quiet unstructured tabulate pdf2image pytesseract"]},{"cell_type":"markdown","metadata":{"id":"waRfWzSLhL_p"},"source":["まず、ディレクトリの中の全体を読むための便利な`DirectoryLoader`を用いて、ある場所にあるファイルをサブディレクトリもまとめて取得する\n","- ここでは拡張子がmdであるファイルに限定している\n","\n","- そこから次々に文章を取得して、ベクトル化してChromaに格納していく\n","  - この時、embeddingと呼ばれる内部ベクトル表現への変換も同時に行っている\n","\n","パッケージのインポート先は次の通り\n","\n","```python\n","# Document Loaders\n","from langchain_community.document_loaders import DirectoryLoader, TextLoader\n","# Embeddings\n","from langchain_openai import OpenAIEmbeddings\n","# Vector Store\n","from langchain_chroma import Chroma\n","# Text Splitter\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","```\n","コードの流れは次の通り\n","\n","1. **ドキュメント読み込み**: `DirectoryLoader` でファイルを読み込む\n","2. **テキスト分割**: `RecursiveCharacterTextSplitter` でチャンクに分割\n","3. **ベクトル化**: `OpenAIEmbeddings` でテキストをベクトルに変換\n","4. **格納**: `Chroma.from_documents()` でベクトルストアに保存"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INKjEoCQXbft"},"outputs":[],"source":["!pip install --quiet nltk click joblib regex tqdm"]},{"cell_type":"code","source":["from langchain_community.document_loaders import DirectoryLoader, TextLoader\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_chroma import Chroma\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","import os\n","import shutil\n","\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","# 正しいドキュメントパスを設定（git sparse-checkoutで取得したのはルートのdocs）\n","docs_path = \"/content/langchain/libs/cli/langchain_cli/integration_template/docs/\"\n","\n","# ドキュメントがロードできたかどうかのフラグ\n","loaded = False\n","documents = []\n","\n","print(f\"Checking path: {docs_path}\")\n","if os.path.exists(docs_path):\n","    # .ipynb を試す\n","    print(\"Trying to load .ipynb files...\")\n","    try:\n","        loader = DirectoryLoader(docs_path, glob=\"*.ipynb\", loader_cls=TextLoader, show_progress=True)\n","        documents = loader.load()\n","        if len(documents) > 0:\n","            loaded = True\n","    except Exception as e:\n","        print(f\"Warning: Failed to load .ipynb files: {e}\")\n","\n","# ロードできなかった場合はサンプルを作成して使用\n","if not loaded:\n","    print(\"\\nドキュメントが見つからないため、サンプルデータを作成して使用します。\")\n","    sample_dir = \"./sample_docs\"\n","    if os.path.exists(sample_dir):\n","        shutil.rmtree(sample_dir)\n","    os.makedirs(sample_dir, exist_ok=True)\n","\n","    sample_texts = [\n","        (\"langchain_overview.md\", '''# LangChain Overview\n","LangChain is a framework for developing applications powered by language models.\n","It provides tools for prompt management, chains, and agents.\n","Key features include: memory management, document loaders, and vector stores.\n","'''),\n","        (\"index_module.md\", '''# Index Module\n","The Index module in LangChain helps manage and query document collections.\n","It supports various vector stores like Chroma, FAISS, and Pinecone.\n","Documents are split into chunks and embedded for efficient retrieval.\n","'''),\n","        (\"chains.md\", '''# Chains in LangChain\n","Chains allow you to combine multiple components into a single pipeline.\n","LCEL (LangChain Expression Language) uses the pipe operator for chaining.\n","Example: prompt | llm | output_parser\n","'''),\n","    ]\n","\n","    for filename, content in sample_texts:\n","        with open(f\"{sample_dir}/{filename}\", \"w\") as f:\n","            f.write(content)\n","\n","    docs_path = sample_dir\n","    loader = DirectoryLoader(docs_path, glob=\"**/*.md\", loader_cls=TextLoader, show_progress=True)\n","    documents = loader.load()\n","\n","print(f\"\\n最終的なドキュメントパス: {docs_path}\")\n","print(f\"Loaded {len(documents)} documents\")\n","\n","if len(documents) > 0:\n","    # テキストを分割\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","    texts = text_splitter.split_documents(documents)\n","\n","    # Embeddingを作成してChromaに保存\n","    embeddings = OpenAIEmbeddings()\n","    vectorstore = Chroma.from_documents(texts, embeddings)\n","\n","    print(f\"Split into {len(texts)} chunks and created vectorstore\")\n","else:\n","    print(\"エラー: 有効なドキュメントがありませんでした。\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhzDDsUBO8hk","executionInfo":{"status":"ok","timestamp":1768055651494,"user_tz":-540,"elapsed":26563,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"b4160051-454d-4a98-fc33-1cd54dcacfe1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"stream","name":"stdout","text":["Checking path: /content/langchain/libs/cli/langchain_cli/integration_template/docs/\n","Trying to load .ipynb files...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 3596.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","最終的なドキュメントパス: /content/langchain/libs/cli/langchain_cli/integration_template/docs/\n","Loaded 10 documents\n","Split into 85 chunks and created vectorstore\n"]}]},{"cell_type":"markdown","metadata":{"id":"2-k97WTohvnK"},"source":["では、そのvector storeを利用して、質問する\n","\n","なお、現時点でChatGPTに\"LangChain\"について問い合わせると、データセットに含まれていないという回答になる"]},{"cell_type":"markdown","source":["### LCEL記法によるRAG（検索拡張生成）チェーンの構築\n","\n","- RAGとは？\n","\n","**R**etrieval **A**ugmented **G**eneration（検索拡張生成）のことで、ユーザーの質問に関連するドキュメントを検索し、それをコンテキストとしてLLMに渡す手法\n","\n","- データの流れ\n","\n","```\n","\"LangChainのIndex moduleについて教えて\"\n","      ↓\n","┌──────────────────────────────────────┐\n","│  context: retriever | format_docs  │  ← 類似ドキュメントを検索→文字列化  |\n","│  question: RunnablePassthrough()   │  ← 質問をそのまま渡す                |\n","└──────────────────────────────────────┘\n","      ↓ 両方が辞書として prompt に渡される\n","{\"context\": \"検索されたドキュメント...\", \"question\": \"質問文\"}\n","      ↓ prompt\n","\"コンテキスト: ...\\n質問: ...\\n回答:\"\n","      ↓ chat → StrOutputParser\n","\"回答テキスト\"\n","```\n","\n","辞書 `{}` の中で複数の処理を並列に実行でき、`retriever | format_docs`として検索結果（Documentのリスト）を文字列に変換する"],"metadata":{"id":"nhkzuzm9PZPk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJbppn-lRblc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056005549,"user_tz":-540,"elapsed":2091,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e0403f86-98ab-4826-949a-ab146b4ec8af"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] Entering Chain run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:format_docs] [1ms] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"\\\"\\\\n\\\",\\n        \\\"- TODO: Add any other relevant links, like information about underlying API, etc.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Overview\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Make sure links and features are correct\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/__module_name__) |  Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__](https://python.langchain.com/v0.2/api_reference/community/tools/langchain_community.tools.__module_name__.tool.__ModuleName__.html) | [langchain-community](https://api.python.langchain.com/en/latest/community_api_reference.html) | beta/❌ | ✅/❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Tool features\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Add feature table if it makes sense\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"809c6577\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"59c710c4\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"0a760037\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our model object and generate chat completions:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"a0562a13\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n\\n\\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our byte store:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"from __module_name__ import __ModuleName__ByteStore\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"kv_store = __ModuleName__ByteStore(\\\\n\\\",\\n        \\\"    # params...\\\\n\\\",\\n        \\\")\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n\\n\\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Fill in table features.\\\\n\\\",\\n        \\\"- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\\\\n\\\",\\n        \\\"- TODO: Make sure API reference links are correct.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/llms/__package_name_short_snake__) | Package downloads | Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__LLM](https://api.python.langchain.com/en/latest/llms/__module_name__.llms.__ModuleName__LLM.html) | [__package_name__](https://api.python.langchain.com/en/latest/__package_name_short_snake___api_reference.html) | ✅/❌ | beta/❌ | ✅/❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/__package_name__&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/__package_name__&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Setup\\\\n\\\",\\n        \\\"\\\\n\\\",\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [605ms] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"\\\"\\\\n\\\",\\n        \\\"- TODO: Add any other relevant links, like information about underlying API, etc.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Overview\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Make sure links and features are correct\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/__module_name__) |  Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__](https://python.langchain.com/v0.2/api_reference/community/tools/langchain_community.tools.__module_name__.tool.__ModuleName__.html) | [langchain-community](https://api.python.langchain.com/en/latest/community_api_reference.html) | beta/❌ | ✅/❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Tool features\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Add feature table if it makes sense\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"809c6577\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"59c710c4\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"0a760037\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our model object and generate chat completions:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"a0562a13\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n\\n\\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our byte store:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"from __module_name__ import __ModuleName__ByteStore\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"kv_store = __ModuleName__ByteStore(\\\\n\\\",\\n        \\\"    # params...\\\\n\\\",\\n        \\\")\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n\\n\\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Fill in table features.\\\\n\\\",\\n        \\\"- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\\\\n\\\",\\n        \\\"- TODO: Make sure API reference links are correct.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/llms/__package_name_short_snake__) | Package downloads | Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__LLM](https://api.python.langchain.com/en/latest/llms/__module_name__.llms.__ModuleName__LLM.html) | [__package_name__](https://api.python.langchain.com/en/latest/__package_name_short_snake___api_reference.html) | ✅/❌ | beta/❌ | ✅/❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/__package_name__&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/__package_name__&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Setup\\\\n\\\",\\n        \\\"\\\\n\\\",\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [607ms] Exiting Chain run with output:\n","\u001b[0m{\n","  \"context\": \"\\\"\\\\n\\\",\\n        \\\"- TODO: Add any other relevant links, like information about underlying API, etc.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Overview\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Make sure links and features are correct\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/__module_name__) |  Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__](https://python.langchain.com/v0.2/api_reference/community/tools/langchain_community.tools.__module_name__.tool.__ModuleName__.html) | [langchain-community](https://api.python.langchain.com/en/latest/community_api_reference.html) | beta/❌ | ✅/❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Tool features\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Add feature table if it makes sense\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"809c6577\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"59c710c4\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"0a760037\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our model object and generate chat completions:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"a0562a13\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n\\n\\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our byte store:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"from __module_name__ import __ModuleName__ByteStore\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"kv_store = __ModuleName__ByteStore(\\\\n\\\",\\n        \\\"    # params...\\\\n\\\",\\n        \\\")\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n\\n\\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Fill in table features.\\\\n\\\",\\n        \\\"- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\\\\n\\\",\\n        \\\"- TODO: Make sure API reference links are correct.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/llms/__package_name_short_snake__) | Package downloads | Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__LLM](https://api.python.langchain.com/en/latest/llms/__module_name__.llms.__ModuleName__LLM.html) | [__package_name__](https://api.python.langchain.com/en/latest/__package_name_short_snake___api_reference.html) | ✅/❌ | beta/❌ | ✅/❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/__package_name__&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/__package_name__&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Setup\\\\n\\\",\\n        \\\"\\\\n\\\",\",\n","  \"question\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n","\u001b[0m{\n","  \"context\": \"\\\"\\\\n\\\",\\n        \\\"- TODO: Add any other relevant links, like information about underlying API, etc.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Overview\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Make sure links and features are correct\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/__module_name__) |  Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__](https://python.langchain.com/v0.2/api_reference/community/tools/langchain_community.tools.__module_name__.tool.__ModuleName__.html) | [langchain-community](https://api.python.langchain.com/en/latest/community_api_reference.html) | beta/❌ | ✅/❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Tool features\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Add feature table if it makes sense\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"809c6577\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"59c710c4\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"0a760037\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our model object and generate chat completions:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"a0562a13\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n\\n\\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our byte store:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"from __module_name__ import __ModuleName__ByteStore\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"kv_store = __ModuleName__ByteStore(\\\\n\\\",\\n        \\\"    # params...\\\\n\\\",\\n        \\\")\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n\\n\\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Fill in table features.\\\\n\\\",\\n        \\\"- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\\\\n\\\",\\n        \\\"- TODO: Make sure API reference links are correct.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/llms/__package_name_short_snake__) | Package downloads | Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__LLM](https://api.python.langchain.com/en/latest/llms/__module_name__.llms.__ModuleName__LLM.html) | [__package_name__](https://api.python.langchain.com/en/latest/__package_name_short_snake___api_reference.html) | ✅/❌ | beta/❌ | ✅/❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/__package_name__&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/__package_name__&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Setup\\\\n\\\",\\n        \\\"\\\\n\\\",\",\n","  \"question\": \"LangChainにおけるIndex moduleについて概要を1文で説明してください。\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n","\u001b[0m[outputs]\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: 以下のコンテキストに基づいて質問に答えてください。\\n\\nコンテキスト:\\n\\\"\\\\n\\\",\\n        \\\"- TODO: Add any other relevant links, like information about underlying API, etc.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Overview\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Make sure links and features are correct\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Serializable | [JS support](https://js.langchain.com/docs/integrations/tools/__module_name__) |  Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__](https://python.langchain.com/v0.2/api_reference/community/tools/langchain_community.tools.__module_name__.tool.__ModuleName__.html) | [langchain-community](https://api.python.langchain.com/en/latest/community_api_reference.html) | beta/❌ | ✅/❌ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-community&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"### Tool features\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Add feature table if it makes sense\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"809c6577\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"59c710c4\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"id\\\": \\\"0a760037\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our model object and generate chat completions:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"id\\\": \\\"a0562a13\\\",\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n\\n\\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"### Installation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"The LangChain __ModuleName__ integration lives in the `__package_name__` package:\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"%pip install -qU __package_name__\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n      \\\"metadata\\\": {},\\n      \\\"source\\\": [\\n        \\\"## Instantiation\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"Now we can instantiate our byte store:\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Update model instantiation with relevant params.\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"code\\\",\\n      \\\"execution_count\\\": null,\\n      \\\"metadata\\\": {},\\n      \\\"outputs\\\": [],\\n      \\\"source\\\": [\\n        \\\"from __module_name__ import __ModuleName__ByteStore\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"kv_store = __ModuleName__ByteStore(\\\\n\\\",\\n        \\\"    # params...\\\\n\\\",\\n        \\\")\\\"\\n      ]\\n    },\\n    {\\n      \\\"cell_type\\\": \\\"markdown\\\",\\n\\n\\\"### Integration details\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"- TODO: Fill in table features.\\\\n\\\",\\n        \\\"- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\\\\n\\\",\\n        \\\"- TODO: Make sure API reference links are correct.\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/llms/__package_name_short_snake__) | Package downloads | Package latest |\\\\n\\\",\\n        \\\"| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |\\\\n\\\",\\n        \\\"| [__ModuleName__LLM](https://api.python.langchain.com/en/latest/llms/__module_name__.llms.__ModuleName__LLM.html) | [__package_name__](https://api.python.langchain.com/en/latest/__package_name_short_snake___api_reference.html) | ✅/❌ | beta/❌ | ✅/❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/__package_name__&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/__package_name__&label=%20) |\\\\n\\\",\\n        \\\"\\\\n\\\",\\n        \\\"## Setup\\\\n\\\",\\n        \\\"\\\\n\\\",\\n\\n質問: LangChainにおけるIndex moduleについて概要を1文で説明してください。\\n\\n回答:\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.47s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"LangChainのIndexモジュールは、APIリファレンスやパッケージの最新情報などのリンクを提供します。\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"LangChainのIndexモジュールは、APIリファレンスやパッケージの最新情報などのリンクを提供します。\",\n","            \"additional_kwargs\": {\n","              \"refusal\": null\n","            },\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 40,\n","                \"prompt_tokens\": 1086,\n","                \"total_tokens\": 1126,\n","                \"completion_tokens_details\": {\n","                  \"accepted_prediction_tokens\": 0,\n","                  \"audio_tokens\": 0,\n","                  \"reasoning_tokens\": 0,\n","                  \"rejected_prediction_tokens\": 0\n","                },\n","                \"prompt_tokens_details\": {\n","                  \"audio_tokens\": 0,\n","                  \"cached_tokens\": 0\n","                }\n","              },\n","              \"model_provider\": \"openai\",\n","              \"model_name\": \"gpt-3.5-turbo-0125\",\n","              \"system_fingerprint\": null,\n","              \"id\": \"chatcmpl-CwUQF1O0YLSc2KPZ42sqgBtCC1J3t\",\n","              \"service_tier\": \"default\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"lc_run--019ba859-9be6-7b03-b8fe-899b456456e4-0\",\n","            \"usage_metadata\": {\n","              \"input_tokens\": 1086,\n","              \"output_tokens\": 40,\n","              \"total_tokens\": 1126,\n","              \"input_token_details\": {\n","                \"audio\": 0,\n","                \"cache_read\": 0\n","              },\n","              \"output_token_details\": {\n","                \"audio\": 0,\n","                \"reasoning\": 0\n","              }\n","            },\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 40,\n","      \"prompt_tokens\": 1086,\n","      \"total_tokens\": 1126,\n","      \"completion_tokens_details\": {\n","        \"accepted_prediction_tokens\": 0,\n","        \"audio_tokens\": 0,\n","        \"reasoning_tokens\": 0,\n","        \"rejected_prediction_tokens\": 0\n","      },\n","      \"prompt_tokens_details\": {\n","        \"audio_tokens\": 0,\n","        \"cached_tokens\": 0\n","      }\n","    },\n","    \"model_provider\": \"openai\",\n","    \"model_name\": \"gpt-3.5-turbo-0125\",\n","    \"system_fingerprint\": null,\n","    \"id\": \"chatcmpl-CwUQF1O0YLSc2KPZ42sqgBtCC1J3t\",\n","    \"service_tier\": \"default\"\n","  },\n","  \"run\": null,\n","  \"type\": \"LLMResult\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n","\u001b[0m{\n","  \"output\": \"LangChainのIndexモジュールは、APIリファレンスやパッケージの最新情報などのリンクを提供します。\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.08s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"LangChainのIndexモジュールは、APIリファレンスやパッケージの最新情報などのリンクを提供します。\"\n","}\n","LangChainのIndexモジュールは、APIリファレンスやパッケージの最新情報などのリンクを提供します。\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# RAG用のプロンプトテンプレート\n","template = \"\"\"以下のコンテキストに基づいて質問に答えてください。\n","\n","コンテキスト:\n","{context}\n","\n","質問: {question}\n","\n","回答:\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# Retrieverを作成\n","retriever = vectorstore.as_retriever()\n","\n","# ドキュメントを文字列に変換する関数\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","# LCEL RAG Chain\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | chat\n","    | StrOutputParser()\n",")\n","\n","result = rag_chain.invoke(\"LangChainにおけるIndex moduleについて概要を1文で説明してください。\", config={'callbacks': [ConsoleCallbackHandler()]})\n","#result = rag_chain.invoke(\"LangChainにおけるIndex moduleについて概要を1文で説明してください。\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"jFRjpFYqiJh3"},"source":["`langchain.verbose = True`のため、無意味な動作確認メッセージも表示されているが、要約文が出力されていることがわかる\n","- なお、若干回答は的を得ていない\n"]},{"cell_type":"markdown","metadata":{"id":"8PBuUnC4jU0_"},"source":["DocumentLoadersは、webサイト、GoogleDrive、Slackなどを読み込む機能が存在しているため、様々な情報をVctor Storeに格納することができる"]},{"cell_type":"markdown","metadata":{"id":"S10hxRg_kURG"},"source":["このように、ある特定分野のデータを一連の要素としてエンコードし、それぞれが内部で 1 つの「ベクトル」として表現している\n","- これには、単語であればWord2Vecなどが利用できるが、ここでは文章であることから、BERTをFine-TuningしたSentenceTransformerの利用が検討される\n","- このベクトル数値は、多次元ベクトル空間で要素を相互に関連づけてマッピングしている\n","\n","ベクトル要素がセマンティックであり、ある一つの意味を表していると考えるならば、そのベクトルの近接性が文脈関係の指標となりえる\n","- このベクトルはエンベディングと呼ばれる\n","- 互いに関連性のある意味要素がまとまって配置されるようにエンベディングを行う\n","\n","特定分野の文脈によって、セマンティック要素は単語、フレーズ、センテンス、パラグラフ、文書全体、画像、あるいはまったく別のものになる可能性があり、エンベディングが最善というわけではない\n","\n","\n","\n","プロンプトで必要となる文脈を生成するため、データベースに問い合わせを行い、ベクトル空間の入力と密接に関連する要素を抽出する必要がある\n","\n","ベクトルデータストアは、大量のベクトルを保存し、問い合わせに答えるシステム\n","- 効率的な最近傍クエリアルゴリズム(k-NNなど)と適切なインデックスにより、データ検索を行う"]},{"cell_type":"markdown","metadata":{"id":"ARj8Y8_RdlD6"},"source":["## Memory\n","\n","会話履歴管理の推奨方法について、LangChainのMemory機能は進化中である：\n","\n","- 従来の `ConversationBufferMemory` は引き続き使用可能\n","- より柔軟な方法として、**メッセージリストを直接管理**することが推奨される\n","- 複雑なステート管理には **LangGraph** の利用も検討\n","\n","以下では次の点を学ぶ\n","\n","1. APIが過去の会話を記憶しないことを確認\n","2. `HumanMessage` と `AIMessage` を使った履歴管理\n","3. `ChatPromptTemplate` と `MessagesPlaceholder` の活用"]},{"cell_type":"markdown","metadata":{"id":"B0EPjzsTQntU"},"source":["ChatGPTをブラウザで利用した場合、過去の会話の履歴を踏まえて返答するが、APIではそのような振る舞いは行わない\n","\n","APIを利用する場合は、過去の履歴をプロンプトに入力する必要がある\n","\n","実際にその振る舞いを確認する"]},{"cell_type":"markdown","metadata":{"id":"YzABPdjkRm7e"},"source":["次のような関数を用意してAPIを用いて文章を渡す"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7pxRwGnUDvn"},"outputs":[],"source":["def post_chat_completions(content):\n","  url = \"https://api.openai.com/v1/chat/completions\"\n","  headers = {\n","      \"Content-Type\": \"application/json\",\n","      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","  }\n","  data = {\n","      \"model\": \"gpt-3.5-turbo\",\n","      \"messages\": [\n","          {\"role\": \"user\", \"content\": content}\n","      ],\n","      \"temperature\": 0,\n","  }\n","\n","  response = requests.post(url=url, headers=headers, json=data)\n","  # メッセージと使用トークン数を表示\n","  result = response.json()\n","  print(result[\"choices\"][0][\"message\"][\"content\"])\n","  print(f\"\\n使用トークン: {result['usage']['total_tokens']}\")\n","  # ログも表示する\n","  # print(json.dumps(response.json(), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"2tIxorsKRxtu"},"source":["では、実際に名前を伝える"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_OBzUQGVAwa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056124563,"user_tz":-540,"elapsed":738,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"99c45a7f-5923-4ecf-dc56-9808682ddf8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello Keio Yukichi! How can I assist you today?\n","\n","使用トークン: 29\n"]}],"source":["post_chat_completions(\"Hi! I'm Keio Yukichi!\")"]},{"cell_type":"markdown","metadata":{"id":"3-ruccLKR-Zi"},"source":["\"Hello Keio Yukichi! How can I assist you today?\" といった回答が得られているであろう\n","- Generativeであるため、この答えは毎回異なる\n","\n","その上で、名前を憶えているか聞いてみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYjoBKp8VAjZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056128028,"user_tz":-540,"elapsed":632,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"6123af84-4b15-49f0-e3dc-83c6164522e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["I'm sorry, I do not know your name.\n","\n","使用トークン: 24\n"]}],"source":["post_chat_completions(\"Do you know my name?\")"]},{"cell_type":"markdown","metadata":{"id":"RpXJqn09SJS0"},"source":["当然であるが、知らないという答えになる\n","\n","そこで、過去の会話の履歴を全て入れて、同じ質問を行ってみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ctwcsZ-VLdD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056131681,"user_tz":-540,"elapsed":525,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"bfbc2b9a-c484-421a-b053-4644a10d6390"},"outputs":[{"output_type":"stream","name":"stdout","text":["Yes, I do! Your name is Keio Yukichi. How can I assist you today?\n","\n","使用トークン: 64\n"]}],"source":["post_chat_completions(\"\"\"A: Hi! I'm Keio Yukichi!\n","B: Hello Keio Yukichi! How can I assist you today?\n","A: Do you know my name?\n","B: \"\"\")"]},{"cell_type":"markdown","metadata":{"id":"PuqgDlE5SipG"},"source":["となり、今度は\"Yes, you introduced yourself as Keio Yukichi.\"と回答している\n","\n","A:やB:は、会話しているのがどちらかを示す識別子であり、どのような形でもよい\n","- ただしLangChainは、内部でhumanとAIという用語を利用している\n","\n","このように、会話の過去の履歴を含めて問い合わせを行うため、過去の履歴を記録し、挿入するという処理が必要となる\n","- これがMemory moduleの役割である"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZHfxu9hY62e"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"Bu3uKzYrYo8a"},"source":["### ChatPromptTemplate と MessagesPlaceholder\n","\n","MessagesPlaceholder の役割\n","\n","- 会話履歴（`HumanMessage` と `AIMessage` のリスト）を挿入する場所を指定\n","- `variable_name=\"history\"` で、invoke時に `{\"history\": [...]}` として渡す\n","\n","コードにおいて、`StrOutputParser()` を使っているので、`ai_message` は文字列であり、履歴に追加する際は `AIMessage(content=ai_message)` でラップする"]},{"cell_type":"markdown","source":["実際に使うには、会話履歴をメッセージリストとして管理し、LLMに渡す\n","\n","プロンプト(文字を入力するための入力窓)が出てきたら、\n","- Hello. I'm Keio Yukichi\n","- Do you know my name?\n","- EOC\n","と入力する\n","\n","EOCは会話を終了させるおまじないである\n","- これは、ChatGPTの機能ではなく、そのようにプログラムしている"],"metadata":{"id":"CSpt7C_cOTke"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjkhKO7nVeEK","outputId":"42055bfb-02e0-4d83-db31-21b2b92cd04d","executionInfo":{"status":"ok","timestamp":1768056473329,"user_tz":-540,"elapsed":17164,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["You: Hello. I'm Keio Yukichi\n","AI: Hello Keio Yukichi! How can I assist you today?\n","You: Do you know my name?\n","AI: Yes, you introduced yourself as Keio Yukichi. How can I assist you further, Keio Yukichi?\n","You: EOC\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","\n","# プロンプトテンプレート（会話履歴を含む）\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful assistant.\"),   # システムプロンプト\n","    MessagesPlaceholder(variable_name=\"history\"), # 会話履歴を挿入\n","    (\"human\", \"{input}\")                          # 現在の入力\n","])\n","\n","# LCEL Chain\n","chain = prompt | chat | StrOutputParser()\n","\n","# 会話履歴を保持\n","history = []\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if user_message == 'EOC':\n","        break\n","\n","    # チェーンを実行\n","    ai_message = chain.invoke({\n","        \"history\": history,    # MessagesPlaceholder に対応\n","        \"input\": user_message  # (\"human\", \"{input}\") に対応\n","        })\n","\n","    # 履歴に追加\n","    history.append(HumanMessage(content=user_message))\n","    history.append(AIMessage(content=ai_message))\n","\n","    print(f\"AI: {ai_message}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JPsQbr0ZJ1z"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"C0xTWbzzV4_n"},"source":["## Agents\n","\n","Agentsとは、LLMが自律的に判断して、必要なツールを使い分けながらタスクを実行する仕組みである\n","\n","```\n","質問 → LLMが考える(Thought) → ツールを選択(Action) → 結果を観察(Observation) → ...\n","     → 最終回答\n","```\n","\n","なお、LangChainのAgents機能は前バージョンから大きく変更された\n","\n","- 従来の `initialize_agent` は**非推奨**\n","- 新しいプロジェクトでは **LangGraph** ベースの `create_react_agent` を使用\n","- より細かい制御と状態管理が可能になった\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xnb4KsklUOeo"},"source":["LLMが必要に応じて様々なタスクを実行すると便利と思うであろうが、これに対応するのがAgentsである\n","\n","例えば、\n","- 検索エンジンで検索させる\n","- 実際にコマンドを実行させる\n","- プログラミング言語やスクリプト言語でコードを実行させる\n","\n","これを行うのがAgents moduleである\n","\n","Agentsの利用により、実際にはLLMが何かを操作するわけではないが、LLMが何かしらアプリを操作しているかのように動作させることができる\n","\n","利用可能なツールの例は次の通り\n","\n","| ツール | 用途 |\n","|--------|------|\n","| `PythonREPLTool` | Pythonコードを実行 |\n","| `terminal` | シェルコマンドを実行（セキュリティリスク高）|\n","| `Wikipedia` | Wikipediaを検索 |\n","| `Calculator` | 計算を実行 |\n","| `WebSearch` | Web検索 |\n","\n","なお、セキュリティ上の警告として、以下の点に注意する\n","\n","- `PythonREPLTool` や `terminal` は**任意のコードが実行される**リスクがある\n","- 悪意のあるプロンプトにより危険なコマンドが実行される可能性\n","- 本番環境では十分な注意が必要"]},{"cell_type":"markdown","metadata":{"id":"qmfqEmF_VsdA"},"source":["実際にAgentsを利用してみよう\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGJkjbiBfq-2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056644764,"user_tz":-540,"elapsed":13200,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"12375db8-82cd-465f-d4d0-8f4ead3081cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/210.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# エージェント用の追加パッケージ\n","!pip install --quiet langchain-experimental\n","!pip install --quiet langgraph langchain-core"]},{"cell_type":"markdown","source":["なお、下記において\n","```\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","```\n","とすると、\n","```\n","/mnt/data\n","```\n","といった嘘の解答をすることがあり、これはハルシネーションである\n","\n","GPT-3.5は時々ツールを使わず「知識」で答える場合がるため、この問題を避けるには、GPT-4を使うか、より明示的な指示を与える必要がある\n","\n","```\n","result = agent_executor.invoke({\n","    \"messages\": [(\"user\", \"\"\"Execute this Python code and tell me the result:\n","import os\n","print(os.getcwd())\n","\n","You MUST actually run the code using the Python REPL tool.\"\"\")]\n","})\n","```"],"metadata":{"id":"HAptxFPvZkDd"}},{"cell_type":"markdown","source":["下記は正しく動作するが、"],"metadata":{"id":"hREMGCB-aK12"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768056981777,"user_tz":-540,"elapsed":8017,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"1af5a516-f4e8-4350-820e-ee5621607f49","id":"TcivVgZnZ9Es"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3917036320.py:14: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n","  agent_executor = create_react_agent(chat, tools)\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","\n","[HumanMessage]\n","Execute this Python code and tell me the result:\n","import os\n","print(os.getcwd())\n","\n","You MUST actually run the code using the Python REPL tool.\n","\n","[AIMessage]\n","Action: Python_REPL\n","Action Input: {'query': 'import os\\nprint(os.getcwd())'}\n","\n","[ToolMessage]\n","/content\n","\n","\n","[AIMessage]\n","The result of running the Python code `import os print(os.getcwd())` is `/content`.\n","==================================================\n","\n","The result of running the Python code `import os print(os.getcwd())` is `/content`.\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_experimental.tools import PythonREPLTool\n","from langgraph.prebuilt import create_react_agent\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# PythonREPLToolを使用（terminalは安全性の問題があるため）\n","tools = [PythonREPLTool()]\n","\n","# エージェントを作成（langgraph版 ReAct）\n","agent_executor = create_react_agent(chat, tools)\n","\n","# 実行\n","result = agent_executor.invoke({\n","    \"messages\": [(\"user\", \"\"\"Execute this Python code and tell me the result:\n","import os\n","print(os.getcwd())\n","\n","You MUST actually run the code using the Python REPL tool.\"\"\")]\n","})\n","\n","# 結果を表示（verbose風に思考過程も表示）\n","print(\"=\" * 50)\n","for msg in result[\"messages\"]:\n","    role = msg.__class__.__name__\n","    print(f\"\\n[{role}]\")\n","    if hasattr(msg, 'content') and msg.content:\n","        print(msg.content)\n","    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n","        for tc in msg.tool_calls:\n","            print(f\"Action: {tc['name']}\")\n","            print(f\"Action Input: {tc['args']}\")\n","print(\"=\" * 50)\n","\n","# 最終回答を出力\n","final_message = result[\"messages\"][-1]\n","print(f\"\\n{final_message.content}\")"]},{"cell_type":"markdown","source":["こちらは正しく動作しない"],"metadata":{"id":"qrKBnTUWaQhG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057057195,"user_tz":-540,"elapsed":8510,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"6e89414c-eea4-4dc0-9250-14fce2cc9ef5","id":"eNZKhOadaP75"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-527196736.py:14: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n","  agent_executor = create_react_agent(chat, tools)\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","\n","[HumanMessage]\n","What is the current working directory? Use Python's os module to find out.\n","\n","[AIMessage]\n","Action: Python_REPL\n","Action Input: {'query': 'import os\\nos.getcwd()'}\n","\n","[ToolMessage]\n","\n","[AIMessage]\n","The current working directory is: `/mnt/data`\n","==================================================\n","\n","The current working directory is: `/mnt/data`\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_experimental.tools import PythonREPLTool\n","from langgraph.prebuilt import create_react_agent\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# PythonREPLToolを使用（terminalは安全性の問題があるため）\n","tools = [PythonREPLTool()]\n","\n","# エージェントを作成（langgraph版 ReAct）\n","agent_executor = create_react_agent(chat, tools)\n","\n","# 実行\n","result = agent_executor.invoke({\n","       \"messages\": [(\"user\", \"What is the current working directory? Use Python's os module to find out.\")]\n","})\n","\n","# 結果を表示（verbose風に思考過程も表示）\n","print(\"=\" * 50)\n","for msg in result[\"messages\"]:\n","    role = msg.__class__.__name__\n","    print(f\"\\n[{role}]\")\n","    if hasattr(msg, 'content') and msg.content:\n","        print(msg.content)\n","    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n","        for tc in msg.tool_calls:\n","            print(f\"Action: {tc['name']}\")\n","            print(f\"Action Input: {tc['args']}\")\n","print(\"=\" * 50)\n","\n","# 最終回答を出力\n","final_message = result[\"messages\"][-1]\n","print(f\"\\n{final_message.content}\")"]},{"cell_type":"markdown","source":["gpt-4を利用するとうまくいく"],"metadata":{"id":"6_jYmA-laipS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbSTOTuKX7kJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057088739,"user_tz":-540,"elapsed":2414,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"7e67789f-8f29-49e8-9e91-0d6cbe0449af"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3171825838.py:11: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n","  agent_executor = create_react_agent(chat, tools)\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","\n","[HumanMessage]\n","What is the current working directory? Use Python's os module to find out.\n","\n","[AIMessage]\n","Action: Python_REPL\n","Action Input: {'query': 'import os; print(os.getcwd())'}\n","\n","[ToolMessage]\n","/content\n","\n","\n","[AIMessage]\n","The current working directory is `/content`.\n","==================================================\n","\n","The current working directory is `/content`.\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_experimental.tools import PythonREPLTool\n","from langgraph.prebuilt import create_react_agent\n","\n","chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n","\n","# PythonREPLToolを使用（terminalは安全性の問題があるため）\n","tools = [PythonREPLTool()]\n","\n","# エージェントを作成（langgraph版 ReAct）\n","agent_executor = create_react_agent(chat, tools)\n","\n","# 実行\n","result = agent_executor.invoke({\n","    \"messages\": [(\"user\", \"What is the current working directory? Use Python's os module to find out.\")]\n","})\n","\n","# 結果を表示（verbose風に思考過程も表示）\n","print(\"=\" * 50)\n","for msg in result[\"messages\"]:\n","    role = msg.__class__.__name__\n","    print(f\"\\n[{role}]\")\n","    if hasattr(msg, 'content') and msg.content:\n","        print(msg.content)\n","    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n","        for tc in msg.tool_calls:\n","            print(f\"Action: {tc['name']}\")\n","            print(f\"Action Input: {tc['args']}\")\n","print(\"=\" * 50)\n","\n","# 最終回答を出力\n","final_message = result[\"messages\"][-1]\n","print(f\"\\n{final_message.content}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbICnbceJ1nO"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"NCkT4uK8fHWE"},"source":["単純に/content というディレクトリにいるという回答であるが、実際正解である\n","\n","ただ、ここで疑問が生じる **なぜ、ChatGPTはこちらのシェル環境のカレントディレクトリがわかったのだろうか？**\n","\n","これは、AgentsがMRKL(ミラクル)やReActなどを利用して動作しているためである\n","- MRKL(Multi-Round Knowledge Loop)\n","- ReAct (Reasoning/Acting) なお、Reactではない\n"]},{"cell_type":"markdown","metadata":{"id":"-3eUMCemaz3X"},"source":["ログをみると、LangChainは次のようなプロンプトを生成している\n","```\n","Answer the following questions as best you can. You have access to the following tools:\n","\n","terminal: Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","```\n","\n","翻訳すると、\n","```\n","次の質問にできるだけ答えてください。あなたは以下のツールにアクセスできる：\n","\n","terminal: このLinuxマシンでシェルコマンドを実行する。\n","\n","以下の書式を使う：\n","\n","Question: あなたが答えなければならない入力問題\n","Thought: 何をすべきかを常に考える。\n","Action: 取るべき行動。[terminal]のどれかであるべき。\n","Action Input: アクションへの入力\n","Observation: 行動の結果\n","...（このThought/Action/Action Input/ObservationはN回繰り返すことができる）\n","Thought: 最終的な答えがわかった\n","Final Answer: 元の入力された質問に対する最終的な答え\n","\n","始める！\n","```\n","\n","となっており、これらの書式を用いて処理が進む\n","```\n","Question: What is your current directory?\n","```\n","という問いかけに対して、\n","```\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","```\n","とChatGPTが返答する\n","\n","そこで、AgentはAction Inputに記載されているコマンドを実行する\n","- その結果を Observationとして埋め込む\n","\n","さらに質問を続けるが、先のプロンプトに加えて、次の文章が加わっている\n","- つまり、これまでの動作をプロンプトに入力している\n","\n","```\n","Question: What is your current directory?\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","Observation: /content\n","\n","Thought:\n","```\n","これに対して\n","```\n","I now know the final answer\n","Final Answer: The current directory is /content.\n","```\n","と回答している\n","\n","Final Answerとして現在のディレクトリは/contentであることが示されており、Final Answerが返されたので、実行を終了している"]},{"cell_type":"markdown","metadata":{"id":"uJ0Z4_VyZfbU"},"source":["では、どんどんやってみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sX2ZcN2ZjT1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057109979,"user_tz":-540,"elapsed":2715,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"8c9d847c-8f91-477c-fa4d-001dba80d555"},"outputs":[{"output_type":"stream","name":"stdout","text":["I have created a new directory named 'testdir-by-agent'.\n"]}],"source":["result = agent_executor.invoke({\n","    \"messages\": [(\"user\", \"Make a new directory called 'testdir-by-agent' using Python's os module\")]\n","})\n","\n","print(result[\"messages\"][-1].content)"]},{"cell_type":"markdown","metadata":{"id":"hOQ0y-iOZ3W7"},"source":["ファイルも作ってみよう\n","\n","なお、途中で無料枠の場合はスロットを使い切ってしまうので、ワーニングメッセージと待ちが発生する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16ILSPFfZ486","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057122294,"user_tz":-540,"elapsed":3030,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f4695590-6558-441e-ccc7-13663b8fcd02"},"outputs":[{"output_type":"stream","name":"stdout","text":["It seems like the directory 'testdir-by-agent' already exists. Would you like to create a directory with a different name or delete the existing one and create it again?\n"]}],"source":["result = agent_executor.invoke({\n","    \"messages\": [(\"user\", \"Make a new directory called 'testdir-by-agent' using Python's os module\")]\n","})\n","\n","# 最終回答を出力\n","print(result[\"messages\"][-1].content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVBNXV0IKnbe"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"M_GU_jxdadC8"},"source":["testdir-by-agentの下にtest.txtがあり、その中身がThis is a testであることを確認しよう"]},{"cell_type":"markdown","metadata":{"id":"cQzbRtM5hCkG"},"source":["## MRKL(Multi-Round Knowledge Loop)\n","\n","例えば、「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」という問いに対して、ChatGPTは答えることができるか？\n","\n","これを直接WebのChatGPTに問い合わせても答えることができない\n","\n","しかしながら、APIでは答えることができる\n","- つまり、APIでChatGPTをアクセスすると、Webとは異なる仕組みでアクセスできるということ\n","\n","では、その手順であるが、まず質問に答えるための手順を考える\n","\n","- 最初に問い。「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」（Question）\n","\n","- Googleで未知の情報を調べるために検索ワードを考える(Thought)\n","\n","- 現在の日本とフランスの首相の年齢を知る必要がある\n","  - 検索ワードは「現在の日本の首相の年齢」(Action Input)\n","  - さらに検索ワードは「現在のフランス首相の年齢」(Action Input)\n","\n","- 検索を実行「現在の日本の首相の年齢」（Action）\n","  - 結果は65歳でした（obsabation)\n","\n","- 検索を実行します「現在のフランス首相の年齢」（Action）\n","  - 結果は61歳でした（obsabation)\n","\n","- 日本の首相の年齢とフランスの首相の年齢の差分を計算する必要がある（Thought）\n","\n","- 計算（Action）\n","\n","- 結果は4でした(obsabation)\n","\n","- 答えは4歳です (final)\n","\n","このようにMRKLは、ChatGPTが情報をもとに次のアクションを考え、結果を評価し、次のアクションを考えるというプロセスを繰り返すことで回答精度を上げる方法論である\n","\n","考察（Thought）、観察（Observation）、行動（Action）のサイクルを繰り返すことで、回答精度が向上する\n","\n","よく言われる、ステップバイステップで考えるように指示すると正答率が上がるのと似ているが、Agent側で実際に実行して応答できるように工夫されている"]},{"cell_type":"markdown","metadata":{"id":"ya0rvvL2jaAT"},"source":["## Prompt Coding\n","\n","プロンプトコーディングはChatGPTの活用において必須となる技術である\n","\n","ChatGPTから精度の高い回答を得るために、人間に質問するのと同様に、質問力が重要であり、その質問の仕方に関する研究が進められている\n","\n","例えば、以下のように役割の指定や回答の形式を細かく設定することで、正答率を上げることができる\n","- 質問や回答が定型化されており、プログラムで文字列を処理することが容易になり、解析が可能となる\n","\n","```\n","あなたは、英語の先生です。これから私の英語を英語教師として文法の誤りを訂正して下ださい。\n","回答のフォーマットは以下のようにします。\n","あなたの英語：{入力分}\n","訂正後の英文:{英文例}\n","文法の解説:{解説1000文字以内}\n","```"]},{"cell_type":"markdown","metadata":{"id":"3u5jaB1ikP5o"},"source":["## 実際のプロンプト\n","\n","先の年齢差を問う問題に答えさせる場合、次のようなプロンプトが想定される\n","- 内容は、シェルを実行するAgentの問い合わせと酷似する\n","\n","```\n","Answer the following questions as best you can.\n","You have access to the following tools:\\n\\n\n","\n","Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n\n","Calculator: Useful for when you need to answer questions about math.\\n\\n\n","\n","Use the following format:\\n\\n\n","\n","Question: the input question you must answer\\n\n","Thought: you should always think about what to do\\n\n","Action: the action to take, should be one of [Search, Calculator]\\n\n","Action Input: the input to the action\\n\n","Observation: the result of the action\\n\n","... (this Thought/Action/Action Input/Observation can repeat N times)\\n\n","Thought: I now know the final answer\\n\n","Final Answer: the final answer to the original input question\\n\\n\n","\n","Begin!\\n\\n\n","\n","\n","Question: 現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？ 計算してください\\nThought:')\n","```\n","\n","最初の\"Use the following format:\\n\\n\" 以前について、\n","\n","ここで、toolsについて何がどのように利用できるかを伝えているが、重要な点は次の通りである\n","\n","- Searchというツール名\n","  - コロンの前にツール名が記載されている\n","- ユースケースを伝える\n","  - (時事問題に関する質問に答える必要があるときに便利です)\n","- 入力形式を指定する\n","  - (入力は、検索クエリである必要がある)\n","\n","「入力は検索クエリである必要がある」と伝えているため、半角スペース区切りの単語単位での検索クエリを作成するようになる\n","- ChatGPTは時事問題に関する内容は、Searchツールを使うようになる\n","\n","最初の\"Use the following format:\\n\\n\" 以降について\n","\n","進め方とフォーマットを伝えている\n","\n","- Question:質問内容を記載\n","- Thought:何をすべきかを常に考える必要がある\n","  - アクションを考えるように指示\n","- Action: 実行するアクションは、 [Search, Calculator]のいずれかである必要がある\n","  - アクション名はツール名と同じであり、ChatGPTからActionの指示が出る際には[Search,Calculator]のキーワードが出力される\n","- Action Input: アクションへの入力\n","  - Searchの場合は指示されたクエリ形式で入力する\n","- Observation:Actionの結果\n","  - アクションの結果を表示\n","\n","さらに、最後について\n","\n","- (this Thought/Action/Action Input/Observation can repeat N times)\n","  - N回繰り返すは、答えが出ない場合に打ち切る回数や、API利用料金を抑えるための制限回数として、Nを指定できるようにしている\n","- Thought:回答が判明したら下記に進みます\n","- Final Answer:最終的な回答をします"]},{"cell_type":"markdown","metadata":{"id":"3BoivuKFnmUn"},"source":["実際に試行すると次のような結果を得ることができる\n","\n","```\n","> Entering new AgentExecutor chain...\n","I need to find out the age of the current Japanese and French Prime Ministers\n","Action: Search\n","Action Input: \"age of current Japanese Prime Minister\"params\n","\n","Observation: 65歳\n","Thought: Now I need to find out the age of the current French Prime Minister\n","Action: Search\n","Action Input: \"age of current French Prime Minister\"params\n","\n","Observation: 61歳\n","Thought: I now know the final answer\n","Final Answer: 4歳\n","```\n","\n"]},{"cell_type":"code","source":["!wget https://class.west.sd.keio.ac.jp/dataai/text/catimg.jpg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRZSelm3EWcz","executionInfo":{"status":"ok","timestamp":1768068059937,"user_tz":-540,"elapsed":1451,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"2eb5886a-7007-4187-d993-98b39b5fe528"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-10 18:00:58--  https://class.west.sd.keio.ac.jp/dataai/text/catimg.jpg\n","Resolving class.west.sd.keio.ac.jp (class.west.sd.keio.ac.jp)... 131.113.98.81\n","Connecting to class.west.sd.keio.ac.jp (class.west.sd.keio.ac.jp)|131.113.98.81|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 259945 (254K) [image/jpeg]\n","Saving to: ‘catimg.jpg’\n","\n","catimg.jpg          100%[===================>] 253.85K   487KB/s    in 0.5s    \n","\n","2026-01-10 18:00:59 (487 KB/s) - ‘catimg.jpg’ saved [259945/259945]\n","\n"]}]},{"cell_type":"markdown","source":["## 画像を読み込ませる\n","langchainで画像を処理する場合は次のようにする"],"metadata":{"id":"nw7o9lXmFMm8"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, SystemMessage\n","import base64\n","\n","# 画像ファイルをbase64エンコードする\n","def encode_image(image_path):\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode('utf-8')\n","\n","image_path = \"catimg.jpg\"\n","base64_image = encode_image(image_path)\n","\n","chat = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n","\n","# メッセージを直接作成（テンプレートを使わない）\n","messages = [\n","    SystemMessage(content=\"あなたは有能なアシスタントです。ユーザーの問いに回答してください\"),\n","    HumanMessage(\n","        content=[\n","            {\"type\": \"text\", \"text\": \"この画像は何の絵ですか？\"},\n","            {\n","                \"type\": \"image_url\",\n","                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n","            },\n","        ]\n","    ),\n","]\n","\n","result = chat.invoke(messages)\n","print(result.content)  # ← .content でテキスト取得"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Um363132B4NR","executionInfo":{"status":"ok","timestamp":1768068266349,"user_tz":-540,"elapsed":3089,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"0b2e4abc-ef7d-423a-d08d-8657dcfc569e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["この画像は、床に横たわっている三毛猫の写真です。猫はリラックスした様子で、カメラの方を見ています。背景には籐のバスケットが見えます。\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bqrKxwnhlstV"},"source":["##  Chat API におけるプロンプトの構築"]},{"cell_type":"markdown","metadata":{"id":"XNIICFODuMO_"},"source":["先のmemoryの例に加えて、\n","`import openai`\n","\n","および\n","\n","`langchain.verbose = True`\n","\n","を追加して、ログを詳細に取得する\n","\n","プロンプトに対して、\n","- Hi, I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC\n","\n","と入力する\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jX1cLM9FJfbe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057290039,"user_tz":-540,"elapsed":36341,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"a4a05112-1dc2-4d10-c6fc-37bd880207a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["You: Hi, I'm Keio Yukichi.\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n","\u001b[0m{\n","  \"history\": [],\n","  \"input\": \"Hi, I'm Keio Yukichi.\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n","\u001b[0m{\n","  \"history\": [],\n","  \"input\": \"Hi, I'm Keio Yukichi.\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n","\u001b[0m[outputs]\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"System: You are a helpful assistant.\\nHuman: Hi, I'm Keio Yukichi.\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [580ms] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"Hello Keio Yukichi! How can I assist you today?\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"Hello Keio Yukichi! How can I assist you today?\",\n","            \"additional_kwargs\": {\n","              \"refusal\": null\n","            },\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 13,\n","                \"prompt_tokens\": 26,\n","                \"total_tokens\": 39,\n","                \"completion_tokens_details\": {\n","                  \"accepted_prediction_tokens\": 0,\n","                  \"audio_tokens\": 0,\n","                  \"reasoning_tokens\": 0,\n","                  \"rejected_prediction_tokens\": 0\n","                },\n","                \"prompt_tokens_details\": {\n","                  \"audio_tokens\": 0,\n","                  \"cached_tokens\": 0\n","                }\n","              },\n","              \"model_provider\": \"openai\",\n","              \"model_name\": \"gpt-3.5-turbo-0125\",\n","              \"system_fingerprint\": null,\n","              \"id\": \"chatcmpl-CwUkUYAMimyutP27GXPcmdIO1QKR5\",\n","              \"service_tier\": \"default\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"lc_run--019ba86c-c09f-7c53-b704-8d823e4ec40f-0\",\n","            \"usage_metadata\": {\n","              \"input_tokens\": 26,\n","              \"output_tokens\": 13,\n","              \"total_tokens\": 39,\n","              \"input_token_details\": {\n","                \"audio\": 0,\n","                \"cache_read\": 0\n","              },\n","              \"output_token_details\": {\n","                \"audio\": 0,\n","                \"reasoning\": 0\n","              }\n","            },\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 13,\n","      \"prompt_tokens\": 26,\n","      \"total_tokens\": 39,\n","      \"completion_tokens_details\": {\n","        \"accepted_prediction_tokens\": 0,\n","        \"audio_tokens\": 0,\n","        \"reasoning_tokens\": 0,\n","        \"rejected_prediction_tokens\": 0\n","      },\n","      \"prompt_tokens_details\": {\n","        \"audio_tokens\": 0,\n","        \"cached_tokens\": 0\n","      }\n","    },\n","    \"model_provider\": \"openai\",\n","    \"model_name\": \"gpt-3.5-turbo-0125\",\n","    \"system_fingerprint\": null,\n","    \"id\": \"chatcmpl-CwUkUYAMimyutP27GXPcmdIO1QKR5\",\n","    \"service_tier\": \"default\"\n","  },\n","  \"run\": null,\n","  \"type\": \"LLMResult\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n","\u001b[0m{\n","  \"output\": \"Hello Keio Yukichi! How can I assist you today?\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [584ms] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"Hello Keio Yukichi! How can I assist you today?\"\n","}\n","AI: Hello Keio Yukichi! How can I assist you today?\n","You: Do you know my name?\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n","\u001b[0m[inputs]\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n","\u001b[0m[outputs]\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"System: You are a helpful assistant.\\nHuman: Hi, I'm Keio Yukichi.\\nAI: Hello Keio Yukichi! How can I assist you today?\\nHuman: Do you know my name?\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.32s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"Yes, you mentioned your name when you introduced yourself. How can I assist you further, Keio Yukichi?\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        },\n","        \"type\": \"ChatGeneration\",\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"Yes, you mentioned your name when you introduced yourself. How can I assist you further, Keio Yukichi?\",\n","            \"additional_kwargs\": {\n","              \"refusal\": null\n","            },\n","            \"response_metadata\": {\n","              \"token_usage\": {\n","                \"completion_tokens\": 23,\n","                \"prompt_tokens\": 53,\n","                \"total_tokens\": 76,\n","                \"completion_tokens_details\": {\n","                  \"accepted_prediction_tokens\": 0,\n","                  \"audio_tokens\": 0,\n","                  \"reasoning_tokens\": 0,\n","                  \"rejected_prediction_tokens\": 0\n","                },\n","                \"prompt_tokens_details\": {\n","                  \"audio_tokens\": 0,\n","                  \"cached_tokens\": 0\n","                }\n","              },\n","              \"model_provider\": \"openai\",\n","              \"model_name\": \"gpt-3.5-turbo-0125\",\n","              \"system_fingerprint\": null,\n","              \"id\": \"chatcmpl-CwUkjUfPywljsMDwH7ACScwAhpx7v\",\n","              \"service_tier\": \"default\",\n","              \"finish_reason\": \"stop\",\n","              \"logprobs\": null\n","            },\n","            \"type\": \"ai\",\n","            \"id\": \"lc_run--019ba86c-f981-7080-acc8-0f34d188d14c-0\",\n","            \"usage_metadata\": {\n","              \"input_tokens\": 53,\n","              \"output_tokens\": 23,\n","              \"total_tokens\": 76,\n","              \"input_token_details\": {\n","                \"audio\": 0,\n","                \"cache_read\": 0\n","              },\n","              \"output_token_details\": {\n","                \"audio\": 0,\n","                \"reasoning\": 0\n","              }\n","            },\n","            \"tool_calls\": [],\n","            \"invalid_tool_calls\": []\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 23,\n","      \"prompt_tokens\": 53,\n","      \"total_tokens\": 76,\n","      \"completion_tokens_details\": {\n","        \"accepted_prediction_tokens\": 0,\n","        \"audio_tokens\": 0,\n","        \"reasoning_tokens\": 0,\n","        \"rejected_prediction_tokens\": 0\n","      },\n","      \"prompt_tokens_details\": {\n","        \"audio_tokens\": 0,\n","        \"cached_tokens\": 0\n","      }\n","    },\n","    \"model_provider\": \"openai\",\n","    \"model_name\": \"gpt-3.5-turbo-0125\",\n","    \"system_fingerprint\": null,\n","    \"id\": \"chatcmpl-CwUkjUfPywljsMDwH7ACScwAhpx7v\",\n","    \"service_tier\": \"default\"\n","  },\n","  \"run\": null,\n","  \"type\": \"LLMResult\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n","\u001b[0m[inputs]\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n","\u001b[0m{\n","  \"output\": \"Yes, you mentioned your name when you introduced yourself. How can I assist you further, Keio Yukichi?\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.32s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"Yes, you mentioned your name when you introduced yourself. How can I assist you further, Keio Yukichi?\"\n","}\n","AI: Yes, you mentioned your name when you introduced yourself. How can I assist you further, Keio Yukichi?\n","You: EOC\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Note: openai.log = \"debug\" は新しいopenaiライブラリでは動作しません\n","# 代わりにLangSmithなどを使用してデバッグできます\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","\n","# プロンプトテンプレート\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful assistant.\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# LCEL Chain\n","chain = prompt | chat | StrOutputParser()\n","\n","# 会話履歴\n","history = []\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if user_message == 'EOC':\n","        break\n","\n","    ai_message = chain.invoke({\"history\": history, \"input\": user_message}, config={'callbacks': [ConsoleCallbackHandler()]})\n","\n","    history.append(HumanMessage(content=user_message))\n","    history.append(AIMessage(content=ai_message))\n","\n","    print(f\"AI: {ai_message}\")"]},{"cell_type":"markdown","metadata":{"id":"Ke23qQpWvIZM"},"source":["ログを参照することで、動作の詳細を獲得できる\n","\n","例えば、Memoryにより過去の履歴をプロンプトを与えることができるが、具体的には次のような動作をしている\n","\n","```\n","{\n","  \"prompts\": [\n","    \"System: You are a helpful assistant.\\nHuman: Hi, I'm Keio Yukichi.\\nAI: Hello Keio Yukichi! How can I assist you today?\\nHuman: Do you know my name?\"\n","  ]\n","}\n","```\n","\n","このように、すべてpromptとして混入している"]},{"cell_type":"markdown","metadata":{"id":"Nv6G19MewSJN"},"source":["さらに、`SystemMessage`, `HumanMessage`, `AIMessage`を用いて、それぞれの会話を仕分けできる\n","\n","- メッセージタイプの使い分け\n","\n","```python\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","```\n","\n","| タイプ | 用途 | OpenAI APIでのrole |\n","|--------|------|--------------------|\n","| `SystemMessage` | AIの振る舞いを設定 | `system` |\n","| `HumanMessage` | ユーザーからの発話 | `user` |\n","| `AIMessage` | AIからの回答 | `assistant` |\n","\n","- invoke()の戻り値\n","\n","```python\n","result = chat.invoke(messages)\n","\n","# resultはAIMessageオブジェクト\n","print(type(result))    # → <class 'AIMessage'>\n","print(result)          # → content='回答' additional_kwargs={...}\n","print(result.content)  # → '回答' （テキストだけ欲しい場合）\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X90oA5kIPGYG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057547434,"user_tz":-540,"elapsed":608,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"b5056381-9daf-40d2-c8ea-114fb8777afe"},"outputs":[{"output_type":"stream","name":"stdout","text":["content='How can I assist you today, Keio Yukichi?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 39, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CwUp8lpx18qA52puIuWranohejOLR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019ba871-2659-7613-8166-e19e88932ce8-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 39, 'output_tokens': 12, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","\n","messages = [\n","    SystemMessage(content=\"You are a helpful assistant.\"),\n","    HumanMessage(content=\"Hi! I'm Keio Yukichi!\"),\n","    AIMessage(content=\"Yes, You are Keio Yukichi.\")\n","]\n","\n","result = chat.invoke(messages)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wij-UYXMLZzW"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"GamQYH-iwucs"},"source":["これを踏まえて、先ほどのループ問い合わせプログラムを改善する\n","\n","メッセージリストを直接管理する\n","\n","1. **`invoke()` にはメッセージリストを渡す**\n","   - 辞書ではなく、`[HumanMessage(...), AIMessage(...), ...]` のリスト\n","2. **戻り値 `ai_response` は `AIMessage` オブジェクト**\n","   - そのまま `messages.append(ai_response)` で履歴に追加可能\n","   - テキストだけ欲しい場合は `ai_response.content`\n","3. **毎回全履歴をLLMに送信**\n","   - APIは状態を持たないため、文脈を理解させるには履歴全体を送る必要がある\n","\n","実際に実行して、次のようにプロンプトに入力する\n","- Hi. I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZAE9dogmna_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768057788435,"user_tz":-540,"elapsed":35991,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c03226fe-7457-4a1d-ff70-265bda80f2c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["You: Hi. I'm Keio Yukichi.\n","AI: Hello Keio Yukichi! How can I assist you today?\n","You: Do you know my name?\n","AI: Yes, you introduced yourself as Keio Yukichi. How can I assist you today, Keio?\n","You: EOC\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# メッセージ履歴を保持\n","messages = []\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if user_message == 'EOC':\n","        break\n","    # ユーザーメッセージを追加\n","    messages.append(HumanMessage(content=user_message))\n","    # LLMを呼び出し（履歴リスト全体を渡す）\n","    ai_response = chat.invoke(messages)\n","    # AI応答を履歴に追加\n","    messages.append(ai_response)  # ← AIMessageオブジェクトをそのまま追加\n","\n","    print(f\"AI: {ai_response.content}\")"]},{"cell_type":"markdown","metadata":{"id":"CApGZbWFGkaZ"},"source":["ここで一度実行を終了する\n","- 以降は「ランタイム」から「以降のセルを実行する」を選択して実行するとよい\n","- もしくは一つ一つクリックして実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j0TQZvcMZU5"},"outputs":[],"source":["from google.colab import runtime\n","#runtime.unassign()"]},{"cell_type":"markdown","metadata":{"id":"vy5mm7Trmjh4"},"source":["# Gradioについて"]},{"cell_type":"markdown","metadata":{"id":"SY5aPDLRFzfE"},"source":["***ここからは、まとめて実行せず、つ一つ一つクリックして実行した方が良い***\n"]},{"cell_type":"markdown","metadata":{"id":"LHpGhwMab4H1"},"source":["Webアプリを簡単に実装できるPythonライブラリ\n","\n","百聞は一見に如かずということで、早速実行してみよう\n","\n","---\n","\n","### Gradioとは？\n","\n","- 機械学習モデルのデモUIを**数行のコード**で作成可能\n","- チャットUI、画像認識、音声処理など様々なインターフェースに対応\n","- Google Colabと相性が良く、自動的にパブリックURLが発行される\n","\n","### 基本的な使い方\n","\n","```python\n","import gradio as gr\n","\n","def my_function(input):\n","    return f\"処理結果: {input}\"\n","\n","demo = gr.Interface(fn=my_function, inputs=\"text\", outputs=\"text\")\n","demo.launch()\n","```"]},{"cell_type":"markdown","metadata":{"id":"jhssf_w9moNv"},"source":["## gradio のインストール方法\n","\n","```\n","pip install gradio\n","```\n","\n","とし、例えば、\n","\n","```\n","import gradio as gr\n","```\n","とすることで利用可能となる\n","- (2023/12) ここでは互換性のために3.48.0を導入する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWx4JDa-MGDa"},"outputs":[],"source":["!pip -q install gradio"]},{"cell_type":"markdown","metadata":{"id":"miQy3qBhcQNz"},"source":["シンプルなWeb UIを作成して起動させる例を示す\n","- 名前を入力し、名前へのあいさつを出力するWeb UIである\n","\n","Colab上で実行すると、Colabのwebに統合される\n","- URLが出力されている通り、そのURLにアクセスできる環境にあれば、webのページとして表示される\n","- この例では、`https://localhost:7860/`などと表示されているが、クリックすると実は`https://wq0lbccui9-496ff2e9c6d22116-7860-colab.googleusercontent.com/`に転送されており、ネットワークセキュリティ上隔離されたColabの外からアクセスできるようになる\n","- Colab環境では、セキュリティ上の問題もあり、このようなグローバルアドレスが提供されない場合は、別途ボートフォーワーディングなどの知識が必要な場合がある\n","- また、ローカル上で他のWeb UIアプリを動作させているなどにより、ポートが競合する可能性がある\n","  - この場合、7860 が 7861 や 7862 といった番号に変わることがある\n","\n","\n","また、gradio clientを用いることで、作成したwebアプリケーションをweb APIのように利用することもできる\n","- 下に小さく「Use via API」と記載されているが、これをクリックし、記載の通りに実行すると動作がわかるであろう\n","·"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8KX4wgicW2Z","executionInfo":{"status":"ok","timestamp":1768057839021,"user_tz":-540,"elapsed":15974,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":648},"outputId":"d86a02b2-80b0-4c5a-cffb-d66e1ebce13f"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://91c7d0346e944f9e20.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://91c7d0346e944f9e20.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":66}],"source":["import gradio as gr\n","\n","# あいさつの関数\n","def greet(name):\n","    return \"Hello \" + name + \"!\"\n","\n","# Interfaceの作成\n","demo = gr.Interface(\n","    fn=greet,\n","    inputs=\"text\",\n","    outputs=\"text\"\n",")\n","\n","# 起動\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"lslskTPQflov"},"source":["## 設計手順\n","\n","次の手順となる\n","- コールバック関数を定義\n","- レイアウトを定義\n","- WebUIの起動\n","\n","それぞれについて概要をみてみよう\n","\n","---\n","\n","### Gradioアプリの3ステップ\n","\n","```python\n","import gradio as gr\n","\n","# 1. コールバック関数を定義\n","def process(input1, input2):\n","    result = input1 + input2\n","    return result\n","\n","# 2. レイアウトを定義\n","demo = gr.Interface(\n","    fn=process,           # 呼び出す関数\n","    inputs=[\"text\", \"text\"],  # 入力コンポーネント\n","    outputs=\"text\"        # 出力コンポーネント\n",")\n","\n","# 3. 起動\n","demo.launch()\n","```"]},{"cell_type":"markdown","metadata":{"id":"ZKnVtHYXgNiu"},"source":["\n","### コールバック関数\n","\n","まず、コールバック関数として、画面レイアウトでボタンが押された時に呼び出したい関数を記述する  \n","\n","例えば、名前(text)、表示フラグ(boolean)、値(0から100の値)の3つを受け取る関数として次のような関数を想定する  \n","```\n","def my_func(my_name, is_disp, my_value):\n","    return f'### {my_name} ###', my_value * 100\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2PxgV9NKnkjK"},"source":["### Interface\n","\n","「Interface」は、関数をUIでラップするためのクラスであり、主なパラメータは次のとおり\n","- fn : ラップする関数\n","- inputs : 入力コンポーネント (\"text\"、\"image\"、\"audio\"など)\n","- outputs : 出力コンポーネント (\"text\"、\"image\"、\"label\"など)\n","\n","---\n","\n","### 入力コンポーネントの種類\n","\n","| 指定 | コンポーネント |\n","|------|---------------|\n","| `\"text\"` | テキストボックス |\n","| `\"checkbox\"` | チェックボックス |\n","| `gr.Slider(0, 100)` | スライダー |\n","| `\"image\"` | 画像アップロード |\n","| `\"audio\"` | 音声アップロード |\n","\n","### 出力コンポーネントの種類\n","\n","| 指定 | コンポーネント |\n","|------|---------------|\n","| `\"text\"` | テキスト表示 |\n","| `\"number\"` | 数値表示 |\n","| `\"label\"` | ラベル（分類結果など）|\n","| `\"image\"` | 画像表示 |\n","\n","### 自動生成されるボタン\n","\n","- inputs には「クリアボタン」と「送信」ボタンが自動追加\n","- outputs には「フラグする」ボタンが自動追加（出力をファイルに保存）"]},{"cell_type":"markdown","metadata":{"id":"XjaSRRUCgmsO"},"source":["\n","- 最後にWebUIを起動する  \n","シンプルに、  \n","```\n","demo.launch()\n","```\n","とするだけでよい\n","\n","inputs には「クリアボタン」と「送信」ボタンが、outputs には「フラグする」ボタンが自動で追加される(フラグボタンは出力をローカルファイルに保存する)\n","\n","簡易的なWebサーバが起動し、ブラウザ上でWeb UIが表示される\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/gradio1.jpg\" width=700>\n"]},{"cell_type":"markdown","metadata":{"id":"fcPpqmEnt4qW"},"source":["## より実践的な例\n","\n","他にも様々な機能があるため、調べてみるとよい\n","\n","```\n","import gradio as gr\n","\n","# コールバック関数の定義\n","def callback_func(val1,val2,val3):\n","    return str(int(val1) * int(val2)), f\"気温は {val3} 度です\"\n","\n","# 画面レイアウトの定義(Interfaceを使用)\n","app = gr.Interface(\n","    title=\"計算機\",\n","    fn=callback_func,\n","    inputs=[\n","        gr.Textbox(label=\"入力欄1\",lines=3, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Textbox(label=\"入力欄2\",lines=5, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Slider(label=\"温度\",minimum=0,maximum=100,step=1)\n","    ],\n","    outputs=[\n","        gr.Label(label=\"計算結果1\",lines=3),\n","        gr.Textbox(label=\"計算結果2\",lines=3)\n","    ]\n","    )\n","\n","# Web UIの起動\n","app.launch(inbrowser=True)\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WXViJMW_oVc5"},"source":["### Blocks\n","\n","簡易的に使用する場合はInterfaceを使い、より複雑なレイアウトを作る場合はBlocksを使う\n","\n","---\n","\n","### Interface vs Blocks\n","\n","| 特徴 | Interface | Blocks |\n","|------|-----------|--------|\n","| 記述量 | 少ない | やや多い |\n","| 柔軟性 | 低い | 高い |\n","| レイアウト | 自動 | カスタム可能 |\n","| ボタン | 自動生成 | 手動配置 |\n","\n","### Blocksの基本構文\n","\n","```python\n","with gr.Blocks() as demo:\n","    # UIコンポーネントを配置\n","    input_box = gr.Textbox(label=\"入力\")\n","    output_box = gr.Textbox(label=\"出力\")\n","    btn = gr.Button(\"実行\")\n","    \n","    # イベントを定義\n","    btn.click(fn=process, inputs=input_box, outputs=output_box)\n","\n","demo.launch()\n","```\n","\n","### レイアウトクラス\n","\n","| クラス | 用途 |\n","|--------|------|\n","| `gr.Row()` | 横に並べる |\n","| `gr.Column()` | 縦に並べる |\n","| `gr.Tab()` | タブ表示 |\n","| `gr.Group()` | グループ化 |\n","| `gr.Accordion()` | 折りたたみ |"]},{"cell_type":"markdown","metadata":{"id":"cGy7qHvAtH4x"},"source":["#### コンポーネントを横や縦に並べる  \n","gr.Row()やgr.Column()を利用する\n","\n","コード中にコメントがあるので、切り替えてみるとよい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQxV_PT9rq2P","executionInfo":{"status":"ok","timestamp":1768057841993,"user_tz":-540,"elapsed":2951,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":648},"outputId":"e6b32b10-b103-4baf-cf8d-dec3861b62c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://c2fba71d3fa2504b23.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://c2fba71d3fa2504b23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":67}],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app:\n","  with gr.Row():\n","#  with gr.Column(scale=2):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\")\n","    # イベントハンドラー\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"4Itelzr_sjVE"},"source":["#### タブで表示する  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyByV3fYsmGQ","executionInfo":{"status":"ok","timestamp":1768057846728,"user_tz":-540,"elapsed":4713,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":648},"outputId":"f481112c-f93a-45c0-b245-d2b8ee149578"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d2506bd99dd96bd965.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d2506bd99dd96bd965.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":68}],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # 入力タブを定義\n","  with gr.Tab(\"入力タブ\"):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    btn = gr.Button(\"クリックしてね!\") # 出力タブを定義\n","  with gr.Tab(\"出力タブ\"):\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"nkv4_Mj4s_qW"},"source":["#### 丸角の子要素を設ける\n","\n","角が丸く、周囲にパディングがあるボックスである\n","- 以前はgr.Boxであったが、gr.Blocksに変更となった"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1IMxhLDtS9j","executionInfo":{"status":"ok","timestamp":1768057850353,"user_tz":-540,"elapsed":3604,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":648},"outputId":"68d9fa45-56a1-45e9-c1ad-e557ea2d5813"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://432a03aec600537dbc.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://432a03aec600537dbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":69}],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Box()関数でレイアウトを定義\n","  with gr.Group():\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"NnTwV4PrtfLK"},"source":["#### 子要素を折りたたみ可能にする\n","\n","Accordionは、子要素を折りたたみ可能なセクションに配置する\n","\n","openパラメータにより初期状態で開いているか(True)、閉じているか(False)を指定でき、デフォルトはOpen(True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCdTBWZduNgD","executionInfo":{"status":"ok","timestamp":1768057853649,"user_tz":-540,"elapsed":3286,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":648},"outputId":"69378e89-4bc5-40b7-9b71-e929da9e8f9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://ece345c1cfe1f0b06e.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ece345c1cfe1f0b06e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":70}],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Accordion()関数でレイアウトを定義\n","  with gr.Accordion(label=\"アプリを見る\", open=False):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"HiEG9ZpVpKGJ"},"source":["### チャットを実装する\n","\n","なんでもなく、\"How are you?\", \"I know you\", \"I'm very hungry\"のどれかをランダムに答えるアプリである"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hAUuk-oo47w","executionInfo":{"status":"ok","timestamp":1768057858732,"user_tz":-540,"elapsed":5078,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":721},"outputId":"7a87d077-cee3-4744-a117-b6c3696d2476"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4274059331.py:6: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot()\n","/tmp/ipython-input-4274059331.py:6: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n","  chatbot = gr.Chatbot()\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://a877ad15ab3413be2c.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://a877ad15ab3413be2c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":71}],"source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I know you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"WKL_t_xT02BX"},"source":["# LLMチャットの実装\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO27LB3bP-Sn","executionInfo":{"status":"ok","timestamp":1768057858743,"user_tz":-540,"elapsed":10,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1571fee-02f9-4d4a-8821-80110ac1b3ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":72}],"source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IErBgXXnKG1N","executionInfo":{"status":"ok","timestamp":1768057862218,"user_tz":-540,"elapsed":3460,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54bd4cad-f17e-433d-b903-dce9a4f31c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# ======================================================\n","# LLMチャットセクション用パッケージのインストール\n","# （セッション再起動後のため再インストールが必要）\n","# ======================================================\n","!pip uninstall -y --quiet tensorflow-probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cAsOhXoKG1S"},"outputs":[],"source":["!pip install --quiet openai cohere tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pTuyW9NKG1S","executionInfo":{"status":"ok","timestamp":1768057897808,"user_tz":-540,"elapsed":30194,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34acfda2-16b1-4a12-a46a-93b51c8d6f9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# opentelemetry依存関係の競合を回避\n","!pip install --quiet \"opentelemetry-api==1.37.0\" \"opentelemetry-sdk==1.37.0\" \\\n","    \"opentelemetry-exporter-otlp-proto-common==1.37.0\" \"opentelemetry-proto==1.37.0\" \\\n","    \"opentelemetry-exporter-otlp-proto-http==1.37.0\" 2>/dev/null\n","!pip install --quiet chromadb kaleido python-multipart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJNxN6pbKG1S"},"outputs":[],"source":["!pip install --quiet langchain langchain-community langchain-openai langchain-experimental"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRDSFHfIKute","executionInfo":{"status":"ok","timestamp":1768057932636,"user_tz":-540,"elapsed":13904,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25f3abcd-f765-4143-f025-20ef2f4268b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ LLMチャットセクションのパッケージインストール完了\n"]}],"source":["!pip -q install gradio\n","print(\"✅ LLMチャットセクションのパッケージインストール完了\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgGEEYyY1KxH"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","def chat(message: str) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","    response = llm.invoke(message)\n","    return response.content"]},{"cell_type":"markdown","metadata":{"id":"KYCXt_wRU6bb"},"source":["まずは、フレームワークとして、似非チャットを実装する\n","- ランダムに答えを返す\n","\n","---\n","\n","### Gradio `gr.Blocks` を使ったチャットUI\n","\n","#### コードの構造\n","\n","```python\n","import gradio as gr\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()  # チャット履歴表示エリア\n","    msg = gr.Textbox()      # 入力テキストボックス\n","    clear = gr.ClearButton([msg, chatbot])  # クリアボタン\n","\n","    def respond(message, chat_history):\n","        # message: 現在の入力テキスト\n","        # chat_history: [(user1, ai1), (user2, ai2), ...] 形式の履歴\n","        bot_message = \"応答\"\n","        chat_history.append((message, bot_message))\n","        return \"\", chat_history  # (入力欄クリア, 更新された履歴)\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()\n","```\n","\n","#### ポイント\n","\n","- **履歴の形式**: `[(\"ユーザー発話1\", \"AI応答1\"), ...]` のタプルリスト\n","- **`msg.submit()`**: Enterキーで送信したときのイベント\n","- **戻り値**: `(\"\", chat_history)` で入力欄をクリアし、履歴を更新"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vinBiFGT1N6n","executionInfo":{"status":"ok","timestamp":1768057940029,"user_tz":-540,"elapsed":7334,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":721},"outputId":"2e61bbf0-f62e-4f01-ebfd-5bbd73560d25"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2065328538.py:6: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot()\n","/tmp/ipython-input-2065328538.py:6: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n","  chatbot = gr.Chatbot()\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://fcf3cd2ae1cccbe988.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://fcf3cd2ae1cccbe988.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":79}],"source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"GoYskNFqH4ud"},"source":["次に、過去の会話履歴を踏まえて回答するように chat関数を次のように更新する\n","\n","---\n","\n","### LangChain + ChatMessageHistory の連携\n","\n","#### コードの構造\n","\n","```python\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.messages import HumanMessage\n","\n","def chat(message: str, history: ChatMessageHistory) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","    # 履歴からメッセージリストを取得\n","    messages = list(history.messages)\n","    \n","    # 現在のメッセージを追加\n","    messages.append(HumanMessage(content=message))\n","\n","    # LLMを呼び出し\n","    response = llm.invoke(messages)\n","    \n","    return response.content  # ← .content でテキストを取得\n","```\n","\n","#### ChatMessageHistory\n","\n","- `langchain_community.chat_message_histories` からインポート\n","- `.messages` プロパティでメッセージリストを取得\n","- `list()` でコピーしてから操作するのが安全"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igs39-zMH-b4"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","\n","def chat(message: str, history: ChatMessageHistory) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","    messages = list(history.messages)\n","    messages.append(HumanMessage(content=message))\n","\n","    response = llm.invoke(messages)\n","    return response.content"]},{"cell_type":"markdown","metadata":{"id":"ayaQTKNLVQUu"},"source":["実際に試してみよう\n","\n","- 私の名前は出田です。\n","- 私の名前がわかりますか？\n","\n","と入力すると、\n","「はい、先ほど出田さんとおっしゃいましたよね。」\n","といった回答になる\n","\n","---\n","\n","### Gradio ChatInterface + LangChain の連携\n","\n","#### コードの構造\n","\n","```python\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import AIMessage, HumanMessage\n","import gradio as gr\n","\n","llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo')\n","\n","def predict(message, history):\n","    # Gradio形式の履歴をLangChain形式に変換\n","    history_langchain_format = []\n","    for human, ai in history:\n","        history_langchain_format.append(HumanMessage(content=human))\n","        history_langchain_format.append(AIMessage(content=ai))\n","    \n","    # 現在のメッセージを追加\n","    history_langchain_format.append(HumanMessage(content=message))\n","    \n","    # LLMを呼び出し\n","    gpt_response = llm.invoke(history_langchain_format)\n","    \n","    return gpt_response.content  # ← .content でテキストを取得\n","\n","gr.ChatInterface(predict).launch()\n","```\n","\n","#### 履歴形式の変換\n","\n","```\n","Gradio形式:    [(\"user1\", \"ai1\"), (\"user2\", \"ai2\"), ...]\n","       ↓ 変換\n","LangChain形式: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...), ...]\n","```\n","\n","#### ポイント\n","\n","1. **`gr.ChatInterface`**: 簡潔にチャットUIを作成できる高レベルAPI\n","2. **履歴形式の違い**: Gradioはタプルリスト、LangChainはMessageオブジェクトリスト\n","3. **戻り値**: `gpt_response.content` で文字列を返す"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JH7cYHuOSLio","executionInfo":{"status":"ok","timestamp":1768057946743,"user_tz":-540,"elapsed":6661,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":684},"outputId":"d61b157a-4737-494f-9a19-fb24670fe78d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://b1d252f1287b5c7b72.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://b1d252f1287b5c7b72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":81}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import AIMessage, HumanMessage\n","import gradio as gr\n","\n","llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo')\n","\n","def predict(message, history):\n","    history_langchain_format = []\n","    for human, ai in history:\n","        history_langchain_format.append(HumanMessage(content=human))\n","        history_langchain_format.append(AIMessage(content=ai))\n","    history_langchain_format.append(HumanMessage(content=message))\n","    gpt_response = llm.invoke(history_langchain_format)\n","    return gpt_response.content\n","\n","gr.ChatInterface(predict).launch()"]},{"cell_type":"markdown","metadata":{"id":"lOIsJaGGdj1y"},"source":["# PDFドキュメントの内容をLangChainを用いて問い合わせる\n","\n","ChatGPTには一度に扱えるテキストの量に限界があり、一度に入力させることはできない\n","\n","そこで、巨大PDFの内容からテキストを抽出し、分割、テキスト間の関連性もつベクトルデータをベクターストアに格納する\n","\n","---\n","\n","### RAGパイプラインの流れ\n","\n","```\n","PDF → テキスト抽出 → チャンク分割 → ベクトル化 → ベクトルストア格納\n","                                                    ↓\n","質問 → ベクトル化 → 類似検索 → 関連チャンク取得 → プロンプトに追加 → LLM回答\n","```\n","\n","### 2025年版 パッケージのインポート先\n","\n","```python\n","# PDF読み込み\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Embeddings\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","\n","# Vector Store\n","from langchain_chroma import Chroma\n","\n","# テキスト分割\n","from langchain_text_splitters import CharacterTextSplitter\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYYsMYHbeVsj","executionInfo":{"status":"ok","timestamp":1768058120562,"user_tz":-540,"elapsed":6722,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6dc515a3-8dd4-44a0-dfb7-8ef94084c50a"},"outputs":[{"output_type":"stream","name":"stdout","text":["PDF QAセクションのパッケージインストール完了\n"]}],"source":["# PDF QAセクション用パッケージ\n","# （LLMチャットセクションから続けて実行する場合、langchain等はインストール済み）\n","!pip install --quiet pypdf langchain-chroma langchain-text-splitters\n","print(\"PDF QAセクションのパッケージインストール完了\")"]},{"cell_type":"markdown","metadata":{"id":"YxsdRd6fi0b2"},"source":["必要となるライブラリを読み込む"]},{"cell_type":"code","source":["# 必要なライブラリが見つからない場合のためにインストール\n","!pip install --quiet langchain-openai langchain-chroma langchain-community langchain-text-splitters openai chromadb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJ6RLsVCfEW0","executionInfo":{"status":"ok","timestamp":1768058302054,"user_tz":-540,"elapsed":15064,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"95ac5fa4-449a-4105-853c-6765f03e4e54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LzkJuKBeee0","executionInfo":{"status":"ok","timestamp":1768058339709,"user_tz":-540,"elapsed":33607,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4d96887-64d6-456a-9c3e-1d33fb051465"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"]}],"source":["import os\n","import platform\n","\n","import openai\n","import chromadb\n","\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_chroma import Chroma\n","from langchain_text_splitters import CharacterTextSplitter\n","from langchain_community.document_loaders import PyPDFLoader"]},{"cell_type":"markdown","metadata":{"id":"vfuM2_fii3k1"},"source":["API-KEYを読み込む\n","- エラーになる場合は、このテキストの最初にあるAPIキー設定箇所を再実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB17dhj0f_BW","executionInfo":{"status":"ok","timestamp":1768058359185,"user_tz":-540,"elapsed":45,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"00229899-cf8c-4393-950f-5d554d910c0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":9}],"source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"KhbAaMY-jEr4"},"source":["テスト用PDFとして、デジタル・ガバメント実行計画を利用する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbX0gqVhelI5","executionInfo":{"status":"ok","timestamp":1768058362195,"user_tz":-540,"elapsed":1222,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"29ec9294-5676-4d4a-9b4b-a67fac891147"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-10 15:19:20--  https://cio.go.jp/sites/default/files/uploads/documents/2020_dg_all.pdf\n","Resolving cio.go.jp (cio.go.jp)... 18.65.229.14, 18.65.229.116, 18.65.229.59, ...\n","Connecting to cio.go.jp (cio.go.jp)|18.65.229.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3609651 (3.4M) [application/pdf]\n","Saving to: ‘wp.pdf’\n","\n","wp.pdf              100%[===================>]   3.44M  5.11MB/s    in 0.7s    \n","\n","2026-01-10 15:19:21 (5.11 MB/s) - ‘wp.pdf’ saved [3609651/3609651]\n","\n"]}],"source":["if not os.path.exists('wp.pdf'):\n","  !wget https://cio.go.jp/sites/default/files/uploads/documents/2020_dg_all.pdf -O wp.pdf"]},{"cell_type":"markdown","metadata":{"id":"xgkvpakBjKHD"},"source":["PDFローダでテキスト化して読み込み、読分割する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc2W5lgNej0E"},"outputs":[],"source":["loader = PyPDFLoader(\"wp.pdf\")\n","pages = loader.load_and_split()"]},{"cell_type":"markdown","metadata":{"id":"eRBuWsaBjVS9"},"source":["5ページ目の文章を確認してみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWOUsB8DfKuw","executionInfo":{"status":"ok","timestamp":1768058371493,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/","height":308},"outputId":"6deec524-878b-4c8f-ed92-5dd231a1605c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'5 \\n \\n１ はじめに  \\n1.1 本計画の趣旨  \\n \\n「デジタルの活用により、一人ひとりのニーズに合ったサービスを選ぶことが\\nでき、多様な幸せが実現できる社会～誰一人取り残さない、人に優しいデジタ\\nル化～」 \\nデジタル庁（仮称）(以下単に「デジタル庁」という。）の設置を見据えた\\n「デジタル社会の実現に向けた改革の基本方針」（令和２年 12 月 25 日閣議決\\n定。以下「デジタル改革基本方針」という。）において、デジタル社会の目指\\nすビジョンが示された。 \\n あわせて、社会全体のデジタル化を進めるために、まずは国・地方の「行\\n政」が、自らが担う行政サービスにおいて、デジタル技術やデータを活用し\\nて、利用者目線に立って新たな価値を創出するデジタル・トランスフォーメー\\nションを実現し、「あらゆる手続が役所に行かずにできる」、「必要な給付が\\n迅速に行われる」といった手続面はもちろん、規制や補助金等においてもデー\\nタを駆使してニーズに即したプッシュ型のサービスを実現するなど、利用者目\\n線の改革を進めていくことが必要であり、これにより、あらゆる世代、あらゆ\\nる産業を対象とする行政サービスを通じて、社会全体にデジタル化によるメリ\\nットを、誰一人取り残さない形で広くいきわたらせていくこと、また、行政が\\n保有する様々なデータを、国民・企業が活用できるような形で連携できるデー\\nタ連携基盤を提供し、民間において様々なデジタル・ビジネスを創出するな\\nど、社会全体のデジタル化のための基盤を構築していくことが明記された。 \\n \\n社会全体のデジタル化を進める上で、デジタル・ガバメント推進の取組は重\\n要な役割を担う。 \\nデジタル・ガバメント推進に係る近年の取組としては、2019 年（令和元年）\\nに改正後の情報通信技術を活用した行政の推進等に関する法律（平成 14 年法\\n律第 151 号。以下「デジタル手続法」という。）が施行され、行政のあらゆる\\nサービスを最初から最後までデジタルで完結させるために不可欠なデジタル３\\n原則（①デジタルファースト：個々の手続・サービスが一貫してデジタルで完\\n結する、②ワンスオンリー：一度提出した情報は、二度提出することを不要と\\nする及び③コネクテッド・ワンストップ：民間サービスを含め、複数の手続・\\nサービスをワンストップで実現する）を基本原則として明確化するとともに、\\n国の行政手続のオンライン化実施が原則とされた。'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["pages[5].page_content"]},{"cell_type":"markdown","metadata":{"id":"tNIT8POUjbHM"},"source":["ChatGPTを準備する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3blnhosfQ5S"},"outputs":[],"source":["# OpenAI API キーは環境変数 OPENAI_API_KEY から自動的に読み取られます\n","llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"]},{"cell_type":"markdown","metadata":{"id":"OMEsL8wCjcsg"},"source":["PDF文章をembeddingしてベクターストアに登録する\n","\n","このベクターストアを LLM に与えることで、テキスト間の関連性について表現したベクトルデータを外部から与え、LLMで利用できるようになる\n","\n","---\n","\n","### コードの解説\n","\n","```python\n","# Embeddingsモデルの作成\n","embeddings = OpenAIEmbeddings()\n","\n","# ベクトルストアの作成\n","vectorstore = Chroma.from_documents(\n","    pages,                          # ドキュメントのリスト\n","    embedding=embeddings,           # 埋め込みモデル\n","    persist_directory=\"./chroma_db\" # 保存先ディレクトリ\n",")\n","```\n","\n","#### Chroma.from_documents() の動作\n","\n","1. 各ドキュメントの `page_content` をベクトル化\n","2. ベクトルとメタデータをChromaDBに格納\n","3. `persist_directory` を指定すると自動的に永続化（再起動後も利用可能）\n","\n","#### 【重要】新しいChromaの変更点\n","\n","- `persist_directory` を指定すると**自動的に永続化**される\n","- 明示的な `vectorstore.persist()` の呼び出しは不要"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCIHq40PgeW5","executionInfo":{"status":"ok","timestamp":1768058479302,"user_tz":-540,"elapsed":7130,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c00c856-7b4f-48d2-ec4b-d8cd96d2da6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Created vectorstore with 335 documents\n"]}],"source":["embeddings = OpenAIEmbeddings()\n","# 新しいバージョンのChromaでは persist_directory を指定すると自動的に永続化されます\n","vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=\"./chroma_db\")\n","print(f\"Created vectorstore with {vectorstore._collection.count()} documents\")"]},{"cell_type":"markdown","metadata":{"id":"3MSvZvvXjh-W"},"source":["PDF ドキュメントへ自然言語で問い合わせる\n","\n","ここでは、回答文の作成に関連した元テキスト群についても示すように指定する\n","\n","コードのポイントは次の通り\n","1. **`retriever.invoke(question)`**: 質問に類似したドキュメントを検索\n","2. **`format_docs(docs)`**: Documentリストを文字列に変換\n","3. **`rag_chain.invoke({...})`**: プロンプトの変数を辞書で渡す\n","4. **戻り値に `source_documents`** を含めることで、回答の根拠を確認可能"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdyjX_OIgjL9","executionInfo":{"status":"ok","timestamp":1768058537817,"user_tz":-540,"elapsed":8,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"298b1b11-8be1-408e-8d97-61f234a7e668"},"outputs":[{"output_type":"stream","name":"stdout","text":["PDF QA system ready!\n"]}],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# RAG用のプロンプトテンプレート\n","qa_template = \"\"\"以下のコンテキストに基づいて質問に答えてください。コンテキストに答えがない場合は、「わかりません」と答えてください。\n","\n","コンテキスト:\n","{context}\n","\n","会話履歴:\n","{chat_history}\n","\n","質問: {question}\n","\n","回答:\"\"\"\n","\n","qa_prompt = ChatPromptTemplate.from_template(qa_template)\n","\n","# Retrieverを作成\n","retriever = vectorstore.as_retriever()\n","\n","# ドキュメントを文字列に変換\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","# 会話履歴をフォーマット\n","def format_chat_history(chat_history):\n","    if not chat_history:\n","        return \"なし\"\n","    formatted = []\n","    for q, a in chat_history:\n","        formatted.append(f\"Q: {q}\\nA: {a}\")\n","    return \"\\n\".join(formatted)\n","\n","# LCEL RAG Chain\n","rag_chain = qa_prompt | llm | StrOutputParser()\n","\n","# 会話履歴対応のRAG関数\n","def pdf_qa_invoke(question: str, chat_history: list):\n","    # 関連ドキュメントを取得\n","    docs = retriever.invoke(question)\n","    context = format_docs(docs)\n","    history_text = format_chat_history(chat_history)\n","\n","    # RAGチェーンを実行\n","    answer = rag_chain.invoke({\n","        \"context\": context,\n","        \"chat_history\": history_text,\n","        \"question\": question\n","    })\n","\n","    return {\n","        \"answer\": answer,\n","        \"source_documents\": docs\n","    }\n","\n","print(\"PDF QA system ready!\")"]},{"cell_type":"markdown","metadata":{"id":"v8FvaEGDj2EB"},"source":["デジタルインフラの整備の予算について問い合わせてみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3zyM2nWidDT","executionInfo":{"status":"ok","timestamp":1768058544600,"user_tz":-540,"elapsed":1414,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e443cde3-1c21-46aa-ee44-a4636da83a37"},"outputs":[{"output_type":"stream","name":"stdout","text":["デジタルインフラの整備に関連する予算は、2021年度（令和３年度）には87システムの整備・運用等に必要な予算計2,986億円が内閣官房及びデジタル庁に一括計上されています。\n"]}],"source":["query = \"デジタルインフラの整備についてどのような予算がありますか？\"\n","chat_history = []\n","\n","result = pdf_qa_invoke(query, chat_history)\n","\n","print(result[\"answer\"])"]},{"cell_type":"markdown","metadata":{"id":"NuhpAlr1kCPk"},"source":["上記の回答に関連した元のテキスト群について確認する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIVIoNhCgx-9","executionInfo":{"status":"ok","timestamp":1768058545945,"user_tz":-540,"elapsed":44,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6aa1728c-6c2b-4352-95a2-677022718964"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source documents:\n","\n","--- Document 1 ---\n","36 \n"," \n","ための手順等が明確化されたところであり、内閣官房は各府省への周知徹底を\n","引き続き実施する。 \n"," また、プロジェクトの規模や政策的重要性等の観点から重点的なプロジェク\n","ト管理を行う必要があると認められるプロジェクト（以下「政府重点プロジェ\n","クト」という。）については、プロジェクトに係る予算の執行府省の担当者、\n","総務省の担当者や財務省の担当者等も参加するプロジェクトチームを内閣官房\n","に編成...\n","\n","--- Document 2 ---\n","18 \n"," \n"," この中で、デザインシステム、ベース・レジストリ等今までにないデジタ\n","ル・ガバメントの考え方が具体化してきている。2030 年（令和 12 年）も見据\n","えて今後も、本グランドデザインを参考にしつつ、デジタル・ガバメントの取\n","組を推進する。 \n"," \n","（参考）デジタル・ガバメント実現のためのグランドデザイン（概要） \n","  \n","（出所：令和２年３月31 日CIO 連絡会議資料） \n"," \n","4.2 デ...\n","\n","--- Document 3 ---\n","37 \n"," \n","のうち、デジタルインフラの整備及び運用に係る予算については、原則とし\n","て、内閣官房が、IT 基本法第 26 条第 1 項に定める高度情報通信ネットワーク\n","社会の形成に関する施策の実施の推進のための事務の一環として一括して要\n","求・計上し、各府省に配分することとしている。 \n"," 情報システム関係予算については、2020 年度（令和２年度）において一括要\n","求・一括計上の開始年として、デジタルイ...\n","\n","--- Document 4 ---\n","8 \n"," \n","る。ただし、本計画ではデジタル庁の設置以前の組織、制度等を前提とした記\n","述も含まれているところであり、デジタル庁が発足した暁には、改めて本計画\n","の見直しや、他の基本計画との関係の整理等を検討する。 \n"," \n","なお、本計画はデジタル手続法第４条に基づく情報通信技術を利用して行わ\n","れる手続等に係る国の行政機関等の情報システムの整備に関する計画として位\n","置付けることとする。 \n"," \n","1.2 計画期間...\n"]}],"source":["print(\"Source documents:\")\n","for i, doc in enumerate(result['source_documents']):\n","    print(f\"\\n--- Document {i+1} ---\")\n","    print(doc.page_content[:200] + \"...\")"]},{"cell_type":"markdown","metadata":{"id":"5ewuvirBkIov"},"source":["デジタルインフラの整備により何が影響を受けるか問い合わせる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCIPxwSxg4Gz","executionInfo":{"status":"ok","timestamp":1768058551278,"user_tz":-540,"elapsed":1946,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"421f3edc-06a1-44ea-b342-278701cee11a"},"outputs":[{"output_type":"stream","name":"stdout","text":["デジタルインフラの整備により、政府全体として共用化・標準化が進み、投資効率の向上や政府横断的な観点からの効果が期待されます。また、デジタルデバイド対策において、年齢や障害の有無、性別、国籍、経済的な理由にかかわらず、全ての国民にデジタル化の恩恵が広く行き渡る環境の整備にも影響を与えることが期待されます。\n"]}],"source":["chat_history = [(query, result[\"answer\"])]\n","query2 = \"デジタルインフラの整備により何が影響を受けるか？\"\n","\n","result2 = pdf_qa_invoke(query2, chat_history)\n","\n","print(result2[\"answer\"])"]},{"cell_type":"markdown","metadata":{"id":"Ue-g0X-0kNJ5"},"source":["ChatGPT による回答の正確性については保証されていないが、難解な資料を素早く理解するにはかなり有効であるといえよう"]},{"cell_type":"markdown","metadata":{"id":"pkz25WaGDlYG"},"source":["# RAG (Retrieval Augmented Generative)\n","\n","(A Cheat Sheet and Some Recipes For Building Advanced RAGで紹介されている内容)\n"]},{"cell_type":"markdown","metadata":{"id":"En2eQBk2FnJr"},"source":["## RAG の基本\n","\n","RAGは、ユーザーの質問に対して、外部データベースから関連するドキュメントを取得し、そのドキュメントと元々のユーザーの質問をセットにしてLLMに渡され、LLMは、この内容をもとに回答を生成する手法\n","\n","既に述べたLangChainのDictionaryがこれに相当する\n","- 目的はハルシネーションを抑え、最新の情報や、極めて専門的もしくは独自の情報を用いて回答を作成したいときなど\n","\n","RAGにより以下が可能となる\n","- ハルシネーションやノイズを削減させる\n","- 情報不十分のとき回答しないようにする\n","- 複数の情報を参照して回答する\n","- 情報ソースの誤りを指摘できる\n","\n","RAG品質は、次の指標で計測する\n","- コンテキストの関連性\n","- 回答の関連性\n","- 忠実度合\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GwmBo4dbFi42"},"source":["## RAGの応用\n","\n","複雑な質問や高度なデータソースに対応するため、次の2つの重要な要件を満たすために高度な技術や戦略について述べる\n","\n","- Retrieval（検索）: ユーザーの質問に最も関連性の高いドキュメントを見つけること\n","- Generation（生成）: 検索されたドキュメントを効果的に利用し、回答を生成すること\n","\n","これを実現する技術的手法は以下の通りである"]},{"cell_type":"markdown","metadata":{"id":"Vg-_gdtIF7BQ"},"source":["### 検索の要件に対する技術\n","\n","- Chunk-size Optimization（チャンクサイズの最適化）:  \n","検索対象のドキュメントを適切なサイズに分割することで、情報の関連性を高める手法  \n","検索対象が多すぎる場合、効率が低下するため、適切なサイズのチャンクに分割することが重要\n","\n","- Sliding Window Chunking（スライディングウィンドウチャンク分割）:  \n","情報の見落としを防ぐために、ドキュメントをオーバーラップしながらスライドさせて分割する手法  \n","重要な情報がチャンクの境界に分割されるのを防ぐ\n","\n","- Structured Knowledge Integration（構造化ナレッジの統合）:   \n","構造化データ（例：データベースやスプレッドシート）を検索システムに統合し、より精度の高い検索結果を提供する手法\n","\n","- Metadata Attachments（メタデータの付与）:   \n","メタデータ（例：著者、出版日、トピックなど）を利用して、検索精度を向上させる手法  \n","文脈に沿った関連性の高いドキュメントを絞り込むことができる\n","\n","- Knowledge Graphs（ナレッジグラフ）:   \n","ナレッジグラフを活用し、ドキュメント間の関連性を明確にする手法  \n","情報同士の関係性を視覚化することで、検索の精度が向上する\n","\n","- Mixed Retrieval（混合検索）:   \n","複数の検索アルゴリズムを組み合わせ、情報源から最も有用なドキュメントを取得する手法  \n","異なるアプローチを組み合わせることで、異なる角度から関連性が高められる\n","\n","- Question Embedding Transformation（質問の埋め込み変換）:  \n"," 質問を埋め込み表現に変換し、情報と質問の一致を効果的に計算する手法  \n","質問の文脈を理解し、より関連性の高い検索結果が取得できる"]},{"cell_type":"markdown","metadata":{"id":"oFR2VQ7QF32a"},"source":["### 生成の要件に対する技術\n","\n","- Information Compression（情報の圧縮）:   \n","生成モデルが扱いやすい形に情報を圧縮する手法  \n","例えば、ノイズを減らしたり、重要な情報を抽出してから生成を行う\n","\n","- Generator Fine-Tuning（生成モデルの微調整）: 検索されたドキュメントに基づいて、生成モデルを特定のタスクに適応させるための微調整を行う手法  \n","生成された回答の精度が向上する\n","\n","- Result Re-Ranking（結果の再ランキング）: 一度生成された結果を再度評価し、最も関連性が高いものを再ランキングする手法  \n","ユーザーに提供する最終結果の品質が向上する\n","\n","- Adapter Methods（アダプターメソッド）: 生成モデルに外部情報を柔軟に取り入れるための手法  \n","新しい情報を簡単に生成モデルに適応させることができる"]},{"cell_type":"markdown","metadata":{"id":"ITzu1DBNG_x9"},"source":["### 同時に2つの要件を満たす技術\n","- Monolithic Fine-Tuning（単一モデルの微調整）:   \n","検索と生成の両方を同時に最適化するために、単一の大規模モデルを微調整する手法  \n","情報検索と生成のプロセスが統合され、シームレスな処理が可能になる\n","\n","- Retrieval-Foundational Model（検索基盤モデル）:   \n","検索モデルそのものを基礎モデルに統合し、検索結果と生成がより密接に結びついた形で動作するように設計する手法\n","\n","- Generator-Enhanced Retrieval（生成強化型検索）:   \n","生成モデルを利用して、検索結果のリランキングやフィルタリングを行う手法  \n","生成モデルが得た情報をさらに効果的に検索に活用できる\n","\n","- Iterative Retrieval-Generation（反復検索生成）:   \n","検索と生成のプロセスを繰り返し行い、精度の高い回答を得る手法  \n","例えば、一度検索された結果を元に、再度質問を修正しながらプロセスを繰り返す"]},{"cell_type":"markdown","metadata":{"id":"_Is6AnBzJAt-"},"source":["### RouterRetriever\n","\n","RAGで複数の埋め込みモデルを利用し、文書検索精度を向上させる手法\n","\n","(KAISTとアレン人工知能研究所により提案)\n","\n","通常のRAGは、埋め込みモデル（エンべディングモデル）を1つだけ利用して、ベクトル検索する\n","- ユーザーの質問とソースとなる大量の文書を、ただ一つの同じ埋め込みモデルでベクトル化して、検索している\n","- この埋め込みモデルとして、OpenAIが公開している「text-embedding」といった汎用的モデルの利用が一般的\n","\n","汎用的な埋め込みモデルではなく、そのドメインに特化したモデルを利用することで検索精度が高くなる\n","\n","RouterRetrieverは、ユーザーの質問のジャンルに応じて、複数の埋め込みモデルを使い分ける手法\n","\n","#### 従来手法における問題点\n","\n","従来はユーザーの質問に専門的な内容が入っている場合、文書検索が適切に行えなかった\n","\n","これは、RAGの文書検索がもつ以下のジレンマが関係している\n","\n","- RAGである分野の検索に強くするには、その分野に特化した埋め込みモデルにするのが一番よい\n","- 一方でその分野に特化すると、それ以外の質問には、弱くなる\n","\n","現状は、これに対し、全ての分野に平均的に強い「バランス型」の埋め込みモデルを1つだけ使うという方針であった\n","- どのような問いあわせでも平均的な性能が獲得できる\n","\n","性能を上げるには、ハイブリッド学習のように、複数の埋め込みモデルを動的に使い分ける\n","- ユーザーからの質問に対して、「どの埋め込みモデルを使うの最適か」を判断し、そのモデルで検索を行う\n","\n","\n","#### 事前にやっておくこと\n","\n","- 専門家モデル（例えば、医学特化の埋め込みモデルなど）を複数作成しておく\n","- `Contriever`という汎用的な埋め込みモデルをLoRA（Low-Rank Adaptation）して複数作成しておく\n","  - ユーザーの質問には複数のモデルを使い分けますが、ドキュメントの埋め込みには、通常通り1つの埋め込みモデル（Contriever）のみを利用\n","- 「どの埋め込みモデルを使うの最適か」を、動的に判断するための仕組みを構築\n","- ユーザーが質問を入力してきた際に、ユーザーの質問を汎用的なモデル（Contriever）でベクトル化\n","  - このベクトルと、事前に用意した「各専門家モデルが得意なタイプの質問のベクトル」を比較\n","\n","#### 埋め込みモデルの選択\n","\n","で最もスコアが高かったモデルを利用して、もう一度ユーザーの質問をベクトル化\n","\n","このベクトルを利用して、検索対象である、大量の文書を検索（ここは通常のRAGと同じ）\n","\n","\n","BEIRベンチマークにおいて、MSMARCOで訓練された単一モデルに比べて+2.1、マルチタスク訓練モデルに比べて+3.2のnDCG@10スコア向上を達成\n","ゲート（専門家モデル）の数を増やすにつれて、一貫して性能が向上\n","ドメインに特化した専門家モデルがないデータセットに対しても汎用性を持つことが確認されている"]},{"cell_type":"markdown","metadata":{"id":"OXsU26yn9VP4"},"source":["# その他トピック\n","\n","ここ3ヵ月で次の通り、追いかけるのも大変\n","- 1年たち、既に古すぎる情報となった\n","\n","## DALL-E3\n","画像生成AIの中でも高精度・高品質画像を作成可能なAI\n","\n","## SDXL Turbo\n","リアルタイムでプロンプトに反応して画像生成\n","https://clipdrop.co/ja/stable-diffusion-turbo\n","\n","## Adobe Firefly\n","ようやくAdobeも動きだし、著作権・商用利用問題クリアの画像生成AIを公開\n","\n","## Notion AIやCursorなどのLLM統合\n","- Notionは様々な機能を持つメモ帳\n","  - AI拡張は有料\n","- Cursorはプログラミング用統合環境\n","  - VSCodeオワコンといわれている\n","\n","## Claude\n","- ChatGPT並み(かそれ以上)の精度を持つLLM\n","\n","## Google Gemini\n","- Googleとの連携がとりやすく、使い勝手の良いLLM\n","- Colabとも連携\n","\n","## Runway Gen-2\n","画像から高精細かつ高品質な動画を生成\n","- 動かしたいものを指定したり、プロンプトで指示することができる\n"]},{"cell_type":"markdown","metadata":{"id":"EXKfzAT0TmDV"},"source":["# 課題1(LangChain)\n","\n","PDFファイル解析アプリケーションの例を、LangChainを用いたチャットボット形式に修正しなさい"]},{"cell_type":"markdown","metadata":{"id":"OEnQXDE_G8Ju"},"source":["# 課題2(LangChainによるチューニング)\n","\n","履歴付きチャットボットを改造し、常に回答が子供の対応であるようにプロンプトエンジニアリングを行いなさい\n","\n","単純に、入力問い合わせに対して、「考え方や言葉遣いを6歳の子供のようにして答えなさい」といった言葉を付け加えなさい"]},{"cell_type":"markdown","metadata":{"id":"xoemzd9MkM_7"},"source":["# 課題3(GPTs)\n","\n","ChatGPTのGPTsを試しなさい\n","\n","この課題を解くには、ChatGPT4を利用する必要があるため、課金が伴う点に注意しなさい\n","\n","例えば、\n","\n","- 政府発行の文章や、博物館などの解説ページなどの情報を参照し、専門的な内容に回答するチャットを作成する\n","  - 会社の内部文章などが面白いが、普通に規定でできないであろう\n","  - 仮想的にそのような「公開されていない情報」(自分の日記など)について試すとよい\n","- 日本語で必ず回答するようにする\n","- 発言の最後に、専門用語についての解説を付与するようにする\n","\n","など\n","\n","また、GPTsにおけるAPI操作機能を用いて、気温や湿度などを住所から入手し、適切な服装などを指示するGPTを作成しなさい\n","\n","https://weather.tsukumijima.net/ などを利用するとよい\n","\n","さらに、HotPepper APIを用いて、お店を検索するGPTを作成しなさい\n","- これについて、HotPepper側の制限によりアクセスが拒否されている場合がある\n","- この場合は、Proxyを立てること"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/keioNishi/lec-mlsys/blob/main/mlsys-text-M-ChatGPT-2-Application.ipynb","timestamp":1703525217224},{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}