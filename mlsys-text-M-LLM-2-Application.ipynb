{"cells":[{"cell_type":"markdown","metadata":{"id":"bHZ54p4iIfem"},"source":["---\n",">「不用意にもらす言葉こそ、ほんとうらしいものをふくんでいるのだ。 」\n",">\n","> 太宰治\n","---"]},{"cell_type":"markdown","metadata":{"id":"hTExuC4mcoQo"},"source":["# はじめに\n","\n","ここでは、LangChainを主として、ChatGPTなどのLLMモデルを便利に使うための仕組みについて学ぶ\n","\n","なお、ChatGPTを対象とする場合、ChatGPTには、GPTsという独自のChatGPTをNoCodeで作成するツールが準備されている\n","- 従って、以下で説明する内容の多くは、この機能を用いて実現できる\n","- ただし、仕様がすぐに変更されるなど、混乱している状況にあるため、各自で触って試してみるとよい\n","- GPTsの開発状況により状況が大きく変わる可能性がある点に注意する事"]},{"cell_type":"markdown","metadata":{"id":"L9HXMz2Zvn8D"},"source":["# LangChain\n","LangChainは日々更新されている\n","- これは、この授業テキスト全般に言えることであるが、特に後半は更新が頻繁に行われている\n","- アップデートにより実行できない場合もあるが、その場合は速やかに申し出ること\n","\n","その前に、OpenAPIのChat APIについて学ぶ\n","\n","なお、このノートブックは、GPUを使わないCPUランタイムを利用している\n","\n","***注意***\n","\n","ChatGPTは大人気のサービスのため負荷が集中しており、特にAPIの利用が多いため、無償料金枠が2024年時点でなくなっている\n","\n","利用においてもし、次のような文章を含むエラーが出力された場合は、しばらく待って再度実行する必要がある\n","\n","```\n","WARNING:RateLimitError: Rate limit reached for default-gpt-3.5-turbo on requests per min. Limit: 3 / min. Please try again in 20s.\n","```\n","\n","もし、まとめて実行する場合、途中で実行を待ってスロットを使いつくさないようにする必要があるため、次の設定を行うとよい\n","- ***制約がかかった場合は、openai_wait = Trueにすること***\n","- 利用にはまずAPIキーを発行してください"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMatVzE3Xfaq"},"outputs":[],"source":["import time\n","#openai_wait = True\n","openai_wait = False"]},{"cell_type":"markdown","metadata":{"id":"dEvnudgbfOTB"},"source":["また、途中でバージョン不一致によるエラーを回避するため、次のコードを実行後、速やかにセッションを再帰同すること(2024年時点で問題発生)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17462,"status":"ok","timestamp":1736481447875,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"1vnn8kmhfNuN","outputId":"28282349-4fcf-4df9-de3c-3231f9f94605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"]}],"source":["!pip install --upgrade nltk\n","import nltk"]},{"cell_type":"markdown","metadata":{"id":"-dyt3hia9vY1"},"source":["# Chat APIを利用する\n"]},{"cell_type":"markdown","metadata":{"id":"3ojZHInfAtB9"},"source":["## API Keyの発行\n","\n","openai.comにアクセスし、DASHBOARDメニューにあるAPI keysでAPI keyを発行する\n","- セキュリティのため、プロジェクトごとに異なるキーを利用すること\n","- ここでは、dataai-keyという鍵をつくるとよい\n","- 発行された鍵をコピーしておくこと"]},{"cell_type":"markdown","metadata":{"id":"oCgLsK6z_wCo"},"source":["## API Keyを使えるようにする\n","\n","発行したKeyを次のコードにペーストして利用する\n","\n","次のセルにある、```%env OPENAI_API_KEY=``` に続けて、APIキーを記録して実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440,"status":"ok","timestamp":1737504949141,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"r0cuW-PeOD_s","outputId":"23f5ea50-5f5a-4ece-cc09-1163bcb6984e"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: OPENAI_API_KEY=ここにキーを登録\n"]}],"source":["%env OPENAI_API_KEY=ここにキーを登録"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1737504949910,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"OjWTvXKWUTT9","outputId":"d101b56c-4025-4623-aba8-de06ab168581"},"outputs":[{"output_type":"stream","name":"stdout","text":["ここにキーを登録\n"]}],"source":["import os\n","print(os.environ['OPENAI_API_KEY'])"]},{"cell_type":"markdown","metadata":{"id":"pZ8jrQtABH08"},"source":["APIはOpenAI Chat APIに統一された\n","\n","モデルは、コストの安いgpt-3.5-turboを利用する\n","- ここでは性能よりも使い方を学ぶためコスト重視とする\n","- model=\"gpt-4\"などとすることで、GPT-4を利用することができるが、利用料が高くなることに注意すること"]},{"cell_type":"markdown","metadata":{"id":"McSntSfqJWVP"},"source":["## APIを使う\n","\n","まず、ChatGPTに挨拶して、APIが使えているかどうかを確認する\n","\n","\"Hello. Am I using the API correctly?\"\n","\n","と聞いて回答を実際に得る\n","\n","必要なライブラリは、単純に次の2つ\n","- インターネットを利用して、シンプルにRESTでリクエストを投げてレスポンスを得るためのrequests\n","- JSONフォーマットを扱うためのjson"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rr8d5MVyeTIO"},"outputs":[],"source":["import requests\n","import os\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":983,"status":"ok","timestamp":1736481449308,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"bOzgI4KGOIcj","outputId":"33c73995-a890-4fe2-ae85-339b639fcabc"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"id\": \"chatcmpl-Ao0REWPrxzGIxQKDXArFYM1NCiJyn\",\n","  \"object\": \"chat.completion\",\n","  \"created\": 1736481448,\n","  \"model\": \"gpt-3.5-turbo-0125\",\n","  \"choices\": [\n","    {\n","      \"index\": 0,\n","      \"message\": {\n","        \"role\": \"assistant\",\n","        \"content\": \"Hello! I'm an AI assistant and I'm here to help you with any questions you may have about using APIs. Can you provide more details about the API you are using and what specific issues you are facing?\",\n","        \"refusal\": null\n","      },\n","      \"logprobs\": null,\n","      \"finish_reason\": \"stop\"\n","    }\n","  ],\n","  \"usage\": {\n","    \"prompt_tokens\": 16,\n","    \"completion_tokens\": 44,\n","    \"total_tokens\": 60,\n","    \"prompt_tokens_details\": {\n","      \"cached_tokens\": 0,\n","      \"audio_tokens\": 0\n","    },\n","    \"completion_tokens_details\": {\n","      \"reasoning_tokens\": 0,\n","      \"audio_tokens\": 0,\n","      \"accepted_prediction_tokens\": 0,\n","      \"rejected_prediction_tokens\": 0\n","    }\n","  },\n","  \"service_tier\": \"default\",\n","  \"system_fingerprint\": null\n","}\n"]}],"source":["url = \"https://api.openai.com/v1/chat/completions\"\n","headers = {\n","    \"Content-Type\": \"application/json\",\n","    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","}\n","data = {\n","    \"model\": \"gpt-3.5-turbo\",\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"Hello. Am I using the API correctly?\"}\n","    ],\n","    \"temperature\": 0,\n","}\n","\n","response = requests.post(url=url, headers=headers, json=data)\n","print(json.dumps(response.json(), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"hw4z_Ng7KUco"},"source":["返事は\"content\"にあるように、\n","\n","\"Hello! I'm an AI language model and I'm here to help you. Could you please provide more details about the API you are using and what you are trying to achieve?\"\n","\n","となるが、意味としては、\n","\n","「こんにちは！私はAI言語モデルです。使用されているAPIの詳細と、何を達成しようとしているのかを教えていただけますか？」\n","\n","となる\n","\n","なお、パラメータについて、\n","- temperatureは0に近いほど、同じ回答を出力するようになる\n","- max_tokensは返答として最大何文字返すかを指定する\n","  - ChatGPTのAPIは、利用文字数(トークン数)により課金されるため、コスト低減を考えるのであれば重要である  \n","  デフォルトは16であるが、これは通常の利用ではかなり少ないといえる\n","- nは同一質問に対する返答数を指定する  \n","  この場合temparatureを大きめの値にしなければ、同じ回答が並ぶことになる\n","\n","これで、ひとまず使えるようになったであろう"]},{"cell_type":"markdown","metadata":{"id":"vI5oDCjZEcJb"},"source":["### エラートラブル(1)\n","\n","次のように表示される場合は、APIを利用するための無償枠を既に使い切ったか、時間が過ぎたためExpireしたことを意味する\n","\n","```\n","{\n","  \"error\": {\n","    \"message\": \"You exceeded your current quota, please check your plan and billing details.\",\n","    \"type\": \"insufficient_quota\",\n","    \"param\": null,\n","    \"code\": \"insufficient_quota\"\n","  }\n","}\n","```\n","\n","- 無償で続けたい場合は、新しいアカウントをつくるとよい\n","- もちろん、コストを支払ってもよい\n","\n","新しいアカウントをつくる\n","\n","- ただし、メールアドレスが必要となる\n","  - 既にメールアドレスが枯渇している場合は、フリーメールアドレスを取得するとよい\n","\n","- ブラウザのシークレットモードで、openai.comを開く(クッキーで既に持っているアカウント情報を利用しないようにするため)\n","\n","- 右上のメニューからLoginを選択し、Sign upを選択する\n","  - メールアドレスとパスワードを入力する\n","  - 確認メールが届くのでVerifyする\n","\n","- OpenAIのページに行くと、名前や誕生日の入力が求められる\n","\n","- スマートフォンの番号を入れて、コードを受け取る\n","  - 現状では利用済の番号でも問題ないが、将来は不明である\n","\n","- APIをクリックして、OpenAI platformに行き、右上の丸いアイコンをクリックして個人メニューに入る\n","\n","- 左のタブでAPI Keysを選択する\n","\n","- \"Create new secret key\"を選択し、dataai-keyという鍵を作成してコピーしておく\n","\n","以上、あらたに入手した鍵を利用して再実行すること\n","- ただし5ドル分しかないので注意すること"]},{"cell_type":"markdown","metadata":{"id":"12TmNkeaPkMV"},"source":["### エラートラブル(2)\n","\n","頻繁にアクセスすると、エラーが発生する\n","\n","これは、単位時間あたりのアクセス数が制限されているためである\n","- しばらく待ってリトライすること"]},{"cell_type":"markdown","metadata":{"id":"o6grQLKrN9xo"},"source":["### roleについて\n","\n","\"role\"は、次の3つがある\n","- userはChatGPTのユーザで、皆さんのこと\n","- assistantがChatGPTによる回答を指し、ChatGPTのこと\n","- systemはassistantのふるまいを制御するために利用\n","\n","但し、gpt-3.5-turboでは、systemは利用しない\n"]},{"cell_type":"markdown","metadata":{"id":"1LggZY0MRJ7W"},"source":["## API利用において重要なこと"]},{"cell_type":"markdown","metadata":{"id":"hMArjv5yOllm"},"source":["### ブラウザ版との違い\n","\n","API利用と、ブラウザ利用における最も大きな違いは、API利用では過去の会話のやり取りを一切考慮しないという点である\n","\n","したがって、APIを利用する場合で過去の会話を参照したい場合は、過去の会話そのものを全てmessageに記載する必要がある\n"]},{"cell_type":"markdown","metadata":{"id":"xTfjkD7QOrIy"},"source":["### 料金の確認\n","\n","https://openai.com/pricing にアクセスすると、\n","\n","| Model\t| Input\t| Output |\n","|:---|:---|:---|\n","| GPT-3.5-Turbo 4K context | \\$0.0015 / 1K tokens | \\$0.002 / 1K tokens |\n","\n","と記載されている\n","\n","現在いくらつかったかは、\n","\n","https://platform.openai.com/account/usage\n","\n","にアクセスして確認するとよい\n"]},{"cell_type":"markdown","metadata":{"id":"YZ6vL8YXOvFN"},"source":["### トークン数\n","\n","課金対象にもなっているトークン数について、トークンは基本的に単語のことであり、トークン数は入力した単語の数を意味する\n","\n","ChatGPTを含む多くのLMにおいて、膨大な単語を効率よく学習するため、一つの単語を複数のトークンに分割して処理している\n","- 例えば、\"humburger\"は、\"hum\", \"bur\", \"ger\"の3つに分解される\n","\n","また、日本語と英語ではトークン数のカウント方法が異なる\n","- 日本語は内部で英語に変換されて処理されているため日本語は似た内容の文章において課金上不利となる\n","\n","- 日本語は1トークンはおよそ4文字、英語ではおよそ0.75語に相当する\n","  - 一般に直接英語を利用した方がお得といわれている所以\n","  - 実際2倍程度の開きがある\n","\n","このトークン数は、各モデルの入力や出力サイズの制限にも利用される\n","\n","直接文字数を計数したい場合は、https://platform.openai.com/tokenizer にアクセスして、文章を入力するとよい\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yw6W-1k9kQ7T"},"source":["# GPT APIを用いたアプリケーション実装\n","\n","***ここでは、料理名を入力することで、材料と手順、調理時間、お勧めのサイドメニューなどを表示するというアプリを作成することを念頭に説明する***"]},{"cell_type":"markdown","metadata":{"id":"toaMHvttly4S"},"source":["## 全体の構成\n","\n","### APIまでの通信手順\n","\n","以下の手順を踏む\n","- スマートフォンやPC、Webのアプリを利用して、レシピ名をサーバプログラムに送信する\n","- サーバプログラムは、プロンプトエンジニアリングを行い、APIキーを付与してOpenAIのAPIを叩く\n","\n","レシピ生成アプリが直接OpenAI APIを叩くような構成は、APIキーが漏えいするため普通は行わない\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi1.jpg\" width=700>"]},{"cell_type":"markdown","metadata":{"id":"AFbLFE6gqLkD"},"source":["# LangChain"]},{"cell_type":"markdown","metadata":{"id":"XpZ3ySBfqQGu"},"source":["## LangChainとは？\n","\n","LLMを使ったアプリケーション開発フレームワーク\n","- PythonとJavaScript/TypeScriptの2つがある\n","- フリーで利用できる\n","\n","詳細は公式のドキュメントを参照すること\n","- 過去のバージョンのマニュアルを見る場合は、githubにあるlangchainに行き、docsのreleasesから目的のバージョンを選択するとよい"]},{"cell_type":"markdown","metadata":{"id":"gUfbxuHqrE-W"},"source":["## Module\n","\n","LangChainにはmoduleと呼ばれる構成要素がある\n","- moduleとして、Models, Prompts, Chains, Indexes, Memory, Agentsがある"]},{"cell_type":"markdown","metadata":{"id":"Czl2ZbdordE3"},"source":["### Models module\n","\n","LongChainで利用する機械学習モデルである\n","\n","- Chat Models: Open AIのChatAPIのためのモジュール\n","- Text Embedding Models: テキストをベクトル化するモデル\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6sB2-PQgsrMO"},"source":["先に示した簡単な対話プログラムを再構成する\n","- 先ほどよりもシンプルに記述できることがわかるであろう\n","- 内部でjsonに変換され通信が行なわれている"]},{"cell_type":"markdown","metadata":{"id":"QiLIDe5732PJ"},"source":["openaiをインストールする\n","\n","ライブラリの開発速度が速く、バージョン競合が簡単に発生するため注意すること\n","- できれば、自分で解決する能力を身に着けるとよい\n","- (2023/12) openai、cohere、tiktokenは同時に導入しないと警告が表示される\n","- (2023/12) tensorflow-probabilityと一部ライブラリがコンフリクトするため、uninstallする\n","\n","次のようにインストールすることで対処する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrJA882U5Blz"},"outputs":[],"source":["!pip uninstall -y --quiet tensorflow-probability tensorflow-metadata tensorflow-datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13117,"status":"ok","timestamp":1736481469409,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"lWZ181WMBpLM","outputId":"2789e6fa-fbc4-4869-b2a4-03579c45070d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install --quiet openai cohere tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41434,"status":"ok","timestamp":1736481510840,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"LxnKpkivFMKx","outputId":"bfec34cb-5f83-4f11-e85e-3797d4adc198"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.2/606.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install --quiet chromadb kaleido python-multipart"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8IbLgpEGqJB","outputId":"ee0a62cd-42af-481f-b4e9-9caa72cfc396"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install --quiet langchain-community"]},{"cell_type":"markdown","metadata":{"id":"0HX01Spxvna6"},"source":["次のようにpredict関数を用いることで、問い合わせと応答を行うことができる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJHDUhwwgKc6"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","result = llm.invoke(\"自己紹介してください。\")\n","print(result.content)"]},{"cell_type":"markdown","metadata":{"id":"eVvTlTgDtQgX"},"source":["### Prompts module\n","\n","モデルへの入力を組み立てるmoduleであり、次の要素がある\n","- Prompt Templates\n","- Chat Prompt Templates\n","- Example Selectors\n","- Output Parsers\n","\n","ここでは、Prompt Templatesについて説明する\n","- ChatGPTへのプロンプトについてテンプレートつまり例文を作成することができる\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0giJqxftt69A"},"source":["次のコードでは、commandがlsに置き換わる\n","- Promptの長さを考慮して埋め込む\n","- 出力する形式を指定して埋め込む\n","\n","などが可能であり、単純なPythonコードによる埋め込みよりも高度な処理が可能である"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sZIx1e8iwJ8"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","\n","コマンド: {command}\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","\n","result = prompt.format(command=\"ls\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"cI2F8zKJuV-M"},"source":["### Chains module\n","\n","Models, Templates, Chainsなどのmoduleを連結する\n","\n","なお、LangChainの挙動の詳細を確認するため、\n","`langchain.verbose = True`\n","としている\n","- いろいろと意味のない文章や宣伝も表示されるが、不要な場合は、Falseとするとよい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QyYR_Iixyuy"},"outputs":[],"source":["import langchain\n","from langchain.chains import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","\n","langchain.verbose = True\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# Prompt を用意\n","template = \"\"\"\n","次のコマンドの概要を説明してください。\n","\n","コマンド: {command}\n","\"\"\"\n","prompt = PromptTemplate(\n","    input_variables=[\"command\"],\n","    template=template,\n",")\n","\n","# Chain を作成\n","chain = LLMChain(llm=chat, prompt=prompt)\n","\n","# 実行\n","result = chain.invoke(\"ls\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"B-A400ZJu2ws"},"source":["chain.invokeを用いて、構築したchainが順に実行される\n","- 最初に文字の埋め込み(prompt)が行なわれる\n","- 次にchat modelにより実際に通信が行なわれる\n","\n","このChainには各種存在する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfIPskvyXo_3"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"nSMyGRodvnbL"},"source":["#### SimpleSequentialChain\n","\n","ChainとChainを直列に連結する\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DwtbSXduwJO-"},"source":["例えば、次の例を考えてみよう\n","- 簡単な算数の問題を問い合わせ、それに対して回答を得る場合、詳細な手順を聞くようにすると解答の精度が向上する\n","- しかしながら、欲しいのは最終的な答えであって、途中経過は不要であるとする\n","- すると、まず、詳細な手順を含む答えを得てから、その答えを要約して最後の答えだけ得るようにするとよい\n","\n","つまり、ChatGPTを2回利用して最終的に欲しい回答を獲得することになる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BETIBsg104QZ"},"outputs":[],"source":["from langchain.chains import LLMChain\n","from langchain.chains import SimpleSequentialChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","\n","langchain.verbose = True\n","\n","# Model を用意\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# 1 つ目の Prompt と Chain を用意\n","cot_template = \"\"\"\n","以下の質問に回答してください。\n","\n","### 質問 ###\n","{question}\n","### 質問終了 ###\n","\n","ステップバイステップで考えましょう。\n","\"\"\"\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"question\"],\n","    template=cot_template,\n",")\n","cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n","\n","# 2 つ目の Prompt と Chain を用意\n","summarize_template = \"\"\"\n","入力を結論だけ抜き出して記述してください。\n","\n","### 入力 ###\n","{input}\n","### 入力終了 ###\n","\"\"\"\n","summarize_prompt = PromptTemplate(\n","    input_variables=[\"input\"],\n","    template=summarize_template,\n",")\n","summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n","\n","# 2 つの Chain を直列に繋ぐ\n","cot_summarize_chain = SimpleSequentialChain(\n","    chains=[cot_chain, summarize_chain])\n","\n","# 実行\n","result = cot_summarize_chain(\n","    \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\")\n","print(result[\"output\"])"]},{"cell_type":"markdown","metadata":{"id":"V-oQdKENxWo_"},"source":["なお、独自のpromptsやchains moduleを作成することも可能である\n","\n","詳細は、LangChainのマニュアル https://python.langchain.com/docs/get_started/introduction を参照されたい\n"]},{"cell_type":"markdown","metadata":{"id":"CXZ034oXyCvv"},"source":["#### Output Parsers\n","\n","出力形式を指定するプロンプトの作成とPythonオブジェクトとのマッピングを提供する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/chatapi2.jpg\" width=700>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkm0Ys9k4t3h"},"outputs":[],"source":["from langchain.chains import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain.prompts import PromptTemplate\n","from pydantic import BaseModel, Field, validator\n","from typing import List\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","class Recipe(BaseModel):\n","    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n","    steps: List[str] = Field(description=\"steps to make the dish\")\n","    time: List[str] = Field(description=\"time to make the dish\")\n","    sides: List[str] = Field(description=\"side menu of the dish\")\n","#    tools: List[str] = Field(description=\"tools which are required to cook the dish\")\n","\n","template = \"\"\"料理のレシピを教えてください。\n","\n","{format_instructions}\n","\n","料理名: {dish}\n","\"\"\"\n","\n","parser = PydanticOutputParser(pydantic_object=Recipe)\n","\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"dish\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","chain = LLMChain(llm=chat, prompt=prompt)\n","\n","output = chain.run(dish=\"カレー\")\n","#output = chain.run(dish=\"インド本格カレー\")\n","print(\"=== output ===\")\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"YEJA39jxzwdW"},"source":["最後に、recipe型にマッピングする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW2oh6_QzrGk"},"outputs":[],"source":["recipe = parser.parse(output)\n","print(\"=== recipe object ===\")\n","print(recipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkf3FX9080tZ"},"outputs":[],"source":["recipe.ingredients"]},{"cell_type":"markdown","metadata":{"id":"prFUg4G3QfbA"},"source":["### Indexes"]},{"cell_type":"markdown","metadata":{"id":"p3Gp3u-hJHR_"},"source":["ChatGPTは学習に用いたデータセットの範疇でのみ答えを出すため、新しい知識や概念については、正しく解答することが難しい\n","\n","例えば、次のような質問に対しては、お手上げになっている\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain1.jpg\" width=500>\n","\n","ここで、質問に対する回答を得るうえで必要となる情報を渡してから処理させると、おおよそ正しい回答を得ることができるようになる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain2.jpg\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"R59skBRVYMJp"},"source":["コンテキストとして、情報を加えることで正しい回答を与えることができており、これは強力な方法であるが、実際に用いる場合は注意が必要である\n","\n","- ChatGPTには、入力文字数に制限があるため、長い文章を入力する必要がある場合はともかく、様々な情報を大量に与えておいて、そこから適切な回答を得るという利用は困難である\n","- 文字数、つまり入力トークン数も課金に関係するため、高コストとなる\n","\n","まず、全ての情報を与えておいて、何かしら回答を得るということは非現実的であるが、自動化という点では有効な手段である\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ob50Tp4IZaSW"},"source":["#### Vector Store\n","\n","そこで、Vector Storeを活用する\n","- 文章をベクトル化してVector Storeに保存、入力と近しいベクトルの文章をVector Storeから検索してcontextに含める手法\n","- 全文章ではなく、「大事と思われる部分文章群についてのみ」情報を与える\n","  - これには、、embeddingと呼ばれる内部ベクトル表現への変換を行い、そのベクトルの近接性を用いて、どの文章が重要かを判断している\n","  - 単語をベクトル化するトークンではない点に注意すること\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/langchain3.jpg\" width=700>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PVg3gQRLcb2X"},"source":["実際にindexを扱う\n","\n","ここでは、LangChainのドキュメントを参考にして、質問できるようにする\n","\n","最新のドキュメントはgitで公開されているため、LangChainのgitをcloneする"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8VY_0R_VGcHv","outputId":"3523541d-5eeb-4c78-b1ad-be33046c0db4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'langchain'...\n","remote: Enumerating objects: 224717, done.\u001b[K\n","remote: Counting objects: 100% (455/455), done.\u001b[K\n","remote: Compressing objects: 100% (325/325), done.\u001b[K\n"]}],"source":["!git clone https://github.com/hwchase17/langchain.git"]},{"cell_type":"markdown","metadata":{"id":"UdmH-q4VcpFn"},"source":["langchainのディレクトリに入る"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhcQzqUqR1t3"},"outputs":[],"source":["!cd langchain"]},{"cell_type":"markdown","metadata":{"id":"gK1vJgKmgpG1"},"source":["以降の動作確認で必要となるライブラリを導入する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znXiN1W1Rd8Q"},"outputs":[],"source":["!pip install --quiet unstructured tabulate pdf2image pytesseract chromadb tiktoken"]},{"cell_type":"markdown","metadata":{"id":"waRfWzSLhL_p"},"source":["まず、ディレクトリの中の全体を読むための便利なDirectoryLoaderを用いて、ある場所にあるファイルをサブディレクトリもまとめて取得する\n","- ここでは拡張子がmdであるファイルに限定している\n","\n","- そこから次々に文章を取得して、VectorstoreIndexCreatorで、Vectorsoreに格納していく\n","  - この時、embeddingと呼ばれる内部ベクトル表現への変換も同時に行っている"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INKjEoCQXbft"},"outputs":[],"source":["!pip install --quiet nltk click joblib regex tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q10Ff7vNQuV1"},"outputs":[],"source":["from langchain.document_loaders import DirectoryLoader\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain.embeddings import OpenAIEmbeddings\n","nltk.download('punkt')\n","loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.mdx\")\n","embeddings = OpenAIEmbeddings()\n","index = VectorstoreIndexCreator(embedding=embeddings).from_loaders([loader])"]},{"cell_type":"markdown","metadata":{"id":"2-k97WTohvnK"},"source":["では、そのvector storeを利用して、質問する\n","\n","なお、現時点でChatGPTに\"LangChain\"について問い合わせると、データセットに含まれていないという回答になる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJbppn-lRblc"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","result = index.query(\"LangChainにおけるIndex moduleについて概要を1文で説明してください。\", llm=chat)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"jFRjpFYqiJh3"},"source":["`langchain.verbose = True`のため、無意味な動作確認メッセージも表示されているが、要約文が出力されていることがわかる\n","- なお、若干回答は的を得ていない\n"]},{"cell_type":"markdown","metadata":{"id":"8PBuUnC4jU0_"},"source":["DocumentLoadersは、webサイト、GoogleDrive、Slackなどを読み込む機能が存在しているため、様々な情報をVctor Storeに格納することができる"]},{"cell_type":"markdown","metadata":{"id":"S10hxRg_kURG"},"source":["このように、ある特定分野のデータを一連の要素としてエンコードし、それぞれが内部で 1 つの「ベクトル」として表現している\n","- これには、単語であればWord2Vecなどが利用できるが、ここでは文章であることから、BERTをFine-TuningしたSentenceTransformerの利用が検討される\n","- このベクトル数値は、多次元ベクトル空間で要素を相互に関連づけてマッピングしている\n","\n","ベクトル要素がセマンティックであり、ある一つの意味を表していると考えるならば、そのベクトルの近接性が文脈関係の指標となりえる\n","- このベクトルはエンベディングと呼ばれる\n","- 互いに関連性のある意味要素がまとまって配置されるようにエンベディングを行う\n","\n","特定分野の文脈によって、セマンティック要素は単語、フレーズ、センテンス、パラグラフ、文書全体、画像、あるいはまったく別のものになる可能性があり、エンベディングが最善というわけではない\n","\n","\n","\n","プロンプトで必要となる文脈を生成するため、データベースに問い合わせを行い、ベクトル空間の入力と密接に関連する要素を抽出する必要がある\n","\n","ベクトルデータストアは、大量のベクトルを保存し、問い合わせに答えるシステム\n","- 効率的な最近傍クエリアルゴリズム(k-NNなど)と適切なインデックスにより、データ検索を行う"]},{"cell_type":"markdown","metadata":{"id":"ARj8Y8_RdlD6"},"source":["### Memory"]},{"cell_type":"markdown","metadata":{"id":"B0EPjzsTQntU"},"source":["ChatGPTをブラウザで利用した場合、過去の会話の履歴を踏まえて返答するが、APIではそのような振る舞いは行わない\n","\n","APIを利用する場合は、過去の履歴をプロンプトに入力する必要がある\n","\n","実際にその振る舞いを確認する"]},{"cell_type":"markdown","metadata":{"id":"YzABPdjkRm7e"},"source":["次のような関数を用意してAPIを用いて文章を渡す"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7pxRwGnUDvn"},"outputs":[],"source":["def post_chat_completions(content):\n","  url = \"https://api.openai.com/v1/chat/completions\"\n","  headers = {\n","      \"Content-Type\": \"application/json\",\n","      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n","  }\n","  data = {\n","      \"model\": \"gpt-3.5-turbo\",\n","      \"messages\": [\n","          {\"role\": \"user\", \"content\": content}\n","      ],\n","      \"temperature\": 0,\n","  }\n","\n","  response = requests.post(url=url, headers=headers, json=data)\n","  print(json.dumps(response.json(), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"2tIxorsKRxtu"},"source":["では、実際に名前を伝える"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_OBzUQGVAwa"},"outputs":[],"source":["post_chat_completions(\"Hi! I'm Keio Yukichi!\")"]},{"cell_type":"markdown","metadata":{"id":"3-ruccLKR-Zi"},"source":["\"Hello Keio Yukichi! How can I assist you today?\" といった回答が得られているであろう\n","- Generativeであるため、この答えは毎回異なる\n","\n","その上で、名前を憶えているか聞いてみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYjoBKp8VAjZ"},"outputs":[],"source":["post_chat_completions(\"Do you know my name?\")"]},{"cell_type":"markdown","metadata":{"id":"RpXJqn09SJS0"},"source":["当然であるが、知らないという答えになる\n","\n","そこで、過去の会話の履歴を全て入れて、同じ質問を行ってみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ctwcsZ-VLdD"},"outputs":[],"source":["post_chat_completions(\"\"\"A: Hi! I'm Keio Yukichi!\n","B: Hello Keio Yukichi! How can I assist you today?\n","A: Do you know my name?\n","B: \"\"\")"]},{"cell_type":"markdown","metadata":{"id":"PuqgDlE5SipG"},"source":["となり、今度は\"Yes, you introduced yourself as Keio Yukichi.\"と回答している\n","\n","A:やB:は、会話しているのがどちらかを示す識別子であり、どのような形でもよい\n","- ただしLangChainは、内部でhumanとAIという用語を利用している\n","\n","このように、会話の過去の履歴を含めて問い合わせを行うため、過去の履歴を記録し、挿入するという処理が必要となる\n","- これがMemory moduleの役割である"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZHfxu9hY62e"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"Bu3uKzYrYo8a"},"source":["実際に使うには、Chainの中にmemoryとしてConversationBufferMemory()を加えるだけでよい\n","\n","プロンプト(文字を入力するための入力窓)が出てきたら、\n","- Hello. I'm Keio Yukichi\n","- Do you know my name?\n","- EOC\n","と入力する\n","\n","EOCは会話を終了させるおまじないである\n","- これは、ChatGPTの機能ではなく、そのようにプログラムしている"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LjkhKO7nVeEK","outputId":"952d0d7a-3366-4faa-9c61-18a33e774027"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-30-5dafd31b1180>:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory=ConversationBufferMemory()\n","<ipython-input-30-5dafd31b1180>:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n","  conversation = ConversationChain(\n"]}],"source":["from langchain.chains import ConversationChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","conversation = ConversationChain(\n","    llm=chat,\n","    memory=ConversationBufferMemory()\n",")\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    ai_message = conversation.predict(input=user_message)\n","    print(f\"AI: {ai_message}\")"]},{"cell_type":"markdown","metadata":{"id":"sva3QIpBVImC"},"source":["roleには、system, assistant, userの3つがあることは述べた\n","- systemは設定に利用され、例えばChatGPTにキャラを与えるような命令も存在する\n","\n","assistantとuserについて、assistant はAIからの回答、 user はユーザーからの発話であり、基本的にuserとして聞きたいことや、会話履歴を含めてを送ることになる\n","- この例では、userに、ユーザとChatGPTの両方の会話を入れ込んでいる\n","\n","しかしながら、本来は、assistantがAIからの回答であることから、userにはユーザからの発話履歴のみ、assistantはAIからの発話履歴のみを入れるという形が望ましい\n","- これについては後で説明する\n","\n","その他、様々なMemory moduleが提供されている\n","- ConversationBufferWindowMemory\n","  - ある範囲の会話履歴のみ入力する\n","- ConversationSummaryMemory\n","  - 会話履歴の要約を入力する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JPsQbr0ZJ1z"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"C0xTWbzzV4_n"},"source":["### Agents"]},{"cell_type":"markdown","metadata":{"id":"xnb4KsklUOeo"},"source":["LLMが必要に応じて様々なタスクを実行すると便利と思うであろう\n","\n","例えば、\n","- 検索エンジンで検索させる\n","- 実際にコマンドを実行させる\n","- プログラミング言語やスクリプト言語でコードを実行させる\n","\n","これを行うのがAgents moduleである\n","\n","Agentsの利用により、実際にはLLMが何かを操作するわけではないが、LLMが何かしらアプリを操作しているかのように動作させることができる\n","\n","Agentsで操作可能なアプリの例\n","- bash (シェル)\n","- Google Search\n","- IFTTT WebHooks (スマートホーム等)\n","- Python REPL\n","- Requests (他のAPIを叩く)\n","- Wikipedia API"]},{"cell_type":"markdown","metadata":{"id":"qmfqEmF_VsdA"},"source":["実際にAgentsを利用してみよう\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGJkjbiBfq-2"},"outputs":[],"source":["!pip install --quiet langchain-experimental"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbSTOTuKX7kJ"},"outputs":[],"source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.chat_models import ChatOpenAI\n","\n","langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","tools = load_tools([\"terminal\"], llm=chat, allow_dangerous_tools=True)\n","agent_chain = initialize_agent(\n","    tools, chat, agent=\"zero-shot-react-description\")\n","\n","result = agent_chain.run(\"What is your current directory?\")\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbICnbceJ1nO"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"NCkT4uK8fHWE"},"source":["単純に/content というディレクトリにいるという回答であるが、実際正解である\n","\n","ただ、ここで疑問が生じる **なぜ、ChatGPTはこちらのシェル環境のカレントディレクトリがわかったのだろうか？**\n","\n","これは、AgentsがMRKL(ミラクル)やReActなどを利用して動作しているためである\n","- MRKL(Multi-Round Knowledge Loop)\n","- ReAct (Reasoning/Acting) なお、Reactではない\n"]},{"cell_type":"markdown","metadata":{"id":"-3eUMCemaz3X"},"source":["ログをみると、LangChainは次のようなプロンプトを生成している\n","```\n","Answer the following questions as best you can. You have access to the following tools:\n","\n","terminal: Run shell commands on this Linux machine.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [terminal]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","```\n","\n","翻訳すると、\n","```\n","次の質問にできるだけ答えてください。あなたは以下のツールにアクセスできる：\n","\n","terminal: このLinuxマシンでシェルコマンドを実行する。\n","\n","以下の書式を使う：\n","\n","Question: あなたが答えなければならない入力問題\n","Thought: 何をすべきかを常に考える。\n","Action: 取るべき行動。[terminal]のどれかであるべき。\n","Action Input: アクションへの入力\n","Observation: 行動の結果\n","...（このThought/Action/Action Input/ObservationはN回繰り返すことができる）\n","Thought: 最終的な答えがわかった\n","Final Answer: 元の入力された質問に対する最終的な答え\n","\n","始める！\n","```\n","\n","となっており、これらの書式を用いて処理が進む\n","```\n","Question: What is your current directory?\n","```\n","という問いかけに対して、\n","```\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","```\n","とChatGPTが返答する\n","\n","そこで、AgentはAction Inputに記載されているコマンドを実行する\n","- その結果を Observationとして埋め込む\n","\n","さらに質問を続けるが、先のプロンプトに加えて、次の文章が加わっている\n","- つまり、これまでの動作をプロンプトに入力している\n","\n","```\n","Question: What is your current directory?\n","Thought:I can use the \"pwd\" command to find out the current directory.\n","Action: terminal\n","Action Input: pwd\n","Observation: /content\n","\n","Thought:\n","```\n","これに対して\n","```\n","I now know the final answer\n","Final Answer: The current directory is /content.\n","```\n","と回答している\n","\n","Final Answerとして現在のディレクトリは/contentであることが示されており、Final Answerが返されたので、実行を終了している"]},{"cell_type":"markdown","metadata":{"id":"uJ0Z4_VyZfbU"},"source":["では、どんどんやってみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sX2ZcN2ZjT1"},"outputs":[],"source":["result = agent_chain.run(\"Make a new directory called 'testdir-by-agent'\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"hOQ0y-iOZ3W7"},"source":["ファイルも作ってみよう\n","\n","なお、途中で無料枠の場合はスロットを使い切ってしまうので、ワーニングメッセージと待ちが発生する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16ILSPFfZ486"},"outputs":[],"source":["result = agent_chain.run(\"Create new file called test.txt in the directory of testdir-by-agent and store the text of This is test in the file.\")\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVBNXV0IKnbe"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"M_GU_jxdadC8"},"source":["testdir-by-agentの下にtest.txtがあり、その中身がThis is a testであることを確認しよう"]},{"cell_type":"markdown","metadata":{"id":"cQzbRtM5hCkG"},"source":["### MRKL(Multi-Round Knowledge Loop)\n","\n","例えば、「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」という問いに対して、ChatGPTは答えることができるか？\n","\n","これを直接WebのChatGPTに問い合わせても答えることができない\n","\n","しかしながら、APIでは答えることができる\n","- つまり、APIでChatGPTをアクセスすると、Webとは異なる仕組みでアクセスできるということ\n","\n","では、その手順であるが、まず質問に答えるための手順を考える\n","\n","- 最初に問い。「現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？」（Question）\n","\n","- Googleで未知の情報を調べるために検索ワードを考える(Thought)\n","\n","- 現在の日本とフランスの首相の年齢を知る必要がある\n","  - 検索ワードは「現在の日本の首相の年齢」(Action Input)\n","  - さらに検索ワードは「現在のフランス首相の年齢」(Action Input)\n","\n","- 検索を実行「現在の日本の首相の年齢」（Action）\n","  - 結果は65歳でした（obsabation)\n","\n","- 検索を実行します「現在のフランス首相の年齢」（Action）\n","  - 結果は61歳でした（obsabation)\n","\n","- 日本の首相の年齢とフランスの首相の年齢の差分を計算する必要がある（Thought）\n","\n","- 計算（Action）\n","\n","- 結果は4でした(obsabation)\n","\n","- 答えは4歳です (final)\n","\n","このようにMRKLは、ChatGPTが情報をもとに次のアクションを考え、結果を評価し、次のアクションを考えるというプロセスを繰り返すことで回答精度を上げる方法論である\n","\n","考察（Thought）、観察（Observation）、行動（Action）のサイクルを繰り返すことで、回答精度が向上する\n","\n","よく言われる、ステップバイステップで考えるように指示すると正答率が上がるのと似ているが、Agent側で実際に実行して応答できるように工夫されている"]},{"cell_type":"markdown","metadata":{"id":"ya0rvvL2jaAT"},"source":["### Prompt Coding\n","\n","プロンプトコーディングはChatGPTの活用において必須となる技術である\n","\n","ChatGPTから精度の高い回答を得るために、人間に質問するのと同様に、質問力が重要であり、その質問の仕方に関する研究が進められている\n","\n","例えば、以下のように役割の指定や回答の形式を細かく設定することで、正答率を上げることができる\n","- 質問や回答が定型化されており、プログラムで文字列を処理することが容易になり、解析が可能となる\n","\n","```\n","あなたは、英語の先生です。これから私の英語を英語教師として文法の誤りを訂正して下ださい。\n","回答のフォーマットは以下のようにします。\n","あなたの英語：{入力分}\n","訂正後の英文:{英文例}\n","文法の解説:{解説1000文字以内}\n","```"]},{"cell_type":"markdown","metadata":{"id":"3u5jaB1ikP5o"},"source":["### 実際のプロンプト\n","\n","先の年齢差を問う問題に答えさせる場合、次のようなプロンプトが想定される\n","- 内容は、シェルを実行するAgentの問い合わせと酷似する\n","\n","```\n","Answer the following questions as best you can.\n","You have access to the following tools:\\n\\n\n","\n","Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n\n","Calculator: Useful for when you need to answer questions about math.\\n\\n\n","\n","Use the following format:\\n\\n\n","\n","Question: the input question you must answer\\n\n","Thought: you should always think about what to do\\n\n","Action: the action to take, should be one of [Search, Calculator]\\n\n","Action Input: the input to the action\\n\n","Observation: the result of the action\\n\n","... (this Thought/Action/Action Input/Observation can repeat N times)\\n\n","Thought: I now know the final answer\\n\n","Final Answer: the final answer to the original input question\\n\\n\n","\n","Begin!\\n\\n\n","\n","\n","Question: 現在の日本の首相の年齢から現在のフランスの首相の年齢を引いたらいくつですか？ 計算してください\\nThought:')\n","```\n","\n","最初の\"Use the following format:\\n\\n\" 以前について、\n","\n","ここで、toolsについて何がどのように利用できるかを伝えているが、重要な点は次の通りである\n","\n","- Searchというツール名\n","  - コロンの前にツール名が記載されている\n","- ユースケースを伝える\n","  - (時事問題に関する質問に答える必要があるときに便利です)\n","- 入力形式を指定する\n","  - (入力は、検索クエリである必要がある)\n","\n","「入力は検索クエリである必要がある」と伝えているため、半角スペース区切りの単語単位での検索クエリを作成するようになる\n","- ChatGPTは時事問題に関する内容は、Searchツールを使うようになる\n","\n","最初の\"Use the following format:\\n\\n\" 以降について\n","\n","進め方とフォーマットを伝えている\n","\n","- Question:質問内容を記載\n","- Thought:何をすべきかを常に考える必要がある\n","  - アクションを考えるように指示\n","- Action: 実行するアクションは、 [Search, Calculator]のいずれかである必要がある\n","  - アクション名はツール名と同じであり、ChatGPTからActionの指示が出る際には[Search,Calculator]のキーワードが出力される\n","- Action Input: アクションへの入力\n","  - Searchの場合は指示されたクエリ形式で入力する\n","- Observation:Actionの結果\n","  - アクションの結果を表示\n","\n","さらに、最後について\n","\n","- (this Thought/Action/Action Input/Observation can repeat N times)\n","  - N回繰り返すは、答えが出ない場合に打ち切る回数や、API利用料金を抑えるための制限回数として、Nを指定できるようにしている\n","- Thought:回答が判明したら下記に進みます\n","- Final Answer:最終的な回答をします"]},{"cell_type":"markdown","metadata":{"id":"3BoivuKFnmUn"},"source":["実際に試行すると次のような結果を得ることができる\n","\n","```\n","> Entering new AgentExecutor chain...\n","I need to find out the age of the current Japanese and French Prime Ministers\n","Action: Search\n","Action Input: \"age of current Japanese Prime Minister\"params\n","\n","Observation: 65歳\n","Thought: Now I need to find out the age of the current French Prime Minister\n","Action: Search\n","Action Input: \"age of current French Prime Minister\"params\n","\n","Observation: 61歳\n","Thought: I now know the final answer\n","Final Answer: 4歳\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bqrKxwnhlstV"},"source":["##  Chat API におけるプロンプトの構築"]},{"cell_type":"markdown","metadata":{"id":"XNIICFODuMO_"},"source":["先のmemoryの例に加えて、\n","`import openai`\n","\n","および\n","\n","`langchain.verbose = True`\n","\n","を追加して、ログを詳細に取得する\n","\n","プロンプトに対して、\n","- Hi, I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC\n","\n","と入力する\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jX1cLM9FJfbe"},"outputs":[],"source":["from langchain.chains import ConversationChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","import openai\n","\n","langchain.verbose = True\n","openai.log = \"debug\"\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","conversation = ConversationChain(\n","    llm=chat,\n","    memory=ConversationBufferMemory()\n",")\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    ai_message = conversation.predict(input=user_message)\n","    print(f\"AI: {ai_message}\")"]},{"cell_type":"markdown","metadata":{"id":"Ke23qQpWvIZM"},"source":["ログを参照することで、動作の詳細を獲得できる\n","\n","例えば、Memoryにより過去の履歴をプロンプトを与えることができるが、具体的には次のような動作をしている\n","\n","```\n","api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\\\n\\\\nCurrent conversation:\\\\nHuman: Hi, I\\'m Keio Yukichi.\\\\nAI: Hello Keio Yukichi! How can I assist you today?\\\\nHuman: Do you know my name?\\\\nAI:\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n","```\n","\n","このように、すべてuserメッセージとして混入している\n","\n","本来は、ChatGPTの言葉は、assistantとして入力するべきであろうことがわかる"]},{"cell_type":"markdown","metadata":{"id":"Nv6G19MewSJN"},"source":["そこで、これを使い分けるには、次のようにする\n","\n","SystemMessage, HumanMessage、またAIMessageを用いて、それぞれの会話を仕分けできる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X90oA5kIPGYG"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import (\n","    AIMessage,\n","    HumanMessage,\n","    SystemMessage\n",")\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n","\n","messages = [\n","    SystemMessage(content=\"You are a helpful assistant.\"),\n","    HumanMessage(content=\"Hi! I'm Keio Yukichi!\"),\n","    AIMessage(content=\"Yes, You are Keio Yukichi.\")\n","]\n","\n","result = chat(messages)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wij-UYXMLZzW"},"outputs":[],"source":["#スロット待ち\n","if openai_wait:\n","  time.sleep(60)"]},{"cell_type":"markdown","metadata":{"id":"GamQYH-iwucs"},"source":["これを踏まえて、先ほどのループ問い合わせプログラムを改善する\n","\n","```\n","    memory.chat_memory.add_user_message(user_message)\n","    ai_message = chat(memory.chat_memory.messages)\n","    memory.chat_memory.add_ai_message(ai_message.content)\n","```\n","とすることで、memoryに対してだれの発言かを仕分けして登録するようにする\n","\n","実際に実行して、次のようにプロンプトに入力する\n","- Hi. I'm Keio Yukichi.\n","- Do you know my name?\n","- EOC\n","\n","ログを見てみると、Do you know my name? の問い合わせの後、\n","\n","`\"messages\": [{\"role\": \"user\", \"content\": \"Hi. I\\'m Keio Yukichi.\"}`とuserが入力した後、`{\"role\": \"assistant\", \"content\": \"Hello Keio Yukichi! How can I assist you today?\"}'とassistantが返答、さらに`{\"role\": \"user\", \"content\": \"Do you know my name?\"}`とuserが入力といった具合に、正しく仕分けされている\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZAE9dogmna_"},"outputs":[],"source":["langchain.verbose = True\n","\n","chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","memory = ConversationBufferMemory()\n","\n","while True:\n","    user_message = input(\"You: \")\n","    if(user_message == 'EOC'):\n","      break\n","    memory.chat_memory.add_user_message(user_message)\n","    ai_message = chat(memory.chat_memory.messages)\n","    memory.chat_memory.add_ai_message(ai_message.content)\n","    print(f\"AI: {ai_message.content}\")\n"]},{"cell_type":"markdown","metadata":{"id":"CApGZbWFGkaZ"},"source":["ここで一度実行を終了する\n","- 以降は「ランタイム」から「以降のセルを実行する」を選択して実行するとよい\n","- もしくは一つ一つクリックして実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j0TQZvcMZU5"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"markdown","metadata":{"id":"8OnF6qsbDG1t"},"source":["# Embedchain\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vqTZatQcJef3"},"source":["Embedchainは、どのようなデータセットでも簡単にLLMボットを作成できるフレームワークである\n","\n","- CSVやExcel、テキストファイルに加えて、YouTube動画など、様々なフォーマットを扱うことができる\n","- バックエンドで、ChatGPT API、OpenAIのembeddingとLangChain、ベクトルデータベースにChromaを利用している\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-VQE9nGwDaiG"},"source":["## ライブラリのインストール"]},{"cell_type":"markdown","metadata":{"id":"kOk3SJEGu1iF"},"source":["次のセルでlangchainをインストールするが、dependancyでエラーが生成されるかもしれない\n","- (2023/12) embedchainの都合により、バージョンを指定して導入する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVxZFGSagE1C"},"outputs":[],"source":["!pip install --quiet langchain==0.0.336"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FM6ubk9KD34G"},"outputs":[],"source":["!pip uninstall -y --quiet tensorflow-probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXf72d02D34G"},"outputs":[],"source":["!pip install --quiet openai cohere tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIS0VkUXD34H"},"outputs":[],"source":["!pip install --quiet chromadb kaleido python-multipart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iuoCtmFMbFn"},"outputs":[],"source":["!pip install --quiet --upgrade embedchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpuq7ieeMPAH"},"outputs":[],"source":["!pip install --quiet pydantic"]},{"cell_type":"markdown","metadata":{"id":"DrrlU4QqDmxl"},"source":["必要なライブラリをインポートする"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohPMkz4zDqxz"},"outputs":[],"source":["import os\n","from embedchain import App"]},{"cell_type":"markdown","metadata":{"id":"JAO_SzlyDr5x"},"source":["APIキーは、先に指定したキーを利用する点に注意する\n","\n","次のようにして、直接キーを指定してもよい\n","\n","```\n","os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n","\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9lhoy9vLiQm"},"outputs":[],"source":["!pip install python-dotenv"]},{"cell_type":"markdown","metadata":{"id":"YrhIHw2LHAtU"},"source":["次のコードでTrueと出ればよいが、Falseと表示された場合は、戻って最初の方にある\"!echo ...\"のセルをもう一度実行するとよい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyRo07nqD08l"},"outputs":[],"source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"k-SllFyqNIB1"},"source":["embechainボットを起動する\n","- ここでは、ChatGPTを起動している\n","- Llama2App()とすると、Llama2が起動する\n","- その他、CustomAppなどを用いることで様々なLLMモデルに対応する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6NHtXp6NCsR"},"outputs":[],"source":["elon_bot = App()"]},{"cell_type":"markdown","metadata":{"id":"WRKWFWSGD1tB"},"source":["次に、embedchainの `.add()` メソッドを使って異なる種類のデータソースを追加する\n","\n","- `\"pdf_file\"`でPDFファイルをURLなどで追加できる\n","- その他の対応フォーマットについては、https://docs.embedchain.ai/advanced/data_types を参照のこと\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hlPK-AlIGnA"},"outputs":[],"source":["!pip install --quiet youtube-transcript-api\n","!pip install --quiet pytube"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhByl8oxEJFH"},"outputs":[],"source":["elon_bot.add(\"web_page\", \"https://en.wikipedia.org/wiki/Elon_Musk\")\n","elon_bot.add(\"youtube_video\", \"https://www.youtube.com/watch?v=kwsTrVoCfSQ\")"]},{"cell_type":"markdown","metadata":{"id":"aeTOQettEfWT"},"source":["ボットの準備ができたので、`.query()`メソッドを使ってボットに質問する\n","\n","なお、ここでは、queryを用いているが、2つのインタフェースがある\n","\n","- クエリー・インターフェース\n","  - このインターフェイスは質問に答えるボットであり、質問を受け、その答えを取得する\n","  - 過去のチャットに関するコンテキストは保持しない\n","  - `.query()`関数を呼び出して、任意のクエリの答えを取得できる\n","\n","- チャット・インターフェース\n","  - 過去の会話を記憶するチャット・インターフェイスであり、デフォルトでは過去5つの会話を記憶している\n","  - `.chat`関数を呼び出して問い合わせの答えを取得できる\n","\n","- ドライ・ラン\n","  - 追加、クエリー、チャットの各メソッドにあるオプションで、LLMに送信せず、生成されたプロンプトを表示することができまる\n","\n","\n","```\n","chatmsg = naval_chat_bot.query('Can you xxxxxx?', dry_run=True)\n","```\n","\n","- ストリーム応答\n","ChatGPT のようにレスポンスをストリームするために、クエリメソッドに追加するオプション\n","- チャンクを希望のフォーマットでレンダリングするには、ダウンストリームハンドラが必要となる\n","- OpenAIモデルとOpenSourceAppの両方をサポートする\n","\n","```\n","app = App()\n","query_config = QueryConfig(stream = True)\n","resp = app.query(\"What is ****** ?\", query_config)\n","\n","```\n","\n","- リセット\n","  - `reset`でデータベースをリセットし、すべての埋め込みを不可逆に削除する\n","\n","- カウント\n","  - `count`でデータベース内の埋め込み（チャンク）の数を数えます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZnx0V18EkzS"},"outputs":[],"source":["elon_bot.query(\"How many companies does Elon Musk run?\")"]},{"cell_type":"markdown","metadata":{"id":"vy5mm7Trmjh4"},"source":["# Gradioについて"]},{"cell_type":"markdown","metadata":{"id":"SY5aPDLRFzfE"},"source":["***ここからは、まとめて実行せず、つ一つ一つクリックして実行した方が良い***\n"]},{"cell_type":"markdown","metadata":{"id":"LHpGhwMab4H1"},"source":["Webアプリを簡単に実装できるPythonライブラリ\n","\n","百聞は一見に如かずということで、早速実行してみよう"]},{"cell_type":"markdown","metadata":{"id":"jhssf_w9moNv"},"source":["## gradio のインストール方法\n","\n","```\n","pip install gradio\n","```\n","\n","とし、例えば、\n","\n","```\n","import gradio as gr\n","```\n","とすることで利用可能となる\n","- (2023/12) ここでは互換性のために3.48.0を導入する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWx4JDa-MGDa"},"outputs":[],"source":["!pip -q install gradio==3.48.0"]},{"cell_type":"markdown","metadata":{"id":"miQy3qBhcQNz"},"source":["シンプルなWeb UIを作成して起動させる例を示す\n","- 名前を入力し、名前へのあいさつを出力するWeb UIである\n","\n","Colab上で実行すると、Colabのwebに統合される\n","- URLが出力されている通り、そのURLにアクセスできる環境にあれば、webのページとして表示される\n","- この例では、`https://localhost:7860/`などと表示されているが、クリックすると実は`https://wq0lbccui9-496ff2e9c6d22116-7860-colab.googleusercontent.com/`に転送されており、ネットワークセキュリティ上隔離されたColabの外からアクセスできるようになる\n","- Colab環境では、セキュリティ上の問題もあり、このようなグローバルアドレスが提供されない場合は、別途ボートフォーワーディングなどの知識が必要な場合がある\n","- また、ローカル上で他のWeb UIアプリを動作させているなどにより、ポートが競合する可能性がある\n","  - この場合、7860 が 7861 や 7862 といった番号に変わることがある\n","\n","\n","また、gradio clientを用いることで、作成したwebアプリケーションをweb APIのように利用することもできる\n","- 下に小さく「Use via API」と記載されているが、これをクリックし、記載の通りに実行すると動作がわかるであろう\n","·"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8KX4wgicW2Z"},"outputs":[],"source":["import gradio as gr\n","\n","# あいさつの関数\n","def greet(name):\n","    return \"Hello \" + name + \"!\"\n","\n","# Interfaceの作成\n","demo = gr.Interface(\n","    fn=greet,\n","    inputs=\"text\",\n","    outputs=\"text\"\n",")\n","\n","# 起動\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"lslskTPQflov"},"source":["## 設計手順\n","\n","次の手順となる\n","- コールバック関数を定義\n","- レイアウトを定義\n","- WebUIの起動\n","\n","それぞれについて概要をみてみよう"]},{"cell_type":"markdown","metadata":{"id":"ZKnVtHYXgNiu"},"source":["\n","### コールバック関数\n","\n","まず、コールバック関数として、画面レイアウトでボタンが押された時に呼び出したい関数を記述する  \n","\n","例えば、名前(text)、表示フラグ(boolean)、値(0から100の値)の3つを受け取る関数として次のような関数を想定する  \n","```\n","def my_func(my_name, is_disp, my_value):\n","    return f'### {my_name} ###', my_value * 100\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2PxgV9NKnkjK"},"source":["### Interface\n","\n","「Interface」は、関数をUIでラップするためのクラスであり、主なパラメータは次のとおり\n","- fn : ラップする関数\n","- inputs : 入力コンポーネント (\"text\"、\"image\"、\"audio\"など)\n","- outputs : 出力コンポーネント (\"text\"、\"image\"、\"label\"など)\n","\n","画面レイアウトの作成として、画面に表示したいUIパーツと、ボタンなどが押された時のアクション(呼ぶ出したいコールバック関数の名前)を記述する  \n","- 入力UIとして、my_nameは\"text\"であり、is_dispは\"checkbox\"であり、my_valueは、0から100の値をスライダーで入力させるとすると、gr.Slider(0, 100)となり、上からこの順に入力UIがレイアウトされる\n","- 出力UIとして、ここでは、文字と数字を想定する\n","\n","最終的に、次のような関数定義となる  \n","```\n","demo = gr.Interface(\n","    fn = my_func,\n","    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n","    outputs=[\"text\", \"number\"],\n",")\n","```\n","\n","なお、\"text\"に対して、より詳細な情報を与えることもできる\n","\n","```\n","demo = gr.Interface(\n","    fn=my_func,\n","    inputs=[\n","      gr.Textbox(\n","        lines=2,  # 行数\n","        placeholder=\"Name Here...\"  # プレースホルダ\n","      ),\n","      \"checkbox\", gr.Slider(0, 100)\n","    ]\n","    outputs=[\"text\", \"number\"],\n",")\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"XjaSRRUCgmsO"},"source":["\n","- 最後にWebUIを起動する  \n","シンプルに、  \n","```\n","demo.launch()\n","```\n","とするだけでよい\n","\n","inputs には「クリアボタン」と「送信」ボタンが、outputs には「フラグする」ボタンが自動で追加される(フラグボタンは出力をローカルファイルに保存する)\n","\n","簡易的なWebサーバが起動し、ブラウザ上でWeb UIが表示される\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/gradio1.jpg\" width=700>\n"]},{"cell_type":"markdown","metadata":{"id":"fcPpqmEnt4qW"},"source":["## より実践的な例\n","\n","他にも様々な機能があるため、調べてみるとよい\n","\n","```\n","import gradio as gr\n","\n","# コールバック関数の定義\n","def callback_func(val1,val2,val3):\n","    return str(int(val1) * int(val2)), f\"気温は {val3} 度です\"\n","\n","# 画面レイアウトの定義(Interfaceを使用)\n","app = gr.Interface(\n","    title=\"計算機\",\n","    fn=callback_func,\n","    inputs=[\n","        gr.Textbox(label=\"入力欄1\",lines=3, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Textbox(label=\"入力欄2\",lines=5, placeholder=\"ここに数値を入れてください...\"),\n","        gr.Slider(label=\"温度\",minimum=0,maximum=100,step=1)\n","    ],\n","    outputs=[\n","        gr.Label(label=\"計算結果1\",lines=3),\n","        gr.Textbox(label=\"計算結果2\",lines=3)\n","    ]\n","    )\n","\n","# Web UIの起動\n","app.launch(inbrowser=True)\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WXViJMW_oVc5"},"source":["### Blocks\n","\n","簡易的に使用する場合はInterfaceを使い、より複雑なレイアウトを作る場合はBlocksを使う\n","\n","- Interfaceのメリット\n","\n","  - ショートカット文字列の使用が可能(Blocksでは使用不可)\n","  - Interfaceでは基本的なボタンを自動生成\n","\n","- Blocksのメリット\n","\n","  - Blocksでのみ利用可能なレイアウトがある\n","  - レイアウトが複雑になった場合でもwith文で可読性の高いコードが書ける\n","\n","Blocksではレイアウトを指定でき、次のようなレイアウトがある\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cGy7qHvAtH4x"},"source":["#### コンポーネントを横や縦に並べる  \n","gr.Row()やgr.Column()を利用する\n","\n","コード中にコメントがあるので、切り替えてみるとよい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQxV_PT9rq2P"},"outputs":[],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app:\n","  with gr.Row():\n","#  with gr.Column(scale=2):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\")\n","    # イベントハンドラー\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"4Itelzr_sjVE"},"source":["#### タブで表示する  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyByV3fYsmGQ"},"outputs":[],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # 入力タブを定義\n","  with gr.Tab(\"入力タブ\"):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    btn = gr.Button(\"クリックしてね!\") # 出力タブを定義\n","  with gr.Tab(\"出力タブ\"):\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"nkv4_Mj4s_qW"},"source":["#### 丸角の子要素を設ける\n","\n","角が丸く、周囲にパディングがあるボックスである\n","- 以前はgr.Boxであったが、gr.Blocksに変更となった"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1IMxhLDtS9j"},"outputs":[],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Box()関数でレイアウトを定義\n","  with gr.Group():\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"NnTwV4PrtfLK"},"source":["#### 子要素を折りたたみ可能にする\n","\n","Accordionは、子要素を折りたたみ可能なセクションに配置する\n","\n","openパラメータにより初期状態で開いているか(True)、閉じているか(False)を指定でき、デフォルトはOpen(True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCdTBWZduNgD"},"outputs":[],"source":["import gradio as gr\n","def greet(name): return \"Hello \" + name + \"!\"\n","with gr.Blocks() as app: # Accordion()関数でレイアウトを定義\n","  with gr.Accordion(label=\"アプリを見る\", open=False):\n","    inputs = gr.Textbox(placeholder=\"名前を入力してね!\", label=\"名前\")\n","    outputs = gr.Textbox(label=\"挨拶\")\n","    btn = gr.Button(\"クリックしてね!\") # イベントを定義\n","    btn.click(fn=greet, inputs=inputs, outputs=outputs)\n","app.launch()"]},{"cell_type":"markdown","metadata":{"id":"HiEG9ZpVpKGJ"},"source":["### チャットを実装する\n","\n","なんでもなく、\"How are you?\", \"I know you\", \"I'm very hungry\"のどれかをランダムに答えるアプリである"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hAUuk-oo47w"},"outputs":[],"source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I know you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"JRBnfYGzKeAL"},"source":["(2023/12) 異常終了するため、ここで一度実行をとめる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmUiESaiCiAY"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"markdown","metadata":{"id":"WKL_t_xT02BX"},"source":["# LLMチャットの実装\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO27LB3bP-Sn"},"outputs":[],"source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IErBgXXnKG1N"},"outputs":[],"source":["!pip uninstall -y --quiet tensorflow-probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cAsOhXoKG1S"},"outputs":[],"source":["!pip install --quiet openai cohere tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pTuyW9NKG1S"},"outputs":[],"source":["!pip install --quiet chromadb kaleido python-multipart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJNxN6pbKG1S"},"outputs":[],"source":["!pip install --quiet langchain-experimental"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRDSFHfIKute"},"outputs":[],"source":["!pip -q install gradio==3.48.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgGEEYyY1KxH"},"outputs":[],"source":["import langchain\n","from langchain.chat_models import ChatOpenAI\n","\n","langchain.verbose = True\n","\n","def chat(message: str) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","    return llm.predict(message)\n"]},{"cell_type":"markdown","metadata":{"id":"KYCXt_wRU6bb"},"source":["まずは、フレームワークとして、似非チャットを実装する\n","- ランダムに答えを返す"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vinBiFGT1N6n"},"outputs":[],"source":["import gradio as gr\n","import random\n","import time\n","\n","with gr.Blocks() as demo:\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox()\n","    clear = gr.ClearButton([msg, chatbot])\n","\n","    def respond(message, chat_history):\n","        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n","        chat_history.append((message, bot_message))\n","        time.sleep(2)\n","        return \"\", chat_history\n","\n","    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"GoYskNFqH4ud"},"source":["次に、過去の会話履歴を踏まえて回答するように chat関数を次のように更新する\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igs39-zMH-b4"},"outputs":[],"source":["import langchain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ChatMessageHistory\n","from langchain.schema import HumanMessage\n","\n","langchain.verbose = True\n","\n","def chat(message: str, history: ChatMessageHistory) -> str:\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","    messages = history.messages\n","    messages.append(HumanMessage(content=message))\n","\n","    return llm(messages).content"]},{"cell_type":"markdown","metadata":{"id":"ayaQTKNLVQUu"},"source":["実際に試してみよう\n","\n","- 私の名前は出田です。\n","- 私の名前がわかりますか？\n","\n","と入力すると、\n","「はい、先ほど出田さんとおっしゃいましたよね。」\n","といった回答になる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JH7cYHuOSLio"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import AIMessage, HumanMessage\n","import openai\n","import gradio as gr\n","\n","llm = ChatOpenAI(temperature=1.0, model='gpt-3.5-turbo-0613')\n","\n","def predict(message, history):\n","    history_langchain_format = []\n","    for human, ai in history:\n","        history_langchain_format.append(HumanMessage(content=human))\n","        history_langchain_format.append(AIMessage(content=ai))\n","    history_langchain_format.append(HumanMessage(content=message))\n","    gpt_response = llm(history_langchain_format)\n","    return gpt_response.content\n","\n","gr.ChatInterface(predict).launch()"]},{"cell_type":"markdown","metadata":{"id":"lOIsJaGGdj1y"},"source":["# PDFドキュメントの内容をLangChainを用いて問い合わせる\n","\n","ChatGPTには一度に扱えるテキストの量に限界があり、一度に入力させることはできない\n","\n","そこで、巨大PDFの内容からテキストを抽出し、分割、テキスト間の関連性もつベクトルデータをベクターストアに格納する\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYYsMYHbeVsj"},"outputs":[],"source":["!pip install --quiet openai chromadb langchain pypdf tiktoken"]},{"cell_type":"markdown","metadata":{"id":"YxsdRd6fi0b2"},"source":["必要となるライブラリを読み込む"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LzkJuKBeee0"},"outputs":[],"source":["import os\n","import platform\n","\n","import openai\n","import chromadb\n","import langchain\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.document_loaders import PyPDFLoader"]},{"cell_type":"markdown","metadata":{"id":"vfuM2_fii3k1"},"source":["API-KEYを読み込む\n","- エラーになる場合は、このテキストの最初にあるAPIキー設定箇所を再実行すること"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB17dhj0f_BW"},"outputs":[],"source":["from dotenv import load_dotenv\n","load_dotenv(verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"KhbAaMY-jEr4"},"source":["テスト用PDFとして、米国CLOUD法のホワイトペーパーを利用する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbX0gqVhelI5"},"outputs":[],"source":["if not os.path.exists('wp.pdf'):\n","  !wget https://www.justice.gov/criminal-oia/page/file/1153436/download -O wp.pdf"]},{"cell_type":"markdown","metadata":{"id":"xgkvpakBjKHD"},"source":["PDFローダでテキスト化して読み込み、読分割する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc2W5lgNej0E"},"outputs":[],"source":["loader = PyPDFLoader(\"wp.pdf\")\n","pages = loader.load_and_split()"]},{"cell_type":"markdown","metadata":{"id":"eRBuWsaBjVS9"},"source":["5ページ目の文章を確認してみよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWOUsB8DfKuw"},"outputs":[],"source":["pages[5].page_content"]},{"cell_type":"markdown","metadata":{"id":"tNIT8POUjbHM"},"source":["ChatGPTを準備する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3blnhosfQ5S"},"outputs":[],"source":["openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n","llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"]},{"cell_type":"markdown","metadata":{"id":"OMEsL8wCjcsg"},"source":["PDF文章をembeddingしてベクターストアに登録する\n","\n","このベクターストアを LLM に与えることで、テキスト間の関連性について表現したベクトルデータを外部から与え、LLMで利用できるようになる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCIHq40PgeW5"},"outputs":[],"source":["embeddings = OpenAIEmbeddings()\n","vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=\".\")\n","vectorstore.persist()"]},{"cell_type":"markdown","metadata":{"id":"3MSvZvvXjh-W"},"source":["PDF ドキュメントへ自然言語で問い合わせる\n","\n","ここでは、回答文の作成に関連した元テキスト群についても示すように指定する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdyjX_OIgjL9"},"outputs":[],"source":["pdf_qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"]},{"cell_type":"markdown","metadata":{"id":"v8FvaEGDj2EB"},"source":["英語で US クラウド法とは何か？について問い合わせてみよう\n","- 回答は日本語を用いるように指示する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3zyM2nWidDT"},"outputs":[],"source":["query = \"What is US Cloud Act? Answer within 50 words in Japanese.\"\n","chat_history = []\n","\n","result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n","\n","result[\"answer\"]"]},{"cell_type":"markdown","metadata":{"id":"NuhpAlr1kCPk"},"source":["上記の回答に関連した元のテキスト群について確認する"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIVIoNhCgx-9"},"outputs":[],"source":["result['source_documents']"]},{"cell_type":"markdown","metadata":{"id":"5ewuvirBkIov"},"source":["US クラウド法によってどういう主体が影響を受けるか問い合わせる"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCIPxwSxg4Gz"},"outputs":[],"source":["chat_history = [(query, result[\"answer\"]), (query, result[\"answer\"])]\n","query2 = \"CLOUD法によって影響を受ける主体にはどういったものがありますか？日本語で回答してください。\"\n","\n","result2 = pdf_qa({\"question\": query2, \"chat_history\": chat_history})\n","\n","result2[\"answer\"]"]},{"cell_type":"markdown","metadata":{"id":"Ue-g0X-0kNJ5"},"source":["ChatGPT による回答の正確性については保証されていないが、難解な資料を素早く理解するにはかなり有効であるといえよう"]},{"cell_type":"markdown","metadata":{"id":"pkz25WaGDlYG"},"source":["# RAG (Retrieval Augmented Generative)\n","\n","(A Cheat Sheet and Some Recipes For Building Advanced RAGで紹介されている内容)\n"]},{"cell_type":"markdown","metadata":{"id":"En2eQBk2FnJr"},"source":["## RAG の基本\n","\n","RAGは、ユーザーの質問に対して、外部データベースから関連するドキュメントを取得し、そのドキュメントと元々のユーザーの質問をセットにしてLLMに渡され、LLMは、この内容をもとに回答を生成する手法\n","\n","既に述べたLangChainのDictionaryがこれに相当する\n","- 目的はハルシネーションを抑え、最新の情報や、極めて専門的もしくは独自の情報を用いて回答を作成したいときなど\n","\n","RAGにより以下が可能となる\n","- ハルシネーションやノイズを削減させる\n","- 情報不十分のとき回答しないようにする\n","- 複数の情報を参照して回答する\n","- 情報ソースの誤りを指摘できる\n","\n","RAG品質は、次の指標で計測する\n","- コンテキストの関連性\n","- 回答の関連性\n","- 忠実度合\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GwmBo4dbFi42"},"source":["## RAGの応用\n","\n","複雑な質問や高度なデータソースに対応するため、次の2つの重要な要件を満たすために高度な技術や戦略について述べる\n","\n","- Retrieval（検索）: ユーザーの質問に最も関連性の高いドキュメントを見つけること\n","- Generation（生成）: 検索されたドキュメントを効果的に利用し、回答を生成すること\n","\n","これを実現する技術的手法は以下の通りである"]},{"cell_type":"markdown","metadata":{"id":"Vg-_gdtIF7BQ"},"source":["### 検索の要件に対する技術\n","\n","- Chunk-size Optimization（チャンクサイズの最適化）:  \n","検索対象のドキュメントを適切なサイズに分割することで、情報の関連性を高める手法  \n","検索対象が多すぎる場合、効率が低下するため、適切なサイズのチャンクに分割することが重要\n","\n","- Sliding Window Chunking（スライディングウィンドウチャンク分割）:  \n","情報の見落としを防ぐために、ドキュメントをオーバーラップしながらスライドさせて分割する手法  \n","重要な情報がチャンクの境界に分割されるのを防ぐ\n","\n","- Structured Knowledge Integration（構造化ナレッジの統合）:   \n","構造化データ（例：データベースやスプレッドシート）を検索システムに統合し、より精度の高い検索結果を提供する手法\n","\n","- Metadata Attachments（メタデータの付与）:   \n","メタデータ（例：著者、出版日、トピックなど）を利用して、検索精度を向上させる手法  \n","文脈に沿った関連性の高いドキュメントを絞り込むことができる\n","\n","- Knowledge Graphs（ナレッジグラフ）:   \n","ナレッジグラフを活用し、ドキュメント間の関連性を明確にする手法  \n","情報同士の関係性を視覚化することで、検索の精度が向上する\n","\n","- Mixed Retrieval（混合検索）:   \n","複数の検索アルゴリズムを組み合わせ、情報源から最も有用なドキュメントを取得する手法  \n","異なるアプローチを組み合わせることで、異なる角度から関連性が高められる\n","\n","- Question Embedding Transformation（質問の埋め込み変換）:  \n"," 質問を埋め込み表現に変換し、情報と質問の一致を効果的に計算する手法  \n","質問の文脈を理解し、より関連性の高い検索結果が取得できる"]},{"cell_type":"markdown","metadata":{"id":"oFR2VQ7QF32a"},"source":["### 生成の要件に対する技術\n","\n","- Information Compression（情報の圧縮）:   \n","生成モデルが扱いやすい形に情報を圧縮する手法  \n","例えば、ノイズを減らしたり、重要な情報を抽出してから生成を行う\n","\n","- Generator Fine-Tuning（生成モデルの微調整）: 検索されたドキュメントに基づいて、生成モデルを特定のタスクに適応させるための微調整を行う手法  \n","生成された回答の精度が向上する\n","\n","- Result Re-Ranking（結果の再ランキング）: 一度生成された結果を再度評価し、最も関連性が高いものを再ランキングする手法  \n","ユーザーに提供する最終結果の品質が向上する\n","\n","- Adapter Methods（アダプターメソッド）: 生成モデルに外部情報を柔軟に取り入れるための手法  \n","新しい情報を簡単に生成モデルに適応させることができる"]},{"cell_type":"markdown","metadata":{"id":"ITzu1DBNG_x9"},"source":["### 同時に2つの要件を満たす技術\n","- Monolithic Fine-Tuning（単一モデルの微調整）:   \n","検索と生成の両方を同時に最適化するために、単一の大規模モデルを微調整する手法  \n","情報検索と生成のプロセスが統合され、シームレスな処理が可能になる\n","\n","- Retrieval-Foundational Model（検索基盤モデル）:   \n","検索モデルそのものを基礎モデルに統合し、検索結果と生成がより密接に結びついた形で動作するように設計する手法\n","\n","- Generator-Enhanced Retrieval（生成強化型検索）:   \n","生成モデルを利用して、検索結果のリランキングやフィルタリングを行う手法  \n","生成モデルが得た情報をさらに効果的に検索に活用できる\n","\n","- Iterative Retrieval-Generation（反復検索生成）:   \n","検索と生成のプロセスを繰り返し行い、精度の高い回答を得る手法  \n","例えば、一度検索された結果を元に、再度質問を修正しながらプロセスを繰り返す"]},{"cell_type":"markdown","metadata":{"id":"_Is6AnBzJAt-"},"source":["### RouterRetriever\n","\n","RAGで複数の埋め込みモデルを利用し、文書検索精度を向上させる手法\n","\n","(KAISTとアレン人工知能研究所により提案)\n","\n","通常のRAGは、埋め込みモデル（エンべディングモデル）を1つだけ利用して、ベクトル検索する\n","- ユーザーの質問とソースとなる大量の文書を、ただ一つの同じ埋め込みモデルでベクトル化して、検索している\n","- この埋め込みモデルとして、OpenAIが公開している「text-embedding」といった汎用的モデルの利用が一般的\n","\n","汎用的な埋め込みモデルではなく、そのドメインに特化したモデルを利用することで検索精度が高くなる\n","\n","RouterRetrieverは、ユーザーの質問のジャンルに応じて、複数の埋め込みモデルを使い分ける手法\n","\n","#### 従来手法における問題点\n","\n","従来はユーザーの質問に専門的な内容が入っている場合、文書検索が適切に行えなかった\n","\n","これは、RAGの文書検索がもつ以下のジレンマが関係している\n","\n","- RAGである分野の検索に強くするには、その分野に特化した埋め込みモデルにするのが一番よい\n","- 一方でその分野に特化すると、それ以外の質問には、弱くなる\n","\n","現状は、これに対し、全ての分野に平均的に強い「バランス型」の埋め込みモデルを1つだけ使うという方針であった\n","- どのような問いあわせでも平均的な性能が獲得できる\n","\n","性能を上げるには、ハイブリッド学習のように、複数の埋め込みモデルを動的に使い分ける\n","- ユーザーからの質問に対して、「どの埋め込みモデルを使うの最適か」を判断し、そのモデルで検索を行う\n","\n","\n","#### 事前にやっておくこと\n","\n","- 専門家モデル（例えば、医学特化の埋め込みモデルなど）を複数作成しておく\n","- `Contriever`という汎用的な埋め込みモデルをLoRA（Low-Rank Adaptation）して複数作成しておく\n","  - ユーザーの質問には複数のモデルを使い分けますが、ドキュメントの埋め込みには、通常通り1つの埋め込みモデル（Contriever）のみを利用\n","- 「どの埋め込みモデルを使うの最適か」を、動的に判断するための仕組みを構築\n","- ユーザーが質問を入力してきた際に、ユーザーの質問を汎用的なモデル（Contriever）でベクトル化\n","  - このベクトルと、事前に用意した「各専門家モデルが得意なタイプの質問のベクトル」を比較\n","\n","#### 埋め込みモデルの選択\n","\n","で最もスコアが高かったモデルを利用して、もう一度ユーザーの質問をベクトル化\n","\n","このベクトルを利用して、検索対象である、大量の文書を検索（ここは通常のRAGと同じ）\n","\n","\n","BEIRベンチマークにおいて、MSMARCOで訓練された単一モデルに比べて+2.1、マルチタスク訓練モデルに比べて+3.2のnDCG@10スコア向上を達成\n","ゲート（専門家モデル）の数を増やすにつれて、一貫して性能が向上\n","ドメインに特化した専門家モデルがないデータセットに対しても汎用性を持つことが確認されている"]},{"cell_type":"markdown","metadata":{"id":"OXsU26yn9VP4"},"source":["# その他トピック\n","\n","ここ3ヵ月で次の通り、追いかけるのも大変\n","- 1年たち、既に古すぎる情報となった\n","\n","## DALL-E3\n","画像生成AIの中でも高精度・高品質画像を作成可能なAI\n","\n","## SDXL Turbo\n","リアルタイムでプロンプトに反応して画像生成\n","https://clipdrop.co/ja/stable-diffusion-turbo\n","\n","## Adobe Firefly\n","ようやくAdobeも動きだし、著作権・商用利用問題クリアの画像生成AIを公開\n","\n","## Notion AIやCursorなどのLLM統合\n","- Notionは様々な機能を持つメモ帳\n","  - AI拡張は有料\n","- Cursorはプログラミング用統合環境\n","  - VSCodeオワコンといわれている\n","\n","## Claude\n","- ChatGPT並み(かそれ以上)の精度を持つLLM\n","\n","## Google Gemini\n","- Googleとの連携がとりやすく、使い勝手の良いLLM\n","- Colabとも連携\n","\n","## Runway Gen-2\n","画像から高精細かつ高品質な動画を生成\n","- 動かしたいものを指定したり、プロンプトで指示することができる\n"]},{"cell_type":"markdown","metadata":{"id":"EXKfzAT0TmDV"},"source":["# 課題1(LangChain)\n","\n","上記のPDFファイル解析アプリケーションの例を、LangChainを用いたチャットボット形式に修正しなさい"]},{"cell_type":"markdown","metadata":{"id":"OEnQXDE_G8Ju"},"source":["# 課題2(LangChainによるチューニング)\n","\n","履歴付きチャットボットを改造し、常に回答が子供の対応であるようにプロンプトエンジニアリングを行いなさい\n","\n","単純に、入力問い合わせに対して、「考え方や言葉遣いを6歳の子供のようにして答えなさい」といった言葉を付け加えなさい"]},{"cell_type":"markdown","metadata":{"id":"jg_-NJjO89GY"},"source":["# 課題3(Embedchain)\n","\n","Embedchainを用いて、何かしら専門的な内容に対して回答可能とするチャットボットを実装しなさい\n","\n","- ChatGPTへの質問結果も示し、実際に回答が改善されていることを示しなさい。\n","- インプットトークンが足りないというエラーが出た場合は、次の方法によりGPT4への切り替えなさい\n","  - gpt4を指定したconfigファイルを作成・配置する\n","  - [推奨] App()で初期化する際に `App.from_config(config_path=\"config_file.yaml\")`とする"]},{"cell_type":"markdown","metadata":{"id":"xoemzd9MkM_7"},"source":["# 課題4(GPTs)\n","\n","ChatGPTのGPTsを試しなさい\n","\n","この課題を解くには、ChatGPT4を利用する必要があるため、課金が伴う点に注意しなさい\n","\n","例えば、\n","\n","- 政府発行の文章や、博物館などの解説ページなどの情報を参照し、専門的な内容に回答するチャットを作成する\n","  - 会社の内部文章などが面白いが、普通に規定でできないであろう\n","  - 仮想的にそのような「公開されていない情報」(自分の日記など)について試すとよい\n","- 日本語で必ず回答するようにする\n","- 発言の最後に、専門用語についての解説を付与するようにする\n","\n","など\n","\n","また、GPTsにおけるAPI操作機能を用いて、気温や湿度などを住所から入手し、適切な服装などを指示するGPTを作成しなさい\n","\n","https://weather.tsukumijima.net/ などを利用するとよい\n","\n","さらに、HotPepper APIを用いて、お店を検索するGPTを作成しなさい\n","- これについて、HotPepper側の制限によりアクセスが拒否されている場合がある\n","- この場合は、Proxyを立てること"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/keioNishi/lec-mlsys/blob/main/mlsys-text-M-ChatGPT-2-Application.ipynb","timestamp":1703525217224},{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}