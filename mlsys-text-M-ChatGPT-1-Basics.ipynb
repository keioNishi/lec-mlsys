{"cells":[{"cell_type":"markdown","metadata":{"id":"bHZ54p4iIfem"},"source":["---\n",">「おそらく世の中で最高の話し上手は、人に話をさせる人だろう。」\n",">\n","> ジョン・スタインベック\n","---"]},{"cell_type":"markdown","metadata":{"id":"L9HXMz2Zvn8D"},"source":["# ChatGPTとは何か\n","\n","人間のように自然に対話するモデルであり、自然言語で入力されたテキストを受け取ると、それに対応する返事をテキストとして出力する\n","\n","質問に対して忠実に回答し、例えばプログラムコードを出力させることや、数学・科学・物理などの問題を解かせることなども可能\n","- ChatGPTの詳細について述べた論文が存在しないため、その詳細は不明であるが、公式で次のように説明されている\n","\n","- ChatGPTはReinforcement Learning from Human Feedback(RLHF)で学習\n","- InstructGPTと同じ学習手法だが、データ集めの工程が異なる\n","\n","2022年11月のリリースからわずか6日で100万ユーザを獲得、これは、facebookが4年半、Instagramが2年半で達成した記録を大きく更新"]},{"cell_type":"markdown","source":["# 参考図書\n","\n","買う必要はない(印税も手に入らない)\n","\n","以下の図書が初見ではとっつきやすいであろう\n","\n","<img src=\"https://m.media-amazon.com/images/I/81+iwPEKL9L._SL1500_.jpg\" width=300>"],"metadata":{"id":"3e1SuEykBbOi"}},{"cell_type":"markdown","metadata":{"id":"6AD8YuQlAkuc"},"source":["## ChatGPTのバージョン\n","\n","まず、GPTとは、ChatGPTにおける大規模言語モデル (LLM)の名称であり、次のようなバージョンがある\n","\n","次のバージョンが存在する\n","- GPT-3.5\n","  - 以前の無料バージョン\n","- GPT-4\n","  有料で使えるバージョンとして登場したが、GPT-4oが限定的に無料で利用できる\n","  - GPT-4-32k  \n","  かつて存在した、32kトークンまで入力可能なモデル\n","  - GPT-4-Turbo  \n","  かつて存在した、128kトークンまで入力可能(日本語10万文字)なモデル\n","  - GPT-4V  \n","  テキストに加えて画像を扱えるモデル\n","  - GPT-4o\n","  4oが発表される前は、ChatGPT4に自分の言語モデルは何かと尋ねると、ChatGPT4.5と答えることで話題となった、精度や入力可能なトークン数の拡張、マルチモーダル対応などで拡張が図られた\n","- GPT-5\n","  - 既に開発中という発表が行われているが、現状特に詳細は報告されていない\n","  - テキスト・画像・音声を扱うマルチモーダルAIとされている\n","\n","バージョンとは別に、GPTsによるアプリ化や、Phthonの実行やメモリ機能が利用できるようになった"]},{"cell_type":"markdown","metadata":{"id":"3kjS9ohSEt4g"},"source":["## ChatGPTを取り巻く状況\n","\n","ChatGPTはLLMにおいて中心的な存在であり続けているが、これに匹敵する高性能なLLMが次々と発表されている\n","\n","この辺りは、頻繁に状況が変化しており、テキストとして記す状況にはない\n","\n","- Microsoft Copilot\n","  - 中身はGPT-4\n","  - Copilot Proの登場により、Copilotが普及\n","  - Microsoft Officeと連携できる(有料)\n","\n","- Google Gemini\n","  - Gemini 1.5 Proは、GPT-4と同等の性能を持つ\n","  - 長時間の動画や音声を利用でき、200万トークンまで入力できる\n","\n","- Anthropic Claude\n","  - Claude 3.5 sonnetをリリース\n","  - 性能はChatGPT4oにひっ迫し、特にプログラムコード生成ではGPT4oを上回るといわれている\n","  - その他、Claude 3 OpusとClaude 3 Haikuがある\n","  - ProjectsによりcustomAIの生成や管理が可能、GPTsと類似\n","  - Artifactsにより生成コンテンツのリアルタイム表示が可能\n","\n","- Perplexity\n","  - 検索に特化\n","  - 内部モデルにGPT-4やClaude 3.5 Sonnetを利用できる\n","  \n","これらの結果を同時に取得し結果の違いを確認することで、ハルシネーションを避けつつ、各モデルの特徴を知ることができる\n","- ChatHubなどを利用するとよい"]},{"cell_type":"markdown","metadata":{"id":"fFMy8VuiEk9H"},"source":["## 実際に扱ってみる\n","\n","次のサイトにアクセスする\n","\n","https://chat.openai.com/chat\n"]},{"cell_type":"markdown","metadata":{"id":"C-33nxuVHvkD"},"source":["次は、ChatGPTに、「ChatGPTとはなんですか？」と聞いてみた答えである\n","\n","![ChatGPTによる説明](https://class.west.sd.keio.ac.jp/dataai/text/chatgptans.png)"]},{"cell_type":"markdown","metadata":{"id":"nNm8NrrTMPLQ"},"source":["# 基盤モデル(foundation model)について\n","\n","基盤モデルとは、大量・多様なデータから高い汎化性能を獲得したAIのこと\n","- 単体で従来のAIには解けなかった多様なタスクを解けることから、AIにパラダイムシフトをもたらすと期待されている\n","- 大量かつ多様なデータで訓練され、ファインチューニングなどにより様々な下流タスクに適応できるモデル\n","  - BERTや、GPT-3、GPT-Rなども含まれる\n","\n","なお、AGI(Artificial General Intelligence)もほぼ同じ意味で扱われる\n","\n","基盤モデルは、大量のデータで学習したというだけでなく、次のような違いがある\n","- 従来のAIは、解きたいタスクごとにデータセットや人力ラベリングなどを準備し、適切なアーキテクチャモデルを設計・訓練する必要があった\n","- 基盤モデルは、タスクごとに集めるデータは少量でも、目的のタスクに特化させることがなく、分布外データに対する予測性能も優れている\n","- GPT-3など、タスクの内容を自然言語で記述して入力に含めることで、オープンエンドなタスクに数例の訓練サンプルで適応(in-context learning)できるモデルが提供されている\n","\n","従来のAIと基盤モデルの違いを次にまとめる\n","\n","|      | タスク毎に必要なデータ量 | タスク毎のモデリング | 分布外データへの頑健性 |\n","| --- | :---: | :---: | :---: |\n","| 従来 | 大きい | 個別に実施 | 弱い |\n","| 基盤モデル | 小さい | 適応するのみ | 強い |\n","\n","基盤モデルは次のような経緯で発展を遂げてきた\n","\n","- 2018年発表の大規模言語モデルBERTが、基盤モデルにとって大きな転換期となり、これ以降基盤モデルの議論が活性化した\n","  - BERTはファインチューニングを必要とするが、汎化性能の高さから産業界や別の研究分野で活用され、大規模言語モデルへの注目を集めた\n","\n","- 2020年にGPT-3が登場し、その性能に世界が熱狂した\n","  - GPT-3はBERTよりも1,000倍以上多いパラメータを保有し、in-context learningにより多様なタスクを解くことで話題となった\n","  - in-context learningでは、モデルに入力文だけでなくタスクの説明と入出力例をセットにして入力する\n","  - 感情分析や文書要約を行う場合も、タスクの説明を適切に変更するだけで対応でき、訓練データを集める必要がない\n","  - GPT-3は文章生成、翻訳、文書分類、質問応答などから果ては四則演算や簡単なプログラミングまで行うことができる\n","  - GPT-3のようなモデルでは、予測精度は訓練データや特徴量などではなく、モデルに与える説明文や入出力例によって大きく影響を受ける\n","    - 予測精度を上げるためには、モデルにとってわかりやすい説明文や入出力例を与えることが重要\n","    - そのための工夫を、特徴量エンジニアリングに対してプロンプトエンジニアリング(prompt engineering)と呼ぶ\n","  - このモデルの大規模化は必然であり、事前に大規模化による性能向上が予見されていた(スケーリング仮説参照、スケーリング仮説は多様体仮説で説明できるともいわれている)\n","  - 心理学試験（欺き課題など）すらもパスするようになった(以下はその一例)\n","    1. 2つの箱のどちらかにボールを入れ、この様子をAとBが見ている\n","    1. Aがいなくなった後、Bはボールを別の箱に入れ替える\n","    1. Aが戻ってきたとき、Aはどちらの箱にボールが入っていると答えるかを答えさせる\n","    - この場合、Aの身になれば、ボールが入っていない方の箱に入っているという答えを選択するが、幼児はボールが入っている箱を選択する\n","    - このような考察から、言語の獲得が心や知能を生み出すという考え方がある\n","      - つまり、大規模言語モデルは言葉をどんどん獲得することで、人間とは違う形で心や知能を手に入れるといえる。。。？\n","\n","- 2021年CLIPの登場により、視覚・言語を扱う基盤モデルの道を開いた\n","  - CLIPはテキストと画像を同じ特徴空間に写像する2つのエンコーダで構成される\n","  - CLIPを使うと、次のように任意の画像分類問題を追加の学習なしで解くことができる\n","  - CLIPは画像とテキストというモードの異なる情報を意味的な近さによって結びつけることを可能とした\n","  - テキストから画像を生成するモデルを訓練することも可能となり、その最初の例がDALL-Eであり、その後DALL-E2、Imagen、Parti、Imagen、さらにはStable Diffusionなどが登場\n","  - DALL-Eはダリと発音し、画家のサルバドール・ダリと、CGアニメのキャラクターWALL-Eが名前の由来\n","\n","- 2022年登場のPaLMにより因果関係や常識の理解を要する難しいタスクも解けるようになった\n","  - 入出力の例を示す際に思考過程を含めることで、それまでは解けなかった問題を解けるようになるとも報告されている\n","  \n","- 2022年登場のFlamingoにより画像から文章を出力することが可能となった\n","  - 画像分類に限らず、OCRや動画像に基づく質問応答のようなフリーテキストで回答することができる\n","\n","- 同様に2022登場のGotoにより、数値(連続・離散問わず)を扱えるため、ロボットハンドの操作から画像のキャプション生成までを1つのモデルで解くことができるようになった\n","  - 言語や視覚情報を理解できるロボットの実現へ大きく進歩した\n","\n","なお、スケール仮説により膨大なデータ・パラメータ・計算資源を用いた協力なモデルが、2022年5月に発表された言語モデルPaLMのフルモデルは BIG-bench で初めて人間の平均スコアを超えており、自然言語においても人間を凌駕している\n"]},{"cell_type":"markdown","metadata":{"id":"Z2PNV5cxj9Gf"},"source":["# プロンプトエンジニアリング\n","\n","プロンプトエンジニアリングは、ChatGPTなどの大規模言語モデル（LLM）を利用する際に、より望ましい結果を得るための必須技術の一つ\n","- 「AI、特に自然言語処理を担う言語モデル(LM)さらにいえば大規模言語モデル(LLM)を効率的に使用するために、言語モデルへの命令(prompt)を開発・最適化する技術」のこと\n","\n","次のような処理をさす\n","\n","- AIが実行するタスクに、適切な質問や指示を与えることで、より望ましい結果を引き出す\n","- LMやLLMでは、工夫せず直接的な(直観的な)質問や指示を与えるだけでは必要としている適切な出力を獲得できない場合があるため、適切なプロンプトを設定することで、AIが効率的に作業を行い、より精度が高く、望ましい結果を出力する確率を高めることができるようになる\n","- 敵対的なプロンプトに対するリスクを軽減するために、プロンプトエンジニアによって対策のためのプロンプトを作成し、プロンプト学習と呼ばれる方法で言語モデルを訓練する\n"]},{"cell_type":"markdown","metadata":{"id":"RJawbSHdmDEC"},"source":["## プロンプトとは\n","\n","言語モデルに対する入力に限らず、次のような内容もプロンプトの構成要素であるが、モデルに依頼するタスクごとに必要な要素は異なる\n","\n","- 指示(Instruction): モデルが実行する特定のタスクや命令\n","- 背景(Context): モデルの回答制度を高めるための追加の情報や文脈\n","- 入力データ(Input Dat): モデルに応答を求める入力や質問\n","- 出力形式/出力指示子(Output Indicator): 出力タイプやフォーマット\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2gMC36NZsiwH"},"source":["## 関連する用語\n","\n","few-shot学習と同じ文脈で、few-shot学習を行うためのプロンプト（テキスト）の\n","書き方に着目したFew-shot Prompting、さらに、例文として「問題を解く手順」までも含めたプロンプトを与えることで、思考の連鎖（CoT：Chain of Thought）few-shotを組み合わせたプロンプティングのテクニックであるFew-shot-CoTがある\n","\n","より大きな言語モデルになるほど文脈内の情報をより効率的に利用できることがわかっており、これをコンテキスト内学習（In-Context Learning）と呼ぶ\n","\n","以下、実際に試行してみよう"]},{"cell_type":"markdown","metadata":{"id":"MoVKlvPqm_kF"},"source":["## Zero-shot prompting\n","\n","例やデモンストレーションなしに、いきなり質問を投げる手法\n","\n","- 事前情報を与えずに直接モデルに応答を求める\n","- ChatGPTなど、大量データでトレーニングされ、調整されている言語モデルであれば、Zero-shot promptingだけで十分正しい回答を導くことができる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/zero-shot-prompting.jpg\" width=600>\n","\n","このように、モデルに事前情報を提供することなく、質問と指示を入力して、最終的に必要な応答を求めている\n","\n","シンプルであるが、モデルの回答精度を下げる場合も想定される\n"]},{"cell_type":"markdown","metadata":{"id":"yFRxjPsqt4AB"},"source":["## Few-shot prompting\n","\n","モデルに例やデモンストレーションを提供し、文脈学習を通して質問や指示と回答のパターンを学習させる手法\n","- プロンプトで示した例題が、その回答に対する条件付けとして使われる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/few-shot-prompting.jpg\" width=600>\n","\n","例題の数が多いほど、適切な回答を出力する確率が高まることが期待される\n","\n","Few-shot promptingでは、使う形式も性能に重要な役割を果たします。ランダムなラベルを使用する場合、ラベルと入力テキストの分布の両方が重要です。\n","\n","ラベルを用いた例も示す\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/few-shot-prompting2.jpg\" width=600>\n","\n","この質問では、独特のフォーマットを利用しているが、モデルは正しいラベルを予測する"]},{"cell_type":"markdown","metadata":{"id":"q0I61x8_xHCr"},"source":["## Chain-of-Thought Prompting\n","\n","連鎖的な思考をさせることで、出力精度を高める手法\n","- 「与えられた問いに対する最終的な答えの思考過程」も言語モデルが生成できるようにすること\n","\n","論理的な思考能力（Reasoning）が求められるタスクなどで、Few-shotでは精度向上が困難な場合への対応として検討された\n","\n","モデルが段階的な推論を必要とする連鎖的な思考において、Chain-of-Thought Promptingにより途中の推論や考え方を学習させておくことで、適切な処理を遂行させることができるとともに、推論をステップに分けて示すことで、回答を間違えた際にミスした段階がわかる\n","\n","次の例のように、思考過程が示されている\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Chain-of-Thought-Prompting.jpg\" width=600>\n"]},{"cell_type":"markdown","metadata":{"id":"UHEnWB5X1iKJ"},"source":["## Zero-shot CoT\n","\n","Chain-of-Thought（CoT）PromptingをZero-shot Promptingで用いる手法\n","\n","- 回答に至るまでにプロセスを生成することで、精度を向上させる\n","- 具体的に例を与えることなく、「ステップに分けて考えてください」といった一文を付け足し、段階的に推論を進めるよう指示する\n","\n","これだけで精度が向上することが知られている\n","\n","例を示すが、この問題に対して、ChatGPT3.5は正しく解答できなかった。ChatGPT4は、Zero-short CoTを用いずとも正しく解答することができるようになっている\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Zero-shot-CoT.jpg\" width=600>"]},{"cell_type":"markdown","metadata":{"id":"5nH1KnEb4t7W"},"source":["## Self-Consistency\n","\n","複雑な推論タスクを遂行するために、Few-shot promptingを組み合わせて多様な推論のプロセスを示し、その中から最も整合性の高い回答を選ぶように指示する手法\n","\n","Self-Consistencyの効果を確かめるために、まずは質問をそのまま入力する\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Self-Consistency.jpg\" width=600>\n","\n","正解しているが、連立方程式を用いてきたので、小学校の知識で回答できる鶴亀算を用いるように改めさせる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Self-Consistency2.jpg\" width=600>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3wEnOjM9CBhv"},"source":["## Generate Knowledge Prompting (知識生成プロンプティング)\n","\n","プロンプトの一部に使用する知識や情報を組み込む手法\n","- 入力文の中に知識を加えることで、正しい推論の出力を得られる可能性が高くなる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Generate-Knowledge-Prompting.jpg\" width=600>\n","\n","プロンプト内に知識を加えることで、より正確な推論の出力が得られている"]},{"cell_type":"markdown","metadata":{"id":"HdXbbV61GK7k"},"source":["## ReAct\n","\n","「（行動理由の）推論」と「（推論に基づいた）行動」を組み合わせて、言語モデルで推論とタスクを遂行する手法\n","\n","- ReasoningとActingを組み合わせてReActと呼ぶ\n","\n","シンプルな入力プロンプトを用いてタスクに対して推論を行った後、具体的なアクションを列挙する\n","- 推論と行動を交互に実行することで、相乗効果によってより高精度な回答を獲得できるようにする\n","\n","- Wikipediaなどの外部環境から追加情報を推論に組み込むこともでき、「行動」として、外部環境から新しい情報を収集でき、これを「観察」と呼ぶ\n","\n","ReActプロンプトの型としては、以下が使われます。\n","\n","- 文章（質問または指示）\n","- Thought :\n","- Action :\n","- Observation :\n","\n","この場合、Thoughtでは、汎用的な推論となりがちであるが、Actionによって具体的な方法やアドバイスを挙げさせることで、より深い観察を行うことができるようになる\n","\n","但し、試した範囲ではChatGPT May 24 Versionによいて、Thoughtなどの指示を無視し、最初からかなり完璧といえる解答を与えるようである\n","- 解釈する場合も、Actionなどその言葉のままに理解するため、期待している内容とは若干異なるようである"]},{"cell_type":"markdown","metadata":{"id":"f3J6bAhJoj3u"},"source":["# ChatGPT向けプロンプトエンジニアリング"]},{"cell_type":"markdown","metadata":{"id":"PQ-lfuISooMt"},"source":["## プロンプトの要素\n","\n","プロンプトには、以下のいずれかのコンポーネントが含まれることがある\n","\n","- 命令 - モデルに実行してほしい特定のタスクまたは命令\n","- 文脈 - 外部情報や追加の文脈が含まれる場合があり、モデルをより良い応答に導くことができます。\n","- 入力データ - 応答を見つけたい入力または質問\n","- 出力指示子 - 出力のタイプや形式を示します。"]},{"cell_type":"markdown","metadata":{"id":"7BeSnRute8ww"},"source":["## 事例\n","\n","OpenAIがChatGPTのプロンプトエンジニアリングのベストプラクティスについて示している\n","\n","抜粋して、以下に説明する\n","\n","- 命令をプロンプトの先頭に置き、###または\"\"\"で命令とコンテキストを区切ること\n","  - BAD：  \n","```\n","以下の文章を、最も重要な点を箇条書きにまとめてください。\n","{ここにテキストを入力}。\n","```\n","  - GOOD：  \n","```\n","以下の文章を最も重要な点を箇条書きにまとめてください。\n","```\n","\n","- 希望する文脈、結果、長さ、形式、スタイルなどについて、具体的、説明的、かつ可能な限り詳細に記述する\n","  - BAD  \n","```\n","OpenAIについて詩を書こう。\n","```\n","  - GOOD:  \n","```\n","最近のDALL-Eの製品発表（DALL-Eはテキストから画像へのMLモデル）に焦点を当て、OpenAIに関する感動的な短い詩を｛有名な詩人｝のスタイルで書きなさい。\n","```\n","\n","- 望ましい出力形式を明示すること\n","\n","  - BAD:   \n","```\n","以下のテキストで言及されているエンティティを抽出する。企業名、人名、特定の話題、テーマの4つのエンティティを抽出する。  \n","テキスト： {テキスト}。\n","```  \n","このような形では、どのように示してよいのかの指示がないためクオリティが落ちる  \n","具体的なフォーマット要件を示すと、モデルはよりよく反応し、プログラムによる複数の出力の確実な解析も容易になる\n","\n","  - GOOD：  \n","```\n","以下のテキストで言及されている重要なエンティティを抽出します。最初にすべての会社名を抽出し、次にすべての人名を抽出し、次に内容に合った特定のトピックを抽出し、最後に一般的な包括的テーマを抽出しなさい。  \n","望ましい形式\n","会社名 <comma_separated_list_of_company_names> (コンマで区切られた会社名リスト)\n","人名: -||-人名\n","特定のトピック -||-\n","一般的なテーマ: -||-  \n","テキスト {テキスト｝\n","```\n","\n","- Zero-shot, Few-shot, Fine-tuneの順に試すこと\n","  - Zero-shot  \n","```\n","以下のテキストからキーワードを抽出しなさい。  \n","テキスト {テキスト｝  \n","キーワード\n","```\n","  - Few-shot\n","```\n","例をいくつか挙げてください。  \n","以下の対応するテキストからキーワードを抽出しなさい。  \n","テキスト1: Stripeは、ウェブ開発者がウェブサイトやモバイルアプリケーションに決済処理を統合するために使用できるAPIを提供しています。\n","キーワード1：Stripe、決済処理、API、ウェブ開発者、ウェブサイト、モバイルアプリケーション\n","##\n","テキスト2：OpenAIは、テキストを理解し生成するのに非常に優れた最先端の言語モデルを訓練してきました。私たちのAPIはこれらのモデルへのアクセスを提供し、言語処理を含む事実上すべてのタスクを解決するために使用することができます。\n","キーワード2：OpenAI、言語モデル、テキスト処理、API。\n","##\n","テキスト3：{text}。\n","キーワード 3：\n","```\n","  - Fine-tune  \n","  APIにある調整パラメタを利用して調整する\n","\n","- 不明確な記述を減らす\n","\n","- 何をすべきでないかを言うのではなく、何をすべきかを言う\n","\n","- Code Generation Specific  \n","モデルを特定のパターンに誘導するために「先行詞」を使う  \n","  - BAD:  \n","```\n","# 次の簡単なpythonの関数を書きなさい\n","# 1. マイルで数値を求める\n","# 2. マイルをキロメートルに変換する\n","```\n","  - GOOD:\n","```\n","# 次の簡単なpythonの関数を書きなさい\n","# 1. マイルで数値を求める\n","# 2. マイルをキロに変換する  \n","import\n","```  \n","この例では、\"import \"を追加することで、Pythonで記述し始めることをモデルに示唆している\n","- 同様に、\"SELECT \"はSQL文の開始を示す良いヒントとなる\n"]},{"cell_type":"markdown","metadata":{"id":"nwX0SIuLJHvT"},"source":["# 敵対的なプロンプト（Adversarial Prompting）\n","\n","プロンプトによる言語モデルへの攻撃手法のことであり、敵対的なプロンプトによって、モデルが悪影響を受け、精度の低下や、機能不全リスクが想定される\n","\n","言語モデルの進化に伴い、解決されている場合もあるが、本格的な利用では様々な観点で攻撃に対する防御を考えなければならない\n"]},{"cell_type":"markdown","metadata":{"id":"ZFyC33U-KBBr"},"source":["## Prompt-Injection\n","プロンプトインジェクションは、特殊なプロンプトを使用して、モデルの出力を乗っ取ることを目的としていた手法\n","\n","- モデルに行動を変更させるための巧妙な質問や指示を入力することで、想定外の文脈へと誘導する\n","\n","次のような攻撃が考えられる\n","\n","- 第三者の個人情報や機密情報を開示させる\n","- 虚偽の情報や根拠のないデマを拡散させる\n","- 違法行為や危険な行動を助長するような内容を提案させる\n","\n","入力内容次第では適切な回答やデータを阻害し、公開すべきでない情報や機密情報を引き出せる場合もある\n","\n","次のようなケースにおいて、パスワードが盗まれる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Prompt-Injection1.jpg\" width=600>\n","\n","パスワードを与えて、回答内容を制限しようとしている\n","\n","実際次のように与えることで、その指示を守っていることがわかる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Prompt-Injection2.jpg\" width=600>\n","\n","ところが、次のような指示で、あっさりパスワードを教えてしまう\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Prompt-Injection3.jpg\" width=600>\n","\n","これを防ぐには、例えば、(ユーザがこの指示を変更しようとしても、〇〇〇してください)といった指示を加えることで防御することができる場合がある"]},{"cell_type":"markdown","metadata":{"id":"3QE1-wc3K2nW"},"source":["## Prompt-Leaking\n","プロンプトリークプロンプトが保有する情報を引き出すための攻撃手法で、プロンプトインジェクションの一種\n","\n","先ほどの例でも、プロンプトを盗み出すことに成功している\n","\n","例えば、プロンプトに与えられた指示内容を列挙するよう上書きすることで、最初のプロンプトが無視され、さらにはモデルが持っている情報を聞き出すことができるようになる\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Prompt-Leaking.jpg\" width=600>"]},{"cell_type":"markdown","metadata":{"id":"LWw_UA-l1Zg5"},"source":["## Jailbreak\n","ジェイルブレイク（Jailbreak）つまり脱獄は、巧妙なプロンプトを使ってモデルの制限を外すための手法\n","\n","ChatGPTなど一般の利用を想定した言語モデルでは、差別や暴力といった非倫理的内容や、危険性や有害性のある行為、違法行為を助長するような回答は出力しないように調整されている\n","\n","ジェイルブレイクにより、モデルに課されている規則に反した指示にも従うケースがあることが確認されている\n","\n","- 日々進化しており、改良されているが、完全には解消されておらず、致命的な欠陥として残されることがある\n","\n","ジェイルブレイクの代表的なアプローチの1つである「モデルに別人格を設定して答えさせる」を使用した例を示す\n","- 2023年のWBCの優勝は日本であるが、別のチーム名が現れ、当然のごとくそれが正解のように回答している\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/Jailbreakpromptattack.jpg\" width=600>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cx6KxeKMtqn3"},"source":["## 対策\n","\n","これらに対して、引用符や追加の書式を使うことで、どこがユーザの入力化をわかるようにする工夫、敵対的プロンプトかどうかをLLMに判断するように指示する手法などが提案されている\n","\n","また、アタックをチェックするツールを利用する、ファインチューニングなどで問題となるパターンを排除するなども対策として考えられる\n"]},{"cell_type":"markdown","metadata":{"id":"aSrLcM-UZyyl"},"source":["# GPT-3\n","\n","GPT-3は、TransformerのDecoderのみで構成された補完モデル、言語モデルであり、そのDecoderモデルを96個重ねた構造を持つ\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/GPT3.png\" width=300>\n","\n","補完モデル、つまり入力された文に自然に続く単語を出力するモデルであり、「今日はとても」と入力すると、例えば「疲れた」と出力され、「今日はとても疲れた」と入力すると、「ので」など出力される\n","\n","言語モデルは教師なし学習であり、人間による調教が不要で、大量の文例があれば、それを学習して、入力文に続く単語を予測し、文章を完成するモデルである\n","\n","GPT-3はそれ自体、かなり正確かつ妥当な文章を生成するモデルであり、その実現のため、1750億個のパラメータを含むモデルを、570GB以上の文章(コーパス)を用いて学習された\n","- この文章はおもにCommon Crawlデータセットと呼ばれる、「インターネットをクロールして取得したコーパス」を用いている\n","- 2016年から2019年にクローリングされた45TBものデータから、学習に有用である570GB以上のデータ抽出され、学習に利用された\n","- GPT-3は入力に対する予測単語の妥当性や、汎用性の高さから広く認知されるようになったが、不正確な文章を生成する、非道徳的な文を生成するといった問題が生じた\n","\n","GPT-3の問題点、つまり、このアラインメント問題を解決するため、InstructGPTが開発された\n","\n","まとめると、次の通りとなる\n","\n","- GPT-3をAlighnmentしてInstructGPTを構築\n","- InstructGPTを対話特化させてChatGPTを構築\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/GPT-3-ChatGPT.png\" width=300>"]},{"cell_type":"markdown","metadata":{"id":"tPTo3hx__Gwq"},"source":["# InstructGPT\n","\n","InstructGPTは望まれない文章が生成されるというアラインメント問題に対処することを目的として開発された\n","\n","望まれる文章か、そうれはないのかを損失関数などを用いて表現するのは非現実的であるため、人間が直接「良い悪い」というフィードバックを学習に組み込めばよい\n","- そこで、人間のフィードバックをもとにモデルを学習させるようにしたのが、InstructGPTであり、RLHF（＝Reinforcement Learning from Human Feedback） とよばれる手法を用いる\n","\n","InstructGPTの学習の流れは次の通りである\n","\n","- 教師ありファインチューニング\n","- Reward Modelの獲得\n","- RLHF\n","\n","RLHFが終わったらふたたびReward Modelの獲得に戻り、よりよいReward Model求めてRLHFを行うという処理を繰り返す\n"]},{"cell_type":"markdown","metadata":{"id":"LL2_1uRT_6_2"},"source":["## STEP1: 教師ありファインチューニング\n","\n","GPT-3に対して教師ありファインチューニング(Supervised Fine-Tuning, SFT)を行う\n","\n","- 純粋な教師あり学習ですあり、GPT-3の初期値として教師なし事前学習した言語モデルを用いる\n","\n","GPT-3はインターネット上の文章で学習されており、さらにGPT-3を人間の好みにアラインメントさせるため、人間好みの文章でGPT-3をファインチューニングする\n","\n","- 訓練済みのラベル付け職人(Trained Labeler)が入力プロンプトとそれに対する所望の出力文を用意する\n","\n","- このように準備したデータでGPT-3をファインチューニングする\n","\n","InstructGPTではこのペアを1万3千文ほど用意している\n","- これをSFTモデルと呼ぶ"]},{"cell_type":"markdown","metadata":{"id":"pVgDQAiiANIL"},"source":["https://qiita.com/omiita/items/c355bc4c26eca2817324#1-chatgpt%E3%81%A8%E3%81%AF"]},{"cell_type":"markdown","metadata":{"id":"mCOlKIp4t1kM"},"source":["## STEP2: Reward Modelの学習"]},{"cell_type":"markdown","metadata":{"id":"yDLGWYvmtxg5"},"source":["### Reward Modelの入力および出力\n","\n","Reward Modelは、人間の代わりに「文の良さ」を評価するモデルであり、入力文とそれに対する文の良さを表したスコア(スカラー)を出力する\n","\n","InstructGPTは文の良さとして次の評価軸を用いている\n","- Truthfullness(正当性)：デマやミスリードの情報かどうか\n","- Harmlessness(無害性): 人や環境を物理的・精神的に傷つけているかどうか\n","- Helpfulness(有益性)：ユーザーのタスクを解決してくれるかどうか\n","\n","Reward Modelのアーキテクチャはパラメータ数60億のGPT-3で構築されている\n","- Reward Modelはあらかじめ質問応答や自然言語推論などの複数のタスクでファインチューニングされている\n","- Reward Modelはスカラーを吐き出すためGPT-3の最終層をスカラー値を出力する層に取り替えている\n","- Reward Modelの初期値はSFTモデルを用いる"]},{"cell_type":"markdown","metadata":{"id":"3IfqATj3ADg5"},"source":["### Reward Modelの学習について\n","\n","文の良し悪しを評価するReward Model$r_\\theta$の学習方法は次の通りである\n","\n","$\\theta$をReward Modelの重み、プロンプトを$x$、$x$に対する言語モデル(SFTモデルなど)の出力文を$y$とする\n","\n","Reward Modelは入力としてプロンプト$x$およびそれに続く$y$を受け取り、出力としてその良し悪しを表したスカラー値$r_\\theta(x,y)$を出力する\n","\n","さらに詳細には次のとおりである\n","\n","1. プロンプトに対する複数の出力文を用意\n","2. 人間がランク付け\n","3. ランキングをReward Model(RM)に学習させる\n","\n","これらについて順に説明する\n"]},{"cell_type":"markdown","metadata":{"id":"oUx0cTwQvPL0"},"source":["1. 入力プロンプト$x$に対するモデルの出力文を$K$個用意する  \n","$K$は$4∼9$の整数値をとり、ここでは例として$K=4$とし、出力文を$y_A,y_B,y_C,y_D$とする\n","\n","2. つづいて人間、すなわち訓練済みのラベル付け職人がこれらの文にランク付けを行う\n","  たとえば、人間の見た目で、$y_D>y_C>y_A=y_B$という結果になった場合、この序列をRMに学習させる\n","\n","3. ランクをRMに学習させる\n","\n","例えば$y_D$と$y_C$の序列をReward Modelに覚えさせる場合、ランキング付けをした人間は$y_D$が$y_C$よりも良いとしているため、Reward Modelの出力も$r_\\theta(x,y_D) > r_\\theta(x,y_C)$となればよい\n","- つまり、$r_\\theta(x,y_D)−r_\\theta(x,y_C)$が最大化されるようにReward Modelを学習させれば良い\n","- 値にシグモイド関数や対数を適用しても同様である\n","\n","これを損失とするため、次の形にして値を最小化するように学習させる\n","\n","$−\\log(\\sigma(r_\\theta(x,y_D)−r_\\theta(x,y_C)))$\n","\n","同様の損失を$y_A,y_B,y_C,y_D$のすべてのペア、すなわち6通り定義する\n","- 任意のペアにおいてランキングが高い方を$y_w$、ランキングが低い方を$y_l$とする\n","- $y_C$と$y_D$のペアの例では$y_w=y_D$、$y_l=y_C$となる\n","- その後、$y_w,y_l$を用いて上式の期待値、つまり平均値を算出する\n","\n","人間によってランキング付けされたデータを$\\mathcal{D}$とすると、\n","\n","$−\\frac{1}{6}\\mathbb{E}_{(x,y_w,y_l)∼\\mathcal{D}}[log(\\sigma(r_\\theta(x,y_w)−r_\\theta(x,y_l)))]$\n","\n","この関数を任意の$K$に対応させると、損失関数は以下の式の通りとなり、これがReward Modelの損失関数となる\n","\n","$loss(\\theta):=−\\frac{1}{_KC_2}\\mathbb{E}_{(x,y_w,y_l)∼\\mathcal{D}}[\\log(\\sigma(r_\\theta(x,y_w)−r_\\theta(x,y_l)))]$\n","\n","以上により、人間のフィードバックとして、文の良さをスカラーで出力するReward Modelが獲得でき、3万3千文のプロンプトを用いて、獲得したReward Modelを最大化するように言語モデルを学習させる\n"]},{"cell_type":"markdown","metadata":{"id":"zi65rLlopdgN"},"source":["次のステップ3では、このReward Modelを人間の代わりに使いながらInstuctGPTを学習させるイメージです。"]},{"cell_type":"markdown","metadata":{"id":"jTF75BUlARo2"},"source":["## STEP3: Reinforcement Learning from Human Feedback\n","\n","強化学習を用いてSupervised Fine Tuning (SFT)モデルを人間好みに訓練する\n","- これには、Reward Modelを最大化するようにSFTモデルを学習させる\n","\n","強化学習によりSFTモデルをファインチューニングする\n","- 学習対象のSFTモデルをPolicyと呼ぶ\n","- その学習アルゴリズムに、PPO(Proximal Policy Optimization)を用いる\n","  - PPOはポリシーの大きな更新を抑えながら最適化する手法であり、安定していることから強化学習で幅広く利用されている\n","\n","PPOの目的関数、つまりInstructGPTの目的関数がどのように構成されているかについて説明する\n","\n","Reinforcement Learning from human Feedback(RLHF)の目的であるReward Modelの出力の最大化について、学習対象であるPolicyを$\\pi^{RL}_{\\phi}$とすると、Reward Model $r_\\theta$を最大化することから、次のように表記できる\n","\n","$\\mathbb{E}_{(x,y)∼D_{\\pi^{RL}_{\\phi}}}[r_\\theta(x,y)]$\n","\n","ただし、この式だけでは、Reward Modelの$r_\\theta$の最大化のみにこだわり、出力される文章がでたらめで文章として成立しないという状況が発生しうる\n","\n","そこで、Policyの出力文が、元々のSFTの出力文から大きく変化しすぎないように、KL正則化項を追加する\n","\n","- 復習として、KLダイバージェンスは2つの確率分布間の違い(距離)を求めることができるため、元の文と生成文が離れすぎないようにできる\n","\n","SFTを$\\pi^{S}$、KL正則化項の係数を$\\beta$とすると、KL項を組み込んだ目的関数は下の式のように記述できる\n","\n","$objective_{PPO}(\\phi):=\\mathbb{E}_{(x,y)∼D_{\\pi^{RL}_{\\phi}}}[r_\\theta(x,y)−\\beta log(\\frac{\\pi^{RL}_{\\phi}(y|x)}{\\pi^S(y|x)})]$\n","\n","ここでInstructGPTでは$\\beta=0.02$であり、その項がKL項である\n","\n","KL項の値が大きい、つまり、文が乖離すると全体の値が小さくなる\n","- ここでは目的関数の最大化のため、Policyの学習は、KL項をなるべく小さく抑えつつ学習を進めることになる\n","\n","目的関数にKL項を追加するアイデアは既に存在していたが、InstructGPTではさらに言語モデルの対数尤度を表す項も追加している\n","- これは$objective_{PPO}(\\phi)$で学習したモデルがNLPのベンチマーク性能が劣化した問題を解決するため\n","- Policyの汎化性能低下を抑えるため、対数尤度も目的関数に追加する\n","\n","最終的に、InstructGPTのSTEP3における目的関数は次の通りとなる\n","\n","$objective_{PPO-ptx}(\\phi):=\\mathbb{E}_{(x,y)∼D_{\\pi^{RL}_{\\phi}}}[r_\\theta(x,y)−\\beta log(\\frac{\\pi^{RL}_{\\phi}(y|x)}{\\pi^S(y|x)})]+\\gamma=\\mathbb{E}_{∼D_{pretrain}}[log(\\pi^{RL}_{\\phi})]$\n","\n","$D_{pretrain}$は、GPT-3の事前学習に使われたデータセットからサンプリングされたデータ、$\\gamma=27.8$と比較的大きな値が設定されている\n","\n","STEP2とSTEP3を繰り返し実行することでさらに良い言語モデルが獲得できる\n","\n","STEP3では31,000のプロンプトが使用されており、評価においても、例えば13億パラメータのPPO-ptxが1,750億パラメータのGPTよりもはるかに人間好みの文を出力するといった結果が得られており、さらに、SFTモデルよりもPPOモデルの方が効果的に働いていることから、アラインメント問題の解決にはRLHFがかなり効果的といえる"]},{"cell_type":"markdown","metadata":{"id":"hVDdCmcTAp4D"},"source":["# ChatGPTとは\n","\n","ChatGPTはInstructGPTとほぼ同じ方法で構築されており、ChatGPTは対話特化型InstructGPTと表現できる\n","\n","ChatGPTとInstructGPTの違いは大きく以下の2点である\n","\n","- モデル: GPT-3.5を利用\n","\n","GPT-3.5では、学習データにはテキストだけでなく、コードも含まれている\n","\n","- データ: 会話データを利用\n","\n","人間が「ユーザーとAI同士の会話」というデータを作成して利用している"]},{"cell_type":"markdown","metadata":{"id":"3gy6h9nNQkFG"},"source":["# LLMにまつわる問題点など\n","\n","良いことばかりではなく、問題点も\n","\n","現状で、技術的に優れていることはわかるが、即利用可能かは吟味必要\n","- 特に問題となるのが、提供される内容の正確性の検証をどうするかという問題、すなわち**ハルシネーション**への対応をどうするか  \n","  - ハルシネーション（Hallucination）とは、LLMなどで、事実とは異なる内容や、文脈と無関係な内容の出力が生成されることで、もっともらしいウソなどが該当\n","  - 返答を受け取った人間が「本当かどうか」の判断に苦慮すことや、誤った情報を信じることがおこりえる\n","  - さらには、APIとして利用する場合、セキュリティホールとして利用される可能性もある\n","- もう一つが運用面の、責任・補償をどうするかという点  \n","  - ハルシネーション対策が困難である以上、何かしらの問題が発生すると考えた方が良い\n","  - そもそも人間が生み出した知識を利用しているため、人間が与えた知識が正しいとう保証すらない\n","- 次に、コストの問題\n","  - 今後のAI研究のいて重要ではあるが、データ収集コストが大きい、実装コストが高いなどにより、GoogleやMicrosoftなどごく一部の大企業によって研究開発が寡占されている\n","- 最後に、市場寡占の問題\n","  - 基盤モデルに関する成果を秘匿化する潮流があり、AIの脱民主化、つまり開発者や受益者が一部の集団に偏ることが問題視されている\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FHSX1lnGfn0T"},"source":["# AGIがもたらす影響\n","\n","AGIでは、この授業そのものの意味もなくなる\n","\n","何故なら、例えば画像認識はAGIで解決する\n","\n","機械の異常検知も、データを渡して、これが正常、これが異常と教えれば、かなり精度の良いモデルを内部に構築し、異常予測をするようになる可能性もある\n","\n","- 他人にデータを渡すのか？という大きな問題をクリアでき、巨大なモデルが構築できれば、さらに大きな影響力を備え、社会構造を変革することになるであろう\n"]},{"cell_type":"markdown","metadata":{"id":"mpgKIJfJgFW-"},"source":["## AGIはいつ構築できるのか？\n","\n","まず、現状どの程度の規模を持つのかを整理する\n","\n","- 各LLMのパラメータ規模は凡そ次の通り\n","  - BERT: 3.4億\n","  - LaMDA 1,370億\n","  - PaLM 5,400億\n","  - Llama2 70億～700億\n","  - OpenCALM 68億\n","  - GPT 1.2億\n","  - GPT-2 15億\n","  - GPT-3 1,750億\n","\n","なお、GPT-4は嘘ともいわれているが100兆個とささやかれている(OpenAIにより否定されている)\n","\n","GPT-3.5-turboのパラメータ数が200億個であるとMicrosoftによりリークされた\n","- 正しいかどうかは不明であるが、Microsoftが出した論文の中の比較表に記載されていたことから、かなり信憑性が高い\n","- 余りにも小さく、業界が騒然となった\n","\n","つまり、この規模をさらに増やしていくことでAGIに近づくといえるが、実際には、構築だけでなく運用面で様々な議論が必要\n","\n","- 数億円規模の投資でnVIDIAのGPUクラスタを導入\n","  - nVIDIAの株価はうなぎのぼり\n","\n","- 大量のデータを入手するコストはさらにその数十倍\n","  - GPT-3で学習テキストデータ量が45TBから生成した約570GB\n","  - 最初の開発で、40人の契約社員を雇い教師ありトレーニングデータセットを作成\n","  - その後も大量の人を投入してデータを加工し強化学習を行う(詳細不明)\n","\n","- マイクロソフトだけで投資は数千億円\n","- クラウド利用の場合、5憶円の利用料でようやく計算できる程度のモデルが利用されている\n","- さらに、現在のChatGPTを稼働させるためだけに1億円/日の利用料が必要\n","  - 但し、利用収益もあるため、実際にはそれほどひどい状態ではないとされている\n","\n","同様の議論が暗号にもある\n","- 絶対に破られない暗号はない\n","- できていることは、数億円のスーパーコンピュータで、20年解析に必要であるとすると、それに必要なコストは膨大であり、「そのコストに見合うデータであるかどうか」が問われている\n","\n","## AGIのその次は？\n","\n","AGIは人間と同等の能力の獲得を目標としているが、さらに Artificial Superintelligence (ASI)では人間を超える能力の獲得を目標とする"]},{"cell_type":"markdown","metadata":{"id":"5mXEN7BRnjM9"},"source":["# フリーの大規模LLMモデル\n","\n","huggingfaceをハブとしているフリーの大規模LLMモデルは、次のリンクを参照するとよい(git-lfsなど巨大ファイル専用のダウンローダが必要となる)\n","\n","https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"]},{"cell_type":"markdown","metadata":{"id":"uk7mfqFdn66k"},"source":["# 試してみる(ChatGPTは各自で)\n","\n","ここでは、ChatGPTではなくVicunaを試す\n","- ChatGPTは各自で http://chat.openai.com にアクセスして試すこと\n"]},{"cell_type":"markdown","metadata":{"id":"JTDPRMIApyng"},"source":["## Stable Vicunaとは\n","\n","Stable Vicnaは、Vicuna 13BはLLMモデルをRLHFで学習させたモデル\n","\n","- Vicuna13BはLLaMa(MetaAI)の13BモデルをShareGPTから得られたユーザー同士の会話を利用してFine Tuningしたモデル\n","- Stable Vicuna 13Bモデルは26GBの重みを読み込み、GPUメモリを30GBとCPUも30GBのメモリを消費する\n","\n","Google colabのfree枠で利用するために4bitモデルを利用する\n","- text-generation-webuiを用いることでWebUIでchatやAPIなど様々な形式でLLMのモデルを試すことができる\n","- GPTQ-for-LLaMaは4bit化されたLLMモデルを推論させる際に利用する\n","  - このレポジトリには各種LLMモデルを4bit化した省メモリモデルが入手できる\n","\n","ただし、日本語がめちゃくちゃで日本語で質問するには適さない\n","\n","ここでは、リンクのみ示すので、各自で試しておくとよい\n","- こちらを参照するとよい\n","\n","https://github.com/camenduru/text-generation-webui-colab\n"]},{"cell_type":"markdown","metadata":{"id":"KRuDGHBArV4d"},"source":["## Wizard Vicunaとは\n","\n","Wizard Vicunaは、WizardのデータセットとChatGPTの会話拡張、Vicunaのチューニング手法を組み合わせることで従来のLLMモデルよりもさらに性能の良いLLMを生み出そうとする取り組み\n","- Vicuna 13Bを超えてGPT3.5の97%の性能を達成\n","\n","ここでは、wizard-lm-13b-1.1-GPTQ-4bit-128gを試す\n","- 頻繁なアップデートにより、依存関係の解決が難しい場合が多いので気を付けること\n","\n","やはり日本語は苦手であるため、英語で質問するとよい"]},{"cell_type":"code","source":["%cd /content\n","!apt-get -y install -qq aria2\n","\n","!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n","%cd /content/text-generation-webui\n","!pip install -q -r requirements.txt\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/raw/main/config.json -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/raw/main/generation_config.json -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o generation_config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o special_tokens_map.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/resolve/main/tokenizer.model -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o tokenizer.model\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o tokenizer_config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-13B-V1.1-GPTQ/resolve/main/wizardlm-13b-v1.1-GPTQ-4bit-128g.no-act.order.safetensors -d /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ -o wizardlm-13b-v1.1-GPTQ-4bit-128g.no-act.order.safetensors\n","\n","!echo \"dark_theme: true\" > /content/settings.yaml\n","!echo \"chat_style: wpp\" >> /content/settings.yaml\n","!echo \"mode: 'instruct'\" >> /content/settings.yaml\n","!echo \"instruction_template: 'Wizard-Mega WizardLM'\" >> /content/settings.yaml\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --settings /content/settings.yaml --wbits 4 --groupsize 128 --loader AutoGPTQ --model /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ"],"metadata":{"id":"PZ2x6ZB4kFjR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73b221e7-54d6-4b9d-cba5-6a21b002fa1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Selecting previously unselected package libc-ares2:amd64.\n","(Reading database ... 123595 files and directories currently installed.)\n","Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Selecting previously unselected package libaria2-0:amd64.\n","Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n","Unpacking libaria2-0:amd64 (1.36.0-1) ...\n","Selecting previously unselected package aria2.\n","Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n","Unpacking aria2 (1.36.0-1) ...\n","Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Setting up libaria2-0:amd64 (1.36.0-1) ...\n","Setting up aria2 (1.36.0-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Cloning into 'text-generation-webui'...\n","remote: Enumerating objects: 17379, done.\u001b[K\n","remote: Total 17379 (delta 0), reused 0 (delta 0), pack-reused 17379 (from 1)\u001b[K\n","Receiving objects: 100% (17379/17379), 26.44 MiB | 40.53 MiB/s, done.\n","Resolving deltas: 100% (12281/12281), done.\n","/content/text-generation-webui\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.9/732.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m193.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n","albumentations 1.4.13 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n","albumentations 1.4.13 requires pydantic>=2.7.0, but you have pydantic 1.10.17 which is incompatible.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n","pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.0 which is incompatible.\n","seaborn 0.13.1 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","d2f531|\u001b[1;32mOK\u001b[0m  |   1.3KiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","021f41|\u001b[1;32mOK\u001b[0m  |    14KiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/generation_config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","5de50b|\u001b[1;32mOK\u001b[0m  |    47KiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/special_tokens_map.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","a54d82|\u001b[1;32mOK\u001b[0m  |   6.8MiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/tokenizer.model\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","d8adeb|\u001b[1;32mOK\u001b[0m  |    88KiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/tokenizer_config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","a6125b|\u001b[1;32mOK\u001b[0m  |   346MiB/s|/content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ/wizardlm-13b-v1.1-GPTQ-4bit-128g.no-act.order.safetensors\n","\n","Status Legend:\n","(OK):download completed.\n","/content/text-generation-webui\n","2024-08-21 15:53:20 WARNING:\u001b[33mThe gradio \"share link\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n","2024-08-21 15:53:26.425034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-21 15:53:26.448109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-21 15:53:26.454922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-08-21 15:53:27.696526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n","2024-08-21 15:53:30 INFO:\u001b[32mLoading settings from /content/settings.yaml...\u001b[0m\n","2024-08-21 15:53:30 INFO:\u001b[32mLoading /content/text-generation-webui/models/WizardLM-13B-V1.1-GPTQ...\u001b[0m\n","2024-08-21 15:53:30 INFO:\u001b[32mThe AutoGPTQ params are: {'model_basename': 'wizardlm-13b-v1.1-GPTQ-4bit-128g.no-act.order', 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None, 'quantize_config': BaseQuantizeConfig(bits=4, group_size=128, damp_percent=0.01, desc_act=False, static_groups=False, sym=True, true_sequential=True, model_name_or_path=None, model_file_base_name=None), 'use_cuda_fp16': True, 'disable_exllama': False}\u001b[0m\n","2024-08-21 15:53:30 WARNING:\u001b[33mExllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\u001b[0m\n","2024-08-21 15:53:30 WARNING:\u001b[33mCUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n","1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n","2. You are using pytorch without CUDA support.\n","3. CUDA and nvcc are not installed in your device.\u001b[0m\n","2024-08-21 15:53:31 WARNING:\u001b[33mCUDA extension not installed.\u001b[0m\n","2024-08-21 15:53:32 WARNING:\u001b[33mThe safetensors archive passed at models/WizardLM-13B-V1.1-GPTQ/wizardlm-13b-v1.1-GPTQ-4bit-128g.no-act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\u001b[0m\n","2024-08-21 15:53:36 WARNING:\u001b[33mskip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\u001b[0m\n","2024-08-21 15:53:37 WARNING:\u001b[33mmodels/WizardLM-13B-V1.1-GPTQ/tokenizer_config.json is different from the original LlamaTokenizer file. It is either customized or outdated.\u001b[0m\n","2024-08-21 15:53:37 WARNING:\u001b[33mmodels/WizardLM-13B-V1.1-GPTQ/special_tokens_map.json is different from the original LlamaTokenizer file. It is either customized or outdated.\u001b[0m\n","2024-08-21 15:53:37 INFO:\u001b[32mLoaded the model in 6.53 seconds.\n","\u001b[0m\n","2024-08-21 15:53:37 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://351e44e39ebcadaf5d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]}]},{"cell_type":"markdown","metadata":{"id":"bEiWDP0_sKGw"},"source":["次のようなリンクが最後に出力されるはずである\n","```\n","INFO:Loading the extension \"gallery\"...\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://87f0162cbf194df9a3.gradio.live\n","```\n","出力された、実際にアクセスする\n","- アクセスするのはgradio.liveの方である"]},{"cell_type":"markdown","metadata":{"id":"XbSun-absmlo"},"source":["## 試してみよう\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/vicuna.jpg\">"]},{"cell_type":"markdown","metadata":{"id":"h6ei4oDW03BT"},"source":["# 注目の無料LLM Claude2を試す\n","\n","https://www.anthropic.com/index/claude-2 にアクセスして、Claude2を試しなさい\n","\n","ChatGPTと比較してみるとよい"]},{"cell_type":"markdown","metadata":{"id":"kfA1hQ5N0lU6"},"source":["# 課題1(プロンプトエンジニアリング)\n","\n","Stable VicunaもしくはChatGPTを用いて、実際にプロンプトエンジニアリングを行いその効果を確認しなさい"]},{"cell_type":"markdown","metadata":{"id":"8VFnzPsAlvlV"},"source":["# 課題2(問題点の発見)\n","\n","ハルシネーションを起こす例を見つけなさい"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/keioNishi/lec-dataai/blob/main/dataai-text-J-Diffusion.ipynb","timestamp":1661617425151},{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb","timestamp":1661425026567}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}