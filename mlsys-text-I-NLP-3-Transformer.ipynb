{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1F4ifdHNnrAUsDv2pj0EXDImHsM_x_E9M","timestamp":1628197853609}],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ebdc7bfc6ef940a6b1c9ea08e15b9cc4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18f85adf01e74ab7bc8762f904353774","IPY_MODEL_263013922a4048d3b9da2696b03ab864","IPY_MODEL_d426495bca084f37935dee467001882d"],"layout":"IPY_MODEL_c354b60282334d2b8d51adee1a6acbb4"}},"18f85adf01e74ab7bc8762f904353774":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab13c1f3149942308e33f7b360aee3ac","placeholder":"​","style":"IPY_MODEL_257ff08d9a444110a58dde708afe806a","value":"tokenizer_config.json: 100%"}},"263013922a4048d3b9da2696b03ab864":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_823cbf48541a4b9ebcbd97ee6284d409","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e8e8a0de36843ce840916773b61a557","value":48}},"d426495bca084f37935dee467001882d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e21cd4c5d8f42f980d0f5b818a27ed0","placeholder":"​","style":"IPY_MODEL_70d863b0693645de832c2ebedc32d8b8","value":" 48.0/48.0 [00:00&lt;00:00, 2.03kB/s]"}},"c354b60282334d2b8d51adee1a6acbb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab13c1f3149942308e33f7b360aee3ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"257ff08d9a444110a58dde708afe806a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"823cbf48541a4b9ebcbd97ee6284d409":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8e8a0de36843ce840916773b61a557":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e21cd4c5d8f42f980d0f5b818a27ed0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d863b0693645de832c2ebedc32d8b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608f990886d94c1c80c4543d5795b6ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3ed9ef6b2094720aa2258ac3d84f05a","IPY_MODEL_34c811082da446f18ffb11cf6e3d4754","IPY_MODEL_85eb97bbfd954d6caf95bf5e18463738"],"layout":"IPY_MODEL_8a0ac92777e94e61ac446b7a82097c3d"}},"e3ed9ef6b2094720aa2258ac3d84f05a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_461692199a2342d9a3dbea4ebc3cf1b2","placeholder":"​","style":"IPY_MODEL_d05a3ca545674ed6857ba0e5578c36fc","value":"config.json: 100%"}},"34c811082da446f18ffb11cf6e3d4754":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b6cea64714c4a598f4f039b11ec8aeb","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ffd642c2b3a4da9b2f9efd6c1dd7367","value":570}},"85eb97bbfd954d6caf95bf5e18463738":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f88c01d66804dbe82c7e796269f94aa","placeholder":"​","style":"IPY_MODEL_0257b592756446ecbf847c36982ef7ab","value":" 570/570 [00:00&lt;00:00, 13.8kB/s]"}},"8a0ac92777e94e61ac446b7a82097c3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"461692199a2342d9a3dbea4ebc3cf1b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d05a3ca545674ed6857ba0e5578c36fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b6cea64714c4a598f4f039b11ec8aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ffd642c2b3a4da9b2f9efd6c1dd7367":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f88c01d66804dbe82c7e796269f94aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0257b592756446ecbf847c36982ef7ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"074bd885b4a948088627e3e47e286457":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4406f1ff829644ce8488fa3f0445ecd7","IPY_MODEL_f7f23c4c0139406cb3562f865caf39e8","IPY_MODEL_e422e1df77ed47b08e2c309dc4a90cc8"],"layout":"IPY_MODEL_8f0ab117a9ef4f30bee64dbedb12ac68"}},"4406f1ff829644ce8488fa3f0445ecd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68a17556145a46e8a53fa1e2f4e0c65b","placeholder":"​","style":"IPY_MODEL_cc77b98c5650498d9d2244ad31f57800","value":"vocab.txt: 100%"}},"f7f23c4c0139406cb3562f865caf39e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0c5a8db4546478e9c9da16d0c5236ad","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad1ba9c41d904caf97a071d257a50b75","value":231508}},"e422e1df77ed47b08e2c309dc4a90cc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a48efe28023344a883ec9b52bde51249","placeholder":"​","style":"IPY_MODEL_efb740f176704877b5f3b5a464567ea5","value":" 232k/232k [00:00&lt;00:00, 233kB/s]"}},"8f0ab117a9ef4f30bee64dbedb12ac68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a17556145a46e8a53fa1e2f4e0c65b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc77b98c5650498d9d2244ad31f57800":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0c5a8db4546478e9c9da16d0c5236ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad1ba9c41d904caf97a071d257a50b75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a48efe28023344a883ec9b52bde51249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb740f176704877b5f3b5a464567ea5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1e296ad72a241b2947866cf3276cd0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_663164f98e604712a61459b9a408a7f6","IPY_MODEL_3a1078bff3df4233b75a189d37ae52e5","IPY_MODEL_c90db5113b9d4e9185b51bee0a091f51"],"layout":"IPY_MODEL_748f6d1a17094b5bbf57a9ac087e982e"}},"663164f98e604712a61459b9a408a7f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b73a7149f1a41cbaedbaf89789f7142","placeholder":"​","style":"IPY_MODEL_f1b9d6abfaaf4b96b33f4248be9a4a06","value":"tokenizer.json: 100%"}},"3a1078bff3df4233b75a189d37ae52e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f5db5b00122498d9b1a26eee5484d8b","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d52b68712004c6caa6144fe45c9dd57","value":466062}},"c90db5113b9d4e9185b51bee0a091f51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46680fb658d8436cba3fa7452ab8d85c","placeholder":"​","style":"IPY_MODEL_9660f0f77a7b45bbb43abdb0fdd8a50c","value":" 466k/466k [00:00&lt;00:00, 781kB/s]"}},"748f6d1a17094b5bbf57a9ac087e982e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b73a7149f1a41cbaedbaf89789f7142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1b9d6abfaaf4b96b33f4248be9a4a06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f5db5b00122498d9b1a26eee5484d8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d52b68712004c6caa6144fe45c9dd57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46680fb658d8436cba3fa7452ab8d85c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9660f0f77a7b45bbb43abdb0fdd8a50c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"609e698f0db641bba9c52dff4e5d3fc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9704bd3b3b842c08bf09205fba033b2","IPY_MODEL_a7492a2b54214c308610fcd52aa6591c","IPY_MODEL_8614679f4aef49608c5f4713d0f1c7af"],"layout":"IPY_MODEL_6d55f24cb45f4f108d5cd02bafab8503"}},"f9704bd3b3b842c08bf09205fba033b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_594d5bccc81b47abb9abdb8517b1c337","placeholder":"​","style":"IPY_MODEL_96a411e285a04508a4bcc409a8d52efc","value":"Downloading: 100%"}},"a7492a2b54214c308610fcd52aa6591c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09196f3a0c7e422fb72a43e394208223","max":4556,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e26f52520ba46dc9e3e49bc2c88419f","value":4556}},"8614679f4aef49608c5f4713d0f1c7af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_994938bef80d4b60be7812a9fc9678f3","placeholder":"​","style":"IPY_MODEL_a55ba8b889aa40c1922386070aae2026","value":" 4.56k/4.56k [00:00&lt;00:00, 123kB/s]"}},"6d55f24cb45f4f108d5cd02bafab8503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"594d5bccc81b47abb9abdb8517b1c337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a411e285a04508a4bcc409a8d52efc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09196f3a0c7e422fb72a43e394208223":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e26f52520ba46dc9e3e49bc2c88419f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"994938bef80d4b60be7812a9fc9678f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55ba8b889aa40c1922386070aae2026":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dbf6ef1329f45de8b454c47c5acdc8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cff47a16ea6b443eae3ea43ed28340ea","IPY_MODEL_07d0c67d4a2344e39616c2986364abf9","IPY_MODEL_9011d054db604ae3b8f824e867dd8882"],"layout":"IPY_MODEL_f25b17b1ced5471a9d48d9b1efc5f520"}},"cff47a16ea6b443eae3ea43ed28340ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb5bb6571a0d4be18e7669138db4a311","placeholder":"​","style":"IPY_MODEL_680e631dc02647f8a3ebe1f4569d1e8a","value":"Downloading: 100%"}},"07d0c67d4a2344e39616c2986364abf9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1e44c5b9f3343f4bd3a94aa70807d35","max":2071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8c4bc1a750a471985ca3c31e27c6797","value":2071}},"9011d054db604ae3b8f824e867dd8882":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a631994141b440fb4713c7fd6172357","placeholder":"​","style":"IPY_MODEL_cd4329bf72904344b64cc609371d38c6","value":" 2.07k/2.07k [00:00&lt;00:00, 26.1kB/s]"}},"f25b17b1ced5471a9d48d9b1efc5f520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb5bb6571a0d4be18e7669138db4a311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"680e631dc02647f8a3ebe1f4569d1e8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1e44c5b9f3343f4bd3a94aa70807d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c4bc1a750a471985ca3c31e27c6797":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a631994141b440fb4713c7fd6172357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd4329bf72904344b64cc609371d38c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f234c27ac152470cae9ad181523798a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75ff2f027205405aa88bec8660c6bdf2","IPY_MODEL_2b04212e31b14536a1cf48c449542d30","IPY_MODEL_5674d060cbc04b7c934331b77588f339"],"layout":"IPY_MODEL_1936d1edf2534816bf53fdfd98549af2"}},"75ff2f027205405aa88bec8660c6bdf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e6530bdde04696a768782a98882967","placeholder":"​","style":"IPY_MODEL_602793bb54bf4168a398de531c9d7e59","value":"Downloading: 100%"}},"2b04212e31b14536a1cf48c449542d30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc905919404c493aba1c9dddb40c15fe","max":84125825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcc1e5b84e694a4a83ecae73606fc5d1","value":84125825}},"5674d060cbc04b7c934331b77588f339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e18f65ba9c484290863fda701f00ad8c","placeholder":"​","style":"IPY_MODEL_1780aaa2bd53431fba7ab3a4f8cb6f4d","value":" 84.1M/84.1M [00:15&lt;00:00, 8.71MB/s]"}},"1936d1edf2534816bf53fdfd98549af2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85e6530bdde04696a768782a98882967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"602793bb54bf4168a398de531c9d7e59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc905919404c493aba1c9dddb40c15fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcc1e5b84e694a4a83ecae73606fc5d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e18f65ba9c484290863fda701f00ad8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1780aaa2bd53431fba7ab3a4f8cb6f4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f310ab30c454db1a4d90d99fdbdca5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7b684cd1503421eb0107f5cc15e5605","IPY_MODEL_31e1c3ffae5941949aec7940aee9c130","IPY_MODEL_a09afbcff7fa44528fafc00d89964696"],"layout":"IPY_MODEL_d915b36618ca4957b297d3f790e383bf"}},"a7b684cd1503421eb0107f5cc15e5605":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea96097b12434ffe9ed9655ec3c2f13c","placeholder":"​","style":"IPY_MODEL_93e2359014dd41cebea5ceca142f08ad","value":""}},"31e1c3ffae5941949aec7940aee9c130":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_c69ca9181ad74f1195ecb4e20de62df5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d7e9b3f2517474a9286efb7bcdcbf98","value":1}},"a09afbcff7fa44528fafc00d89964696":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fae6f75fb814ea499bdd40aac39a432","placeholder":"​","style":"IPY_MODEL_56be8c2ace1b4dba893ca01f66f03ac7","value":" 22752/0 [00:01&lt;00:00, 21394.75 examples/s]"}},"d915b36618ca4957b297d3f790e383bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ea96097b12434ffe9ed9655ec3c2f13c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93e2359014dd41cebea5ceca142f08ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c69ca9181ad74f1195ecb4e20de62df5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9d7e9b3f2517474a9286efb7bcdcbf98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fae6f75fb814ea499bdd40aac39a432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56be8c2ace1b4dba893ca01f66f03ac7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"295178af324f42e69ce04257e6e773be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83baf674e7e2465f87fe0d412cb887e2","IPY_MODEL_cc3fe93dda4440909facf752c0295051","IPY_MODEL_ecaa6dec8fc84b12bc0e4e2932359826"],"layout":"IPY_MODEL_acfaf9f76d2240428ce356be9d061ba0"}},"83baf674e7e2465f87fe0d412cb887e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7737155da5a441f9fbf203e72b665cb","placeholder":"​","style":"IPY_MODEL_abcc5a7c7d094ce5a670e5f4deafc9ec","value":""}},"cc3fe93dda4440909facf752c0295051":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3be20018e5e4e9da3c308ebaf5d7d09","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa198ba2d5584f3cb197f150025950f3","value":1}},"ecaa6dec8fc84b12bc0e4e2932359826":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d5f119ad258461bbab6196e4653695d","placeholder":"​","style":"IPY_MODEL_c7448387580a44dc90367c47660eb742","value":" 23510/0 [00:01&lt;00:00, 22745.34 examples/s]"}},"acfaf9f76d2240428ce356be9d061ba0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c7737155da5a441f9fbf203e72b665cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abcc5a7c7d094ce5a670e5f4deafc9ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3be20018e5e4e9da3c308ebaf5d7d09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"aa198ba2d5584f3cb197f150025950f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d5f119ad258461bbab6196e4653695d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7448387580a44dc90367c47660eb742":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50ba36c02b7e4ecc92996f0e6bdbc4c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42e8ade851c2412289bd9b2658ddb697","IPY_MODEL_73c84b3239404bf9a3113bbeb7a588ff","IPY_MODEL_e9b38bbf05f94d529ef927448794627e"],"layout":"IPY_MODEL_a5c686d61bf24c308427e0b3a9a0e3c8"}},"42e8ade851c2412289bd9b2658ddb697":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9a546787ea441d4a563b29b5837b84d","placeholder":"​","style":"IPY_MODEL_709a26e05202480690d4dceef53bde06","value":""}},"73c84b3239404bf9a3113bbeb7a588ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb97acced5cd4758b7a542511db2cbd5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c467fd6968624f1bbce7fc5c8bc19db7","value":1}},"e9b38bbf05f94d529ef927448794627e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4989afd5b82744a6b811a89d0b9db806","placeholder":"​","style":"IPY_MODEL_6d1b763197994ee7bb2867ae2652ea5e","value":" 47994/0 [00:02&lt;00:00, 25026.15 examples/s]"}},"a5c686d61bf24c308427e0b3a9a0e3c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f9a546787ea441d4a563b29b5837b84d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709a26e05202480690d4dceef53bde06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb97acced5cd4758b7a542511db2cbd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c467fd6968624f1bbce7fc5c8bc19db7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4989afd5b82744a6b811a89d0b9db806":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d1b763197994ee7bb2867ae2652ea5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8286b537bbf14ff292bf68b6272e18eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21352b52d83c41788f854597cd83782c","IPY_MODEL_d4a67e51aa514b7e99c6a58c1ddd1f13","IPY_MODEL_bd17038b86a14cc4ba3ce3d2db392edd"],"layout":"IPY_MODEL_830be51a23c6493f8efba0c3a9a7a1ee"}},"21352b52d83c41788f854597cd83782c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74e3079148954fe3a42e0fc45ddabf72","placeholder":"​","style":"IPY_MODEL_9711c35310b34163b315c996494a51dc","value":"100%"}},"d4a67e51aa514b7e99c6a58c1ddd1f13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb40d24fb7c1481686538a860be44918","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10be5b043cb14a988d19f6bf0b5bf0a2","value":25}},"bd17038b86a14cc4ba3ce3d2db392edd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_354ca5efc9974b3096ff7b5444c93ea7","placeholder":"​","style":"IPY_MODEL_60328d2b969246e897b65df5cb8cbda2","value":" 25/25 [00:39&lt;00:00,  1.41s/it]"}},"830be51a23c6493f8efba0c3a9a7a1ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74e3079148954fe3a42e0fc45ddabf72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9711c35310b34163b315c996494a51dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb40d24fb7c1481686538a860be44918":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10be5b043cb14a988d19f6bf0b5bf0a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"354ca5efc9974b3096ff7b5444c93ea7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60328d2b969246e897b65df5cb8cbda2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd2bd2d9c9e74c7693fe6b71cb569827":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d107a5603ac449cd86b4ec2e49151ea9","IPY_MODEL_fe0e1feeb2704a08b958759dc678cb63","IPY_MODEL_785a2210c6f84094965a61191c867f69"],"layout":"IPY_MODEL_3d7cd89143e74bd9a19f6d96efe442ff"}},"d107a5603ac449cd86b4ec2e49151ea9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_605b0951f83c4ed28929b295ce520fe3","placeholder":"​","style":"IPY_MODEL_8689889bacae48f6880e8244a07d4bff","value":"100%"}},"fe0e1feeb2704a08b958759dc678cb63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98dd42d0987540d99769f8f3ebb86864","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea1519c15750405399e40a2b172dd002","value":25}},"785a2210c6f84094965a61191c867f69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25acc1cdd4b4ceabe194a6c778c2362","placeholder":"​","style":"IPY_MODEL_9b0f19e44e6f4eb4b2c89442ddbcd89b","value":" 25/25 [00:37&lt;00:00,  1.43s/it]"}},"3d7cd89143e74bd9a19f6d96efe442ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"605b0951f83c4ed28929b295ce520fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8689889bacae48f6880e8244a07d4bff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98dd42d0987540d99769f8f3ebb86864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea1519c15750405399e40a2b172dd002":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a25acc1cdd4b4ceabe194a6c778c2362":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b0f19e44e6f4eb4b2c89442ddbcd89b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"gU-Arpc9zElH"},"source":["---\n",">「ドライバーが車を選ぶんじゃない。車がドライバーを選ぶんだ。人間と機械の間には、神秘的な絆があるんだ」 \\\n",">（ボビー:映画トランスフォーマーより）\n","---"]},{"cell_type":"markdown","source":["## なぜTransformerが重要なのか\n","\n","Transformerは、自然言語処理に適する方法として見出されたが、これにはどのような経緯があったのか、簡単に説明する\n"],"metadata":{"id":"_eDIB5jStfZc"}},{"cell_type":"markdown","source":["### 自然言語処理はなぜ困難なのか\n","\n","まず、言語データは画像データと本質的に扱い方が異なる\n","\n","- 画像データは画素の集まりであり、CNNでは画像全体から近隣の画素の特徴をとらえて処理する\n","- 言語データは単語を逐次的に聞いて処理する\n","  - つまり、過去の単語の入力情報を保持し、文脈を理解する必要がある\n","  - 例えば、「まいった」だけではわからないが、「失敗してまいった」「神社にまいった」となればわかる\n","\n","言語データの性質から、逐次的に処理する仕組みが必要\n","- 従来は、この性質から、RNN (Recurrent Neural Network)やLSTM (Long short-term memory)などが利用されてきた\n","  - 内部状態として過去の状態を記録することができるため\n","- 一方で、RNN、LSTMは学習時間が長く大きなモデルを構築するのが困難であった\n","  - 文章の単語を1stepに1単語ずつモデルに投入するため、バッチにより並列的に大量に処理できるCNNと異なり時間がかかる\n","\n","処理速度を稼ぐため、CNNやFCを利用する試みもなされた\n","- 処理速度は向上するが、やはり文章の離れた単語間の関係性を考慮できないため、精度の向上が困難であった\n","  - 主語と述語は文章の最初と最後であり、この主述関係を理解しようとするとCNNや勾配消失の大きいRNNでは困難であることは容易に想像できる\n","\n","RNNとは全く異なるアプローチ\n","- TransformerではEncoderおよびDecoderのいずれにもRNNのような再帰構造をもたず、Attentionが利用されている\n","- その優れた特徴から、自然言語処理以外の分野でも利用が進む"],"metadata":{"id":"rVw9AeeL5sI2"}},{"cell_type":"markdown","metadata":{"id":"Zd4rO-jl1Xe4"},"source":["# Transformer\n","- 2017年に導入されたディープラーニングモデルの一種\n","  - 主に自然言語処理で利用されている\n","- RNNと同様自然言語などの時系列データ処理向けに設計されているが、再帰や畳み込みは利用していない\n","- Attention層のみで構築されている(後述)\n","- 翻訳やテキスト要約などの各種タスクに利用可能\n","- 並列化が容易で訓練時間を削減できる\n","- 「Attention is All You Need」という論文で著名になった\n","- 機械翻訳タスクにおいてRNNを用いたモデルよりも精度がよく、訓練コストが小さいことから革命的であり、NLPではRNNに印籠を渡し現在の主流の方法である\n","  - この後登場するBERT、ELECTRA、GPTなどすべてTransformerを基本としている\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ku4J5yu4iy0"},"source":["### Transformerの構造\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer.png\" width=500>\n","\n","Seq2Seq同様EncoderとDecoderで構成\n","\n"]},{"cell_type":"markdown","source":["#### Encoderの構造\n","\n","文章から意味ベクトルを抽出する\n","\n","1. Embedding層により入力文章をベクトルに圧縮、つまり分散表現に変換する\n","1. Positional Encoder層で文章内のどこにあるかという位置情報を加える\n","1. Multi-Head Attention層\n","  - 最も本質的かつ重要な層\n","1. normalization(正規化)によりデータの偏りを削減する\n","  - batch normalizationではなくlayer normalizationが行なわれる\n","  - Add & Normで学習速度を高めている\n","1. Feed Forward層\n","  - 2層で活性化関数はReLU\n","\n","以上のMulti-Head AttentionからFeed Forward、Add&Normまでを1セットとして、実際のモデルでは例えば6セット繰り返される\n","  - 出力されたベクトルはDecoderに渡される\n","  - 特にPositionwise fully connected feed-forward networkと呼ばれる\n","\n","n文字入ると、Multi-Head Atrtentionはn入力について以外はn並列で処理している\n","\n","以上で、Encoderが構成される\n","\n","なお、文字は512次元でベクトル化される"],"metadata":{"id":"UF2mhLBJgGhi"}},{"cell_type":"markdown","source":["#### Decoderの構造\n","\n","抽出された意味ベクトルから文章を生成する\n","\n","1. Embedding層により入力文章をベクトルに圧縮(分散表現を獲得)\n","1. Positional Encoder層で位置情報を追加\n","1. Masked Multi-Head Attention層、先ほどと同様であるがAttention内のsoftmax関数を通す直前の値にマスキングが適用されている\n","  - 特定のkeyに対して、Attention weightを0にすることで入力した単語の先読みによる「カンニング」を防ぐ\n","  - 入力に予測すべき結果が入らないようにする\n","1. normalization（正規化）などで先ほどと同様\n","1. Multi-Head Attention層（Encoderの出力を入力として使用）\n","1. normalization（正規化）など\n","1. Positionwise fully connected feed-forward network(先ほどと同じ)\n","1. normalization（正規化）など\n","- 例えば、以上を6回繰り返す\n"],"metadata":{"id":"o1skr29TgJck"}},{"cell_type":"markdown","metadata":{"id":"ZCbAQSWygRj8"},"source":["### Transformerの構成要素\n","\n","- Attention\n","  -「文章中のどの単語に注目すればよいかを表すスコア」のこと\n","  - Query、Key、Valueの3つのベクトルで求める\n","    - Query: Inputのうち「検索をかけたいもの」\n","    - Key: 検索対象とQueryの近さ、どれだけ似ているかを測る\n","    - Value: Keyに基づき、適切なValueを出力する\n","  - Self-Attention\n","    - 下図でInputとMemoryが同一のAttention\n","      - 文法の構造や、単語同士の関係性などを獲得するのに使用される\n","  - SourceTarget-Attention\n","    - 下図でInputとMemoryが異なるAttention\n","      - TransformerではDecoderで使用される\n","  - Multi-Head Attention\n","    - Attentionを複数並列して並べたもの(後述)\n","  - Masked Multi-Head Attention\n","    - Multi-Head Attentionにマスクをつけたもの\n","    - 特定の key に対してAttention weight を0にする\n","    - TransformerではDecoderで使われる\n","    - 入力した単語が先読みを防ぐために 情報をマスクで遮断する、言わば「カンニング」を防ぐ\n","  - Attentionは可視化できる\n","    - すでに示したが、attentionは可視化でき、どの単語に注目しているかを知ることができる\n","- Position-wise Fully-connected Feedforward Network\n","  - 2層からなる全結合ニューラルネットワーク\n","  - 単語の位置ごとに個別の順伝播ネットワークとなる\n","    - これにより他単語との影響関係を排除することができる\n","  - パラメータは全てのネットワークで共通\n","$FNN(x) = LeRU(xW_1+b_1)\\cdot W_2+b_2$\n","- Positional Encoding ($PE$)\n","  - 「単語の位置」の情報をベクトルに加える\n","    - 本当に加えるだけで次元を増やさない\n","  - $pos$は位置を表し、$2i$および$2i+1$はEmbeddingの何番目の次元か、$d_{model}$が次元数を示す\n","偶数番目：$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$\n","奇数番目：$ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$"]},{"cell_type":"markdown","metadata":{"id":"Qqqr2QeuiwaR"},"source":["<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention2.png\" width=800>\n","\n","- 丸角(緑)がベクトル(テンソル)、四角角(青)が処理を表す\n","- **InputとMemoryはそれぞれ異なる埋め込みベクトルを表し、例えば2つの異なる文章を表す**\n","- Inputについて全結合層で各単語のQueryを作成する\n","- Memoryについても同様に全結合層でKeyを作成しQueryとの内積をとって関連度合い見る\n","  - 同じ向きを向いていれば掛け算となる\n","  - 垂直である、つまり関連しなければ0\n","  - この値を関連度(logit)とする\n","- logitにSoftmaxを適用して0から1の間に調整して出力、この結果が Attention weightとなる\n","  - メモリのどの単語に注意を払うかの重みづけ\n","  - QueryとKeyの関連が大きいとAttention weightが大きくなる\n","    - 正しくMemoryの単語に注意を向けるように,keyが正しくAttentionに向けられるように学習される\n","- Memoryから全結合層を経て、Memoryの各単語に対する埋め込みベクトルであるValueを算出する\n","  - ValueとAttenthion weightとの内積を求める\n","    - Attention weightに従ってValueを選択することを意味する\n","- 最後に全結合層を挟んで出力を得る\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SUEwYu73qKFn"},"source":["### InputとMemory\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/input-memory.png\" width=600>\n","\n","各文章は分かち書きされIDで表現された後、Embeddingにより埋め込みベクトルに変換される\n","\n","### Attention Weightの算出\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/attention-weight.png\" width=600>\n","\n","QueryとKeyの内積を算出してInputとMemoryの各単語の関連度であるlogitを算出、Softmaxを用いてAttention weightとする\n","- Memoryのどの単語に注意を払うかの重み付け\n","\n","例えば、Inputのスポーツという単語に対して、Memoryの「野球」 「が」 「得意」の各単語について正しく注意を向けるように学習する\n","- ここでは野球が高い値になるようになる\n","\n","### valueとの内積\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/value-naiseki.png\" width=600>\n","\n","この内積は、value、ここでは「野球」「が」「得意」の各単語のValueとAttention weightを掛け合わせて総和を計算することになる\n","\n","最も注目するべきvalueの値が算出されているといえるが、他の単語との関連性も考慮した値となっている"]},{"cell_type":"markdown","metadata":{"id":"ooi7J-rSvL7e"},"source":["### Multi-Head Attention\n","\n","Multi-Head Attentionの構成要素は次のような図で表現されるが、これはこれまでの説明で理解できるであろう\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/mhattention.jpg\" width=200>\n","\n","ここで、Transformerでも用いられるMulti-Head Attentionでは、この図のように$Q$、$K$、$V$に同じ入力$X$を与えている\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer-mha.png\" width=200>\n","\n","- 実際にTransformerの構造図を見るとわかる\n","- Queryつまり入力に$X$を与えるのは理解できるが、$K$にも$V$にも入れるのは、意味不明に感じるであろう\n","\n","だが、先に示したMulti-Head Attentionの説明の通り、再掲すると、\n","\n",">数式では次のように表すことができる\n",">\n",">$$MHA(Q, K, V) = concat(head_i)W^O$$\n",">\n",">ここで\n",">\n",">$$head_i=Attention(QW^Q_i, KW^K_i, VW^V_i)$$\n",">\n",">である\n",">\n",">Q, K, Wに対して、$W^Q_i$, $W^K_i$, $W^V_i$という重み行列を掛けて、Attentionを求め、さらに$W^O$という重み行列を掛けて、行列を1次元ベクトルに変換するといった動作となる\n",">\n",">例えば、Transformerでは、$W^Q_i$, $W^K_i$, $W^V_i$は一般に8次元であり、入力列を8個の列に重みを掛けて拡張し、それに対してAttentionを求め、さらに一列にするという動作である\n","\n","つまり、$W^Q_i$, $W^K_i$, $W^V_i$という重み行列を掛けており、この重み行列で表されるアファイン変換によりXをねじって曲げてたベクトルとの比較を行っていると考えることができる\n","- Transformerでは$W^Q_i$, $W^K_i$, $W^V_i$という重み行列を掛けることで、大きさが$1/8$になった8個のベクトルに変換され、それぞれで計算して、最後にconcatで1つに戻すといった処理が行われており、8個に分割したそれぞれについて、アファイン変換を行い、それらの類似度に合った値を生成して、最後に結合するといった処理を行っている\n","- Xについて、8分割したどの部分をどのように変換したベクトルと、どの部分をどのように変換したベクトルがどの程度一致したら、どの部分で表されるどのような値を出力しなさい、といった表現になる"]},{"cell_type":"markdown","source":["## DecoderにおけるAttention\n","\n"],"metadata":{"id":"c7Y-69bZ9TI_"}},{"cell_type":"markdown","source":["次のような入力形態となっている\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/transformer-mha-dec.png\" width=200>\n","\n","$V$と$K$はEncoderからの入力で、$Q$はDecoderの入力からきている\n","\n","翻訳では、$V$と$K$に元の言語の文章A、$Q$に翻訳先の言語の文章Bを入力して、AとBの類似度を得る、さらにAから作った値を返す、という意味になる"],"metadata":{"id":"iiPxmugjRojs"}},{"cell_type":"markdown","source":["## Positional Encoding\n","\n","Postional Encoding層は、系列データ内の各要素に、要素のデータ内における位置情報を付与する\n","- 文章の場合、Positiona Encodingによって、各単語ベクトルに、その単語が文章内で何番目に登場するかという情報を付与する\n","\n","次の式で算出した値を固定値として、入力に加算する\n","\n","$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$\n","\n","$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$\n","\n","このPositional Encodingの値を図示すると次のようになる\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/pos_encoding.png\" width=400>\n"],"metadata":{"id":"imYDkjsFTgPT"}},{"cell_type":"markdown","source":["## Transformerにおいて交換されるデータの形\n","\n","何が入ってきて何が出ていくのか\n","\n","Transfomerでは、常に$単語数 \\times 埋め込み次元数$で与えられるテンソルが各ブロック間で受け渡される\n","\n","例外もあり、\n","- embedding層の前では各行が語彙数次元のone-hotベクトルとなっている\n","- 最後の出力では語彙数次元の確率分布ベクトルとなっている\n","\n","であるが、Transfomerブロック内では、$単語数 \\times 埋め込み次元数$の行列の各行に各種変換を施すという処理が繰り返される\n","- この処理は各行独立という意味ではなく、例えばSelf-Attentionでは行と単語の関係に注目している\n","\n","また、Transformerでは、RNNの隠れ状態のような文脈ベクトル、つまり文意を1つのベクトルで表した潜在空間表現の類を明確に生成していない\n"],"metadata":{"id":"wCDnsBsGR2Cv"}},{"cell_type":"markdown","source":["### Transfomerにおける逐次処理\n","\n","RNNと比較すれば並列処理という観点で改善されているが、逐次処理をなくすことはできず、逐次処理が行われる\n","\n","- 推論時のDecoder部の動作について、最初にBOSを入力すると1つめの単語が出力され、その1つ目の単語を入力すると2つ目の単語が出力されという具合に、逐次的な処理が行われる\n","  - 並列的に処理する方法も考案されているが、精度で劣るという結果が得られている\n","  - RNNを取り除いても、完全に逐次処理がなくなったわけではない\n","  - Encoderと、学習時のDecoderは並列処理可能である\n"],"metadata":{"id":"RxmJTQkSTbxY"}},{"cell_type":"markdown","source":["## Transformerにおけるよくある誤解\n","\n","### LSTM vs Transformer\n","\n","LSTMよりもTransformerの方が長い文章処理が得意だ、と言われているが、これは安易にYesといえる問題ではない\n","\n","純粋に方法論・アルゴリズム的観点で答えるなら、LSTMと答えるべきである\n","- 潜在空間の扱いにもよるが、LSTMはシーケンス長よりも長い周期の入力を学習できる(LSTMの節を参照)\n","- Transformerは入力シーケンス長の範囲内のみ考える、つまり、それよりも長い内容はそもそも入力されず、勘案されない\n","\n","LSTMは、長期依存を理論的には保持できるが、勾配が完全に消えないというだけで、系列が数百～数千ステップになると情報保持は困難となる\n","- 依存関係がどこにあるかを探索的に学ばなければならず、遠い依存は弱まる傾向にある\n","\n","Transformerは、全結合的アテンションにより、系列内の任意の位置間の関係を直接参照可能\n","- 全ての位置関係を眺めているので、長期依存を 探索しやすい\n","- ただし、これは長期依存が本質的に得意というより設計的に全距離を直接観測できるからである\n","- その代償として、シーケンス長を$n$とすると、$O(n^2)$で計算コストが増加する\n","  - 長系列では非現実的になる\n","  - Efficient Attention系の研究が盛んである理由\n","\n","### なぜ、LSTM < Transformerなのか\n","\n","Self-Attentionの主要計算は 内積演算（GEMM, General Matrix Multiply）であり、GPUは元々BLASライブラリやTensorCoreでGEMMを極端に効率化できるアーキテクチャを採用している\n","\n","- つまりモデルが長期依存を学べるのではなく計算資源をうまく利用できる\n","- GPUの計算パワーを引き出すことができるために無理やり長い系列を入れ込んで処理している\n","  - つまり、見掛け倒しの長期記憶？である\n","- LSTMの逐次計算（シーケンシャル依存が強い）はGPU実装適していない\n","\n","### Transformerが長期依存に強いと言われる理由\n","\n","まとめると、次の理由となる\n","\n","- Attentionにより、計算的に全依存を一度に見ることができる\n","  - 勾配を時間方向に伝播させる必要がない\n","  - LSTMは時系列を逐次的に逆伝播\n","\n","- GPUとの親和性が高い\n","  - GEMMをフル活用でき長系列でも現実的に学習可能\n","\n","- 実証的に自然言語処理・時系列解析のベンチマークで性能が良い\n","  - 原理的に強いというよりも、実装と計算環境の進化のおかげ"],"metadata":{"id":"1R21CvyHz15Q"}},{"cell_type":"markdown","metadata":{"id":"D1PpwmcMJFRB"},"source":["# Transformerモデルを用いた文章分類\n","\n","シンプルなクラス分類のTransformerモデルをフルスクラッチで実装する\n","- 映画の英語レビューがポジテイプな内容かネガテイプな内容かを判定させる\n","- どのような単語に注目して判定したのかをSelf-Attentionの結果から可視化する\n","\n","**<font color=\"red\">+++注意+++</font>**\n","\n","ライブラリの更新が速いため、バージョン違いによる動作エラーが発生する場合がある\n","- 現在把握している問題については、回避方法を記述していますが、再起動が必要となるなど、単純に実行しただけでは結果を得ることができない場合がある\n","\n","なお、PyTorchでは既にTransformerの公式実装が存在しておりそちらを利用するべきであるが、ここでは構造を理解するため全て記述する"]},{"cell_type":"markdown","metadata":{"id":"3IBOjXDeJFRE"},"source":["## 事前準備\n","\n","今回はフルスクラッチで記述するため、シンプルである\n","- このあと機能に特化したライブラリは個別に読み込んでいる\n","- なお、PyTorchのTransformerライブラリを用いるなどして、個別モジュールの設計を避けてパーツを組み合わせることで実装することを推奨する\n","- 提供されるライブラリは下記の記述よりも実行速度が速い、最適化されている、より優れた実装が採用されている、なによりも精度が高くなるなど、良いことばかりであり、そもそも利用するという立場では一から設計する意味はほとんどない"]},{"cell_type":"code","metadata":{"id":"w1kMABqAJFRF"},"source":["import math\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchtext\n","from torch.utils.data import DataLoader\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["nlpライブラリは様々なデータセットを提供している\n","- 映画評論データセットを入手するために利用する"],"metadata":{"id":"6Yp9AeCnpwLj"}},{"cell_type":"code","metadata":{"id":"7qg6t5nnBjqs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724613896674,"user_tz":-540,"elapsed":16316,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"a42eee01-49d6-4f05-d2b5-122b5ec80e78"},"source":["!pip install nlp\n","!pip install --force-reinstall dill==0.3.5.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlp\n","  Downloading nlp-0.4.0-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nlp) (1.26.4)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (14.0.2)\n","Collecting dill (from nlp)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nlp) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (2.32.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp) (4.66.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from nlp) (3.15.4)\n","Collecting xxhash (from nlp)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n","Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, nlp\n","Successfully installed dill-0.3.8 nlp-0.4.0 xxhash-3.5.0\n","Collecting dill==0.3.5.1\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl.metadata (9.7 kB)\n","Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.8\n","    Uninstalling dill-0.3.8:\n","      Successfully uninstalled dill-0.3.8\n","Successfully installed dill-0.3.5.1\n"]}]},{"cell_type":"markdown","source":["transformersライブラリを読み込んでいるが、transformerのモデルを利用するわけではない\n","- ここでは、AutoTokenizerを利用するために読み込んでいる\n","- AutoTokenizerはHuggingfaceが提供している有用性の高いライブラリであり、今後主流となる予感がする\n","- AutoTokenizerはDataLoaderとの相性がよいため安心して利用できる\n","  - `BertForSequenceClassification.from_pretrained`などを用いることもできるが、バッチ処理がかなり面倒になるであろう\n","  - BertTokenizerFastに引けをとらない処理速度を有している\n","\n","**重要な点**\n","今回は学習させるため、どのようなTokenizerを用いても構わない\n","- なんなら自作でも構わない\n","\n","事前学習済みモデルを利用する場合は、そのモデルが用いたTokenizerを用いなければ正しい結果を得ることができない\n","- 当然であるが、「私」を10に変換していたのが、変わって20に変換されては精度が落ちて当然\n","- 危険なのは、この件に限らず、間違えても頑張って学習する結果、それなりに精度が出るため、「誤りを、誤りと気づきにくい」点に注意が必要である"],"metadata":{"id":"a6N8vPoOp4-Z"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrkJBTwtoGXN","executionInfo":{"status":"ok","timestamp":1724613901355,"user_tz":-540,"elapsed":4686,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"fd4dc1bd-4921-4e21-e48b-18301e9aac6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZsgQNMJxpBnW"},"source":["### モデルとTokenizerの読み込み\n","事前学習済みのモデルと、これと紐づいたTokenizerを読み込む\n","- いつもと同様、bert-base-uncased 事前学習モデルを読み込む"]},{"cell_type":"code","metadata":{"id":"9R0HK29fHrf3","colab":{"base_uri":"https://localhost:8080/","height":249,"referenced_widgets":["ebdc7bfc6ef940a6b1c9ea08e15b9cc4","18f85adf01e74ab7bc8762f904353774","263013922a4048d3b9da2696b03ab864","d426495bca084f37935dee467001882d","c354b60282334d2b8d51adee1a6acbb4","ab13c1f3149942308e33f7b360aee3ac","257ff08d9a444110a58dde708afe806a","823cbf48541a4b9ebcbd97ee6284d409","3e8e8a0de36843ce840916773b61a557","6e21cd4c5d8f42f980d0f5b818a27ed0","70d863b0693645de832c2ebedc32d8b8","608f990886d94c1c80c4543d5795b6ed","e3ed9ef6b2094720aa2258ac3d84f05a","34c811082da446f18ffb11cf6e3d4754","85eb97bbfd954d6caf95bf5e18463738","8a0ac92777e94e61ac446b7a82097c3d","461692199a2342d9a3dbea4ebc3cf1b2","d05a3ca545674ed6857ba0e5578c36fc","1b6cea64714c4a598f4f039b11ec8aeb","8ffd642c2b3a4da9b2f9efd6c1dd7367","6f88c01d66804dbe82c7e796269f94aa","0257b592756446ecbf847c36982ef7ab","074bd885b4a948088627e3e47e286457","4406f1ff829644ce8488fa3f0445ecd7","f7f23c4c0139406cb3562f865caf39e8","e422e1df77ed47b08e2c309dc4a90cc8","8f0ab117a9ef4f30bee64dbedb12ac68","68a17556145a46e8a53fa1e2f4e0c65b","cc77b98c5650498d9d2244ad31f57800","c0c5a8db4546478e9c9da16d0c5236ad","ad1ba9c41d904caf97a071d257a50b75","a48efe28023344a883ec9b52bde51249","efb740f176704877b5f3b5a464567ea5","e1e296ad72a241b2947866cf3276cd0d","663164f98e604712a61459b9a408a7f6","3a1078bff3df4233b75a189d37ae52e5","c90db5113b9d4e9185b51bee0a091f51","748f6d1a17094b5bbf57a9ac087e982e","4b73a7149f1a41cbaedbaf89789f7142","f1b9d6abfaaf4b96b33f4248be9a4a06","8f5db5b00122498d9b1a26eee5484d8b","6d52b68712004c6caa6144fe45c9dd57","46680fb658d8436cba3fa7452ab8d85c","9660f0f77a7b45bbb43abdb0fdd8a50c"]},"executionInfo":{"status":"ok","timestamp":1724613908911,"user_tz":-540,"elapsed":7558,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"e834c8f5-3dc4-4a76-dbb3-eef323fd402b"},"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebdc7bfc6ef940a6b1c9ea08e15b9cc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608f990886d94c1c80c4543d5795b6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074bd885b4a948088627e3e47e286457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e296ad72a241b2947866cf3276cd0d"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"aWCmm2TjqToE"},"source":["### データセットの読み込み\n","\n","nlpライブラリに含まれるIMDbデータセットを利用する\n","- IMDbデータセットは、ポジティブかネガティブの好悪感情を表すラベルが付与された25000の映画レビューコメントデータセット\n","- 好意的なレビューは1、否定的なレビューは0が振られている\n","- 感情分析用では鉄板のデータセット\n","\n","https://www.imdb.com/interfaces/"]},{"cell_type":"code","metadata":{"id":"rfEnNpv9HuXI","colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["609e698f0db641bba9c52dff4e5d3fc6","f9704bd3b3b842c08bf09205fba033b2","a7492a2b54214c308610fcd52aa6591c","8614679f4aef49608c5f4713d0f1c7af","6d55f24cb45f4f108d5cd02bafab8503","594d5bccc81b47abb9abdb8517b1c337","96a411e285a04508a4bcc409a8d52efc","09196f3a0c7e422fb72a43e394208223","5e26f52520ba46dc9e3e49bc2c88419f","994938bef80d4b60be7812a9fc9678f3","a55ba8b889aa40c1922386070aae2026","7dbf6ef1329f45de8b454c47c5acdc8f","cff47a16ea6b443eae3ea43ed28340ea","07d0c67d4a2344e39616c2986364abf9","9011d054db604ae3b8f824e867dd8882","f25b17b1ced5471a9d48d9b1efc5f520","bb5bb6571a0d4be18e7669138db4a311","680e631dc02647f8a3ebe1f4569d1e8a","d1e44c5b9f3343f4bd3a94aa70807d35","e8c4bc1a750a471985ca3c31e27c6797","2a631994141b440fb4713c7fd6172357","cd4329bf72904344b64cc609371d38c6","f234c27ac152470cae9ad181523798a0","75ff2f027205405aa88bec8660c6bdf2","2b04212e31b14536a1cf48c449542d30","5674d060cbc04b7c934331b77588f339","1936d1edf2534816bf53fdfd98549af2","85e6530bdde04696a768782a98882967","602793bb54bf4168a398de531c9d7e59","bc905919404c493aba1c9dddb40c15fe","fcc1e5b84e694a4a83ecae73606fc5d1","e18f65ba9c484290863fda701f00ad8c","1780aaa2bd53431fba7ab3a4f8cb6f4d","3f310ab30c454db1a4d90d99fdbdca5b","a7b684cd1503421eb0107f5cc15e5605","31e1c3ffae5941949aec7940aee9c130","a09afbcff7fa44528fafc00d89964696","d915b36618ca4957b297d3f790e383bf","ea96097b12434ffe9ed9655ec3c2f13c","93e2359014dd41cebea5ceca142f08ad","c69ca9181ad74f1195ecb4e20de62df5","9d7e9b3f2517474a9286efb7bcdcbf98","0fae6f75fb814ea499bdd40aac39a432","56be8c2ace1b4dba893ca01f66f03ac7","295178af324f42e69ce04257e6e773be","83baf674e7e2465f87fe0d412cb887e2","cc3fe93dda4440909facf752c0295051","ecaa6dec8fc84b12bc0e4e2932359826","acfaf9f76d2240428ce356be9d061ba0","c7737155da5a441f9fbf203e72b665cb","abcc5a7c7d094ce5a670e5f4deafc9ec","e3be20018e5e4e9da3c308ebaf5d7d09","aa198ba2d5584f3cb197f150025950f3","8d5f119ad258461bbab6196e4653695d","c7448387580a44dc90367c47660eb742","50ba36c02b7e4ecc92996f0e6bdbc4c5","42e8ade851c2412289bd9b2658ddb697","73c84b3239404bf9a3113bbeb7a588ff","e9b38bbf05f94d529ef927448794627e","a5c686d61bf24c308427e0b3a9a0e3c8","f9a546787ea441d4a563b29b5837b84d","709a26e05202480690d4dceef53bde06","cb97acced5cd4758b7a542511db2cbd5","c467fd6968624f1bbce7fc5c8bc19db7","4989afd5b82744a6b811a89d0b9db806","6d1b763197994ee7bb2867ae2652ea5e"]},"executionInfo":{"status":"ok","timestamp":1724613960675,"user_tz":-540,"elapsed":51776,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"3c3e4dd4-a406-4daa-84ab-390ee7f9ed14"},"source":["from nlp import load_dataset\n","raw_train_data, raw_test_data = load_dataset(\"imdb\", split=[\"train\", \"test\"]) # 訓練用と検証用データに分けて読み込む"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609e698f0db641bba9c52dff4e5d3fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dbf6ef1329f45de8b454c47c5acdc8f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown sizetotal: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f234c27ac152470cae9ad181523798a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f310ab30c454db1a4d90d99fdbdca5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"295178af324f42e69ce04257e6e773be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ba36c02b7e4ecc92996f0e6bdbc4c5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743. Subsequent calls will reuse this data.\n"]}]},{"cell_type":"markdown","metadata":{"id":"7t_nwDeOX2Ok"},"source":["例としてデータを表示する\n","- 英語です、がっかりしましたか？"]},{"cell_type":"code","metadata":{"id":"EAB_DeeTX1uu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724613960676,"user_tz":-540,"elapsed":11,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"24f4d477-15f0-46bc-b11b-89e77e7726cf"},"source":["print(raw_train_data[\"label\"][0], raw_train_data[\"text\"][0])  # 好意的なコメントの例\n","print(raw_train_data[\"label\"][20000], raw_train_data[\"text\"][20000])  # 否定的なコメントの例"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n","0 This movie tries hard, but completely lacks the fun of the 1960s TV series, that I am sure people do remember with fondness. Although I am 17, I watched some of the series on YouTube a long time ago and it was enjoyable and fun. Sadly, this movie does little justice to the series.<br /><br />The special effects are rather substandard, and this wasn't helped by the flat camera-work. The script also was dull and lacked any sense of wonder and humour. Other films with under-par scripting are Home Alone 4, Cat in the Hat, Thomas and the Magic Railroad and Addams Family Reunion.<br /><br />Now I will say I liked the idea of the story, but unfortunately it was badly executed and ran out of steam far too early, and I am honestly not sure for this reason this is something for the family to enjoy. And I was annoyed by the talking suit, despite spirited voice work from Wayne Knight.<br /><br />But the thing that angered me most about this movie was that it wasted the talents of Christopher Lloyd, Jeff Daniels and Daryl Hannah, all very talented actors. Jeff Daniels has pulled off some good performances before, but he didn't seem to have a clue what he was supposed to be doing, and Elizabeth Hurley's character sadly came across as useless. Daryl Hannah is a lovely actress and generally ignored, and I liked the idea of her being the love interest, but sadly you see very little of her,(not to mention the Monster attack is likely to scare children than enthrall them) likewise with Wallace Shawn as some kind of government operative. Christopher Lloyd acquits himself better, and as an actor I like Lloyd a lot(he was in two of my favourite films Clue and Who Framed Roger Rabbit, and I am fond of Back To The Future) but he was given little to work with, and had a tendency to overact quite wildly.<br /><br />Overall, as much I wanted to like this movie, I was left unimpressed. Instead of being fun, it came across as pointless, and that is a shame because it had a lot of potential, with some talented actors and a good idea, but wasted with poor execution. 1/10 Bethany Cox\n"]}]},{"cell_type":"markdown","metadata":{"id":"CFIcwJVIaAs-"},"source":["DeepLで訳してみると次のような感じです\n","\n","> 1 ブロムウェル・ハイ」は、カートゥーン・コメディです。ブロムウェル・ハイ』は、『ティーチャーズ』のような学校生活を描いた番組と同時期に放送されていました。私の35年間の教師生活を振り返ると、「ブロムウェル・ハイ」の風刺は「ティーチャーズ」よりもはるかに現実に近いものだと思います。経済的に生き残るために奔走する姿、哀れな教師たちの虚勢を見抜く洞察力のある生徒たち、そしてすべての状況の情けなさは、私が知っている学校とその生徒たちを思い出させてくれます。生徒が何度も学校を燃やそうとしたエピソードを見たとき、すぐに ......... .......... のことを思い出しました。高いですね。古典的なセリフです。検閲官：あなた方の先生の一人をクビにするために来ました。生徒：Bromwell Highへようこそ。私と同年代の大人の多くは、「ブロムウェルハイ」を奇想天外なものだと思っているのではないでしょうか。そうでないのが残念です。\n","\n","> 0 この映画は努力していますが、1960年代のテレビシリーズの面白さが完全に欠けています。私は17歳ですが、ずいぶん前にYouTubeでこのシリーズを見たことがあり、楽しくて仕方がありませんでした。特殊効果は標準的ではなく、平板なカメラワークによって助けられていませんでした。また、「ホームアローン4」、「帽子をかぶった猫」、「きかんしゃトーマス」、「アダムス・ファミリー・リユニオン」などの作品があります。さて、ストーリーのアイデアは良かったのですが、残念ながら出来が悪く、早々に力尽きてしまったので、正直、家族で楽しめる作品ではないと思います。また、ウェイン・ナイトが気合を入れて演じたにもかかわらず、しゃべるスーツにも腹が立ちました。しかし、この映画で最も腹が立ったのは、クリストファー・ロイド、ジェフ・ダニエルズ、ダリル・ハンナという才能ある俳優を無駄にしてしまったことです。ジェフ・ダニエルズはこれまでも良い演技をしてきましたが、彼は何をすべきかわからないようでしたし、エリザベス・ハーリーのキャラクターも残念ながら役立たずでした。ダリル・ハンナは素敵な女優だが、一般的には無視されており、私は彼女が愛の対象になるというアイデアが好きだったが、残念ながら彼女の姿はほとんど見られない。（モンスターの攻撃は、子供たちを魅了するというよりも、怖がらせる可能性が高いのは言うまでもない）同様に、ウォレス・ショーンもある種の政府の工作員として登場する。        1/10 ベサニー・コックス"]},{"cell_type":"markdown","source":["試しに一文をTokenizerで単語IDに変換する"],"metadata":{"id":"qBcib8lrsMTG"}},{"cell_type":"code","source":["torch.tensor(tokenizer(raw_train_data['text'][0], max_length=10)['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlwUvUuYAKRq","executionInfo":{"status":"ok","timestamp":1724613961262,"user_tz":-540,"elapsed":596,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"5fa57e5d-fa2d-45aa-9389-a6baad24f8f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([  101, 22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,   102])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["必要なパラメタを定義する\n","- 各データセットの文章数\n","- バッチサイズ\n","- 最大の文章長さ(これ以上の長さは切られる)"],"metadata":{"id":"dmbl19bhsUo4"}},{"cell_type":"code","source":["num_train_data = raw_train_data.num_rows\n","num_test_data = raw_test_data.num_rows\n","batch_size = 32\n","max_seq_len = 256"],"metadata":{"id":"uOMmUJuzINJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz0UpoNQYIzs"},"source":["mapメソッドを利用して各データに前処理を施す\n","- ここではtokenizeを定義し、このtokenizeを全データに施す\n","- tokenizeは読み込んだIMDbのデータをTokenizerで処理し、語句IDに変換する関数である\n","- バッチサイズはデータ全体、つまり全データに対して一気に処理している(順番に取り出して何かするのではないため、これでよい)\n","- \"input_ids\", \"attention_mask\", \"label\"の順番にデータを並べて、PyTorchで利用できるようにPyTorchのDataLoaderと同様の形で出力させる\n","- max_lengthで長い文章をここで制限しておく\n","  - つけないと512になる\n","\n","なお、set_formatのtype=\"torch\"は、torch.tensorで出力する指定である\n","- だが、set_format自体が変換するわけではなく、tokenizerに渡して変換する仕様のようだ\n","- 従って、指定のtokenizerを利用しなければ変換されないので注意\n","  - これがAutoTokenizerを使う大きな理由の一つであり、バッチ化を簡単にできる\n","\n","本来、テストデータはシャッフルする必要はないが、最後に乱雑に確認したいため、シャッフルしている\n","- **普通ではないので注意**"]},{"cell_type":"code","metadata":{"id":"Z2UD6DkjXzto","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["8286b537bbf14ff292bf68b6272e18eb","21352b52d83c41788f854597cd83782c","d4a67e51aa514b7e99c6a58c1ddd1f13","bd17038b86a14cc4ba3ce3d2db392edd","830be51a23c6493f8efba0c3a9a7a1ee","74e3079148954fe3a42e0fc45ddabf72","9711c35310b34163b315c996494a51dc","fb40d24fb7c1481686538a860be44918","10be5b043cb14a988d19f6bf0b5bf0a2","354ca5efc9974b3096ff7b5444c93ea7","60328d2b969246e897b65df5cb8cbda2","bd2bd2d9c9e74c7693fe6b71cb569827","d107a5603ac449cd86b4ec2e49151ea9","fe0e1feeb2704a08b958759dc678cb63","785a2210c6f84094965a61191c867f69","3d7cd89143e74bd9a19f6d96efe442ff","605b0951f83c4ed28929b295ce520fe3","8689889bacae48f6880e8244a07d4bff","98dd42d0987540d99769f8f3ebb86864","ea1519c15750405399e40a2b172dd002","a25acc1cdd4b4ceabe194a6c778c2362","9b0f19e44e6f4eb4b2c89442ddbcd89b"]},"executionInfo":{"status":"ok","timestamp":1724614038304,"user_tz":-540,"elapsed":77046,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"0dfc3b94-00bf-4c4a-cad9-e13968a0d918"},"source":["train_data = raw_train_data.map(lambda e: tokenizer(e['text'], truncation=True, max_length=max_seq_len, padding='max_length'), batched=True)\n","train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","test_data = raw_test_data.map(lambda e: tokenizer(e['text'], truncation=True, max_length=max_seq_len, padding='max_length'), batched=True)\n","test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True) # ここのシャッフルは意味がないが最後の試行を乱雑にしたい"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8286b537bbf14ff292bf68b6272e18eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd2bd2d9c9e74c7693fe6b71cb569827"}},"metadata":{}}]},{"cell_type":"markdown","source":["train_dataの中身は次の通り"],"metadata":{"id":"Zkt-yrSqs49X"}},{"cell_type":"code","source":["train_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-OJW-XGyt8Dp","executionInfo":{"status":"ok","timestamp":1724614038304,"user_tz":-540,"elapsed":15,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"d9a46a1a-d8b7-4120-cd9f-4866eda215ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset(features: {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None), 'text': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 25000)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["train_dataloaderから試しにデータを取得する"],"metadata":{"id":"ES7ekokus76v"}},{"cell_type":"code","source":["next(iter(train_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8M7xA7-ktYvo","executionInfo":{"status":"ok","timestamp":1724614038304,"user_tz":-540,"elapsed":5,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f5cb359f-bf8a-467e-d8e2-e26d7be5b809"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n","         1, 1, 1, 1, 0, 1, 0, 1]),\n"," 'input_ids': tensor([[ 101, 1045, 3866,  ..., 1005, 4470,  102],\n","         [ 101, 1045, 5136,  ...,    0,    0,    0],\n","         [ 101, 2023, 2001,  ...,    0,    0,    0],\n","         ...,\n","         [ 101, 4066, 1997,  ..., 6979, 5603,  102],\n","         [ 101, 6274, 5125,  ...,    0,    0,    0],\n","         [ 101, 2023, 2143,  ...,    0,    0,    0]]),\n"," 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1,  ..., 1, 1, 1],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0]])}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["語彙数、つまりTokenizerが知っている単語の種類の数をパラ目として設定する"],"metadata":{"id":"Y-izvBgJvpbb"}},{"cell_type":"code","source":["vocab_size = tokenizer.vocab_size\n","vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJ00Kxq20qSZ","executionInfo":{"status":"ok","timestamp":1724614038304,"user_tz":-540,"elapsed":4,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"9459e8ec-cbf7-41a5-f8a1-5f132f20748c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## Transformerのネットワーク構造\n"],"metadata":{"id":"Revn7i8dCLlO"}},{"cell_type":"markdown","source":["### 各層の結合とデータサイズ\n","\n","入カはミニバッチ数$M(=256)$、一文の単語数$W(=256)$とすると、$M\\times W$\n","\n","処理は次の通りとなる\n","\n","- 内部の単語の表現ベクトル$V(=300)$とすると、Embedderにより、単語一つがV次元のベクトル表現になり、その出力は$M\\times W \\times V$となる\n","  - Embedding層は内部で、まず入力記号OneHotベクトルに変換し、そのOneHotベクトルをより低次のベクトル空間上に線形写像している\n","- Embedderの後、PositionalEncoder、TransformerBlockへと処理が映るが、これらは全て入出力で次元を変更していない\n","- 最後のTransformerBlockの出力がClassificationHeadにおいて、クラス数$C(=2)$に変換され、結果的に$M\\times C$となる"],"metadata":{"id":"ik1knMVh9mBD"}},{"cell_type":"markdown","source":["### 各層の動作内容\n","\n","- Embedding\n","  - ここでは、PyTorchが提供するnn.Embeddingを用いており、誤差逆伝搬により更新される\n","  - その他、fasttextや、Word2Vecなどによる事前学習に基づいた分散表現変換も想定される\n","- PositionalEncoder\n","  - 入カデータに位置情報を足し算する\n","  - Self-Attentionを利用するため、各単語がどの単語と関係するかはAttentionで計算、獲得できる\n","  - すると、入力文章の単語の順番がシャッフルされた場合、同様に処理すると、語順という概念が欠落して同じ結果が出る可能性がある、すなわち語順が考慮されない、という問題を解決する\n","  - 今回のように文章の構造を判断材料に入れたいという場合に導入している\n","\n","- TransformerBlock\n","  - 任意の回数繰り返して利用する\n","    - Transformerの図で$\\times n$と記載されている通り\n","  - ここでは2段構成となっている\n","  - 入力のmaskはAttention Mapの一部の値を0に置き換える\n","  - 文章がmax_lengthの256文字よりも短くパディング、つまり<pad>が埋め込まれている部分についてAttentionを求めないように、その重みを0とする\n","  - 翻訳タスクなどのデコーダ側では、マスクされた単語を補完する、マスク位置を次々とずらすことで文章を完成させるといったタスクを達成するために利用する\n","\n","- ClassificationHead\n","  - 今回のタスクがクラス分けであるため、Transformer標準ではないが、最後に設けて次元数2の出力に変換する"],"metadata":{"id":"RcquWAj4CYWr"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1Gr2vIJ1JK8p"}},{"cell_type":"markdown","source":["### Embedder\n","\n","既述は次の通り\n","\n","主なオプションは次の通り\n","- `num_embeddings（int）`: 埋め込み辞書のサイズ\n","- `embedding_dim（int）`: 各埋め込みベクトルのサイズ\n","- `freeze=True`: ここでは利用していないが、誤差逆伝搬において内部重みの更新を阻止する"],"metadata":{"id":"qMSfC0FSJPZX"}},{"cell_type":"code","metadata":{"id":"iAB9spQJJFRM"},"source":["class Embedder(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim):\n","        super(Embedder, self).__init__()\n","        self.embeddings = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n","\n","    def forward(self, x):\n","        x_vec = self.embeddings(x)\n","        return x_vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Embedderの動作を確認する"],"metadata":{"id":"zWxZYSxo3rAn"}},{"cell_type":"code","source":["# モデル構築\n","net1 = Embedder(vocab_size,300)\n","# 入出力\n","test_batch = next(iter(train_loader))\n","x = test_batch['input_ids']\n","x1 = net1(x)  # 単語をベクトルに\n","print(\"入力のテンソルサイズ：\", x.shape)\n","print(\"出力のテンソルサイズ：\", x1.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1r9QL3S0vJa5","executionInfo":{"status":"ok","timestamp":1724614038304,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"6fcb2bd7-fdb7-4855-8edc-ad5546d24cfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n"]}]},{"cell_type":"markdown","source":["### PositionalEncoder\n","\n","入力された単語の位置を示すベクトル情報`pe`を付加する\n","- 位置の計算式はTransfomerの論文のままの標準的な方法\n","- 文章が短く、単語ベクトルがPositional Encodingよりも小さい場合に対応するため、$\\sqrt{V}$を掛けて大きさをある程度そろえる処理が加わっている\n","- `pe`は何度も計算するわけではなく、コンストラクタにおいて、テーブルとして保持している\n","- `pe`は勾配計算の対象外であるため、`requires_grad = False`を忘れないように\n","  - 計算しても動作するがpeが更新され変更される\n","  - 結果として実行速度低下を招く\n","  - 精度低下を招くかどうかはなんともいえない\n","    - 課題としてトライしてみるとよいだろう"],"metadata":{"id":"6goQ4l5B1-md"}},{"cell_type":"code","metadata":{"id":"S-R3xrLWJFRU"},"source":["class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model=300, max_seq_len=max_seq_len, devname='cpu'):\n","        super().__init__()\n","        self.d_model = d_model  # 単語ベクトルの次元数\n","        pe = torch.zeros(max_seq_len, d_model)\n","        pe = pe.to(devname)\n","        for pos in range(max_seq_len):\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\n","        self.pe = pe.unsqueeze(0) # 表peの先頭に、ミニバッチ次元となる次元を足す\n","        self.pe.requires_grad = False # 勾配を計算しないようにする\n","    def forward(self, x):\n","        # 入力xとPositonal Encodingを足し算する\n","        # xがpeよりも小さいので、大きくする\n","        ret = math.sqrt(self.d_model)*x + self.pe\n","        return ret"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PositonalEncoderの動作確認を行う"],"metadata":{"id":"CZ4ZKoA6Mi_W"}},{"cell_type":"code","metadata":{"id":"SZoqYmiiJFRX","executionInfo":{"status":"ok","timestamp":1724614038856,"user_tz":-540,"elapsed":554,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1679519b-7dd6-4120-bfbb-fca220d46ada"},"source":["# モデル構築\n","net2 = PositionalEncoder(d_model=300, max_seq_len=max_seq_len)\n","# 入出力\n","x = test_batch['input_ids']\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)\n","print(\"入力のテンソルサイズ：\", x1.shape)\n","print(\"出力のテンソルサイズ：\", x2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n"]}]},{"cell_type":"markdown","source":["### TransformerBlock\n","\n","LayerNormalization、Dropout、Attention、FeedForwardで構成\n","\n","- LayerNormalization: 各単語が持つ$V$個の特徴量に対し、その特徴量毎に正規化を行う\n","  - 各特徴量が持つ$V$次元の要素の平均と標準偏差が、それぞれ0と1になるように正規化\n","\n","- Attentionにおいて特徴量が変換\n","- その出力にDropoutしたベクトルとLayerNormalizationの入力のベクトルを足し算する\n","- FeedForwardにより特徴量変換を行う\n","\n","なお、オリジナルのTransformerはMulti-Head Attentionであるが、ここではSingle Attentionで実装している\n","- Single Attentionを複数並列するとMulti-Headになる\n","- Milti-headについては演習で扱う\n","\n","- テキストの隙間埋めパディング`<pad>`の部分のmask値は0であるが、Attentionにおいてはこの部分を-le9というマイナス無限大に近い値に置き換える\n","- 結果的に、その後のSofmax計算で邪魔をしなくなる\n","  - Attention Mapにおいて0になるようにするため\n","\n","<img src=\"https://class.west.sd.keio.ac.jp/dataai/text/mytransformerblock.png\" width=500>"],"metadata":{"id":"v6vSrSptNX8m"}},{"cell_type":"code","metadata":{"id":"i8wcSoeDJFRa"},"source":["class Attention(nn.Module):\n","    def __init__(self, d_model=300):\n","        super().__init__()\n","        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)\n","        self.out = nn.Linear(d_model, d_model)  # 出力時に使用する全結合層\n","        self.d_k = d_model  # Attentionの大きさ調整の変数\n","\n","    def forward(self, q, k, v, mask):\n","        # 全結合層で特徴量を変換\n","        k = self.k_linear(k)\n","        q = self.q_linear(q)\n","        v = self.v_linear(v)\n","        # Attentionの値を計算する\n","        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)  # 値が大きくならないようroot(d_k)で割って調整\n","        mask = mask.unsqueeze(1) # maskを計算\n","        weights = weights.masked_fill(mask == 0, -1e9)\n","        normlized_weights = F.softmax(weights, dim=-1)  # softmaxで正規化\n","        output = torch.matmul(normlized_weights, v)  # AttentionをValueとかけ算\n","        output = self.out(output)  # 全結合層で特徴量を変換\n","        return output, normlized_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention層から出力を全結合層2つで特徴量を変換する"],"metadata":{"id":"eOog06Sxcr4T"}},{"cell_type":"code","metadata":{"id":"e1AcDAfjJFRe"},"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.dropout(F.relu(x))\n","        x = self.linear_2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQ5Y2sDDJFRh"},"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, dropout=0.1):\n","        super().__init__()\n","        # LayerNormalization層\n","        self.norm_1 = nn.LayerNorm(d_model)\n","        self.norm_2 = nn.LayerNorm(d_model)\n","        # Attention層\n","        self.attn = Attention(d_model)\n","        # Attentionのあとの全結合層\n","        self.ff = FeedForward(d_model)\n","        # Dropout\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        x_normlized = self.norm_1(x)  # 正規化\n","        output, normlized_weights = self.attn(  # Attention\n","            x_normlized, x_normlized, x_normlized, mask)\n","        x2 = x + self.dropout_1(output)\n","        x_normlized2 = self.norm_2(x2)  # 正規化\n","        output = x2 + self.dropout_2(self.ff(x_normlized2)) # 全結合層\n","        return output, normlized_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 動作確認"],"metadata":{"id":"t2Yf3QtkNmmh"}},{"cell_type":"code","metadata":{"id":"KNaVWuNJJFRk","executionInfo":{"status":"ok","timestamp":1724614040035,"user_tz":-540,"elapsed":1181,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c583ee6d-82e8-457c-d387-271276c2c063"},"source":["net3 = TransformerBlock(d_model=300)  # モデル構築\n","# maskの作成\n","x = test_batch['input_ids']\n","input_pad = 101  # 単語のIDにおいて、'<pad>': 1 なので\n","input_mask = (x != input_pad)\n","# 入出力\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)  # Positon情報を足し算\n","x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n","print(\"入力のテンソルサイズ：\", x2.shape)\n","print(\"出力のテンソルサイズ：\", x3.shape)\n","print(\"Attentionのサイズ：\", normlized_weights.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 256, 300])\n","Attentionのサイズ： torch.Size([32, 256, 256])\n"]}]},{"cell_type":"markdown","source":["### ClassificationHead\n","\n","評価のPositive/Negativeの2つのクラス分類を出力する\n","\n","ここで、どこを利用して特徴量を抽出するかについて選択肢がいくつかある\n","- 文章全体を用いて特徴量を抽出する\n","  - もちろん可能であり、こちらの方がよさそうですが、問題として文章の長さが異なるためパディングによる悪影響が回避できるかどうかが疑わしいという問題がある\n","  - これは、課題として比較してみるとよいであろう\n","- どこかしらの1つの特徴量を利用する\n","  - こうなるとどこか、ということであるが、先頭単語の特徴量を利用するという方針を選択している\n","  - これは、先頭単語に分類に必要な特徴量が存在するというわけではない\n","  - 学習によって、「そうなるように」能力を獲得させるということである\n","  - これでもうまくいくのだから、DNNはそれなりにミスがあっても、見当違いがあっても、おおらかに、かつ甘んじてそれを受け入れ、その制約の中で頑張って学習する健気な存在である"],"metadata":{"id":"MScWOg5K0RbN"}},{"cell_type":"code","metadata":{"id":"pcrNM1rpJFRn"},"source":["class ClassificationHead(nn.Module):\n","    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n","\n","    def __init__(self, d_model=300, output_dim=2):\n","        super().__init__()\n","\n","        # 全結合層\n","        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n","\n","        # 重み初期化処理\n","        nn.init.normal_(self.linear.weight, std=0.02)\n","        nn.init.normal_(self.linear.bias, 0)\n","\n","    def forward(self, x):\n","        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n","        out = self.linear(x0)\n","\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformerの動作確認"],"metadata":{"id":"CL_p27fp2R2H"}},{"cell_type":"code","metadata":{"id":"WUVS-wxoJFRq","executionInfo":{"status":"ok","timestamp":1724614040618,"user_tz":-540,"elapsed":584,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fabfbd5-e29a-4a81-b9b6-6d328f25e8c8"},"source":["batch = next(iter(train_loader))  # ミニバッチの用意\n","# モデル構築\n","net3 = TransformerBlock(d_model=300)\n","net4 = ClassificationHead(output_dim=2, d_model=300)\n","# 入出力\n","x =test_batch['input_ids'][0]\n","x1 = net1(x)  # 単語をベクトルに\n","x2 = net2(x1)  # Positon情報を足し算\n","x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n","x4 = net4(x3)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n","print(\"入力のテンソルサイズ：\", x3.shape)\n","print(\"出力のテンソルサイズ：\", x4.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力のテンソルサイズ： torch.Size([32, 256, 300])\n","出力のテンソルサイズ： torch.Size([32, 2])\n"]}]},{"cell_type":"markdown","source":["最終的なTransformerモデルのクラス"],"metadata":{"id":"xXgFUcrH2doO"}},{"cell_type":"code","metadata":{"id":"k8-NHGc3JFRt"},"source":["class TransformerClassification(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, d_model=300, max_seq_len=max_seq_len, output_dim=2):\n","        super().__init__()\n","        self.net1 = Embedder(num_embeddings, embedding_dim)\n","        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len, devname=device)\n","        self.net3_1 = TransformerBlock(d_model=d_model)\n","        self.net3_2 = TransformerBlock(d_model=d_model)\n","        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n","\n","    def forward(self, x, mask):\n","        x1 = self.net1(x)  # 単語をベクトルに\n","        x2 = self.net2(x1)  # Positon情報を足し算\n","        x3_1, normlized_weights_1 = self.net3_1(\n","            x2, mask)  # Self-Attentionで特徴量を変換\n","        x3_2, normlized_weights_2 = self.net3_2(\n","            x3_1, mask)  # Self-Attentionで特徴量を変換\n","        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n","        return x4, normlized_weights_1, normlized_weights_2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["最終的なTransformerモデルのクラスの動作確認"],"metadata":{"id":"kucURlu72kKc"}},{"cell_type":"code","metadata":{"id":"g2dt7_rvJFRv","executionInfo":{"status":"ok","timestamp":1724614043205,"user_tz":-540,"elapsed":2589,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b3e312c-4e59-4b21-ebba-c7cea312a3a1"},"source":["model = TransformerClassification(  # モデル構築(batchは前の値を利用する)\n","\n","    num_embeddings=vocab_size, embedding_dim=300, d_model=300, max_seq_len=max_seq_len, output_dim=2).to(device)\n","\n","# 入出力\n","x = test_batch['input_ids']\n","x = x.to(device)\n","input_mask = (x != input_pad).to(device)\n","out, normlized_weights_1, normlized_weights_2 = model(x, input_mask)\n","\n","print(\"出力のテンソルサイズ：\", out.shape)\n","print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["出力のテンソルサイズ： torch.Size([32, 2])\n","出力テンソルのsigmoid： tensor([[9.9933e-01, 6.6577e-04],\n","        [9.9930e-01, 7.0195e-04],\n","        [9.9938e-01, 6.2266e-04],\n","        [9.9938e-01, 6.1639e-04],\n","        [9.9936e-01, 6.4372e-04],\n","        [9.9938e-01, 6.2473e-04],\n","        [9.9934e-01, 6.5709e-04],\n","        [9.9940e-01, 5.9947e-04],\n","        [9.9947e-01, 5.3263e-04],\n","        [9.9928e-01, 7.1647e-04],\n","        [9.9930e-01, 6.9813e-04],\n","        [9.9922e-01, 7.7591e-04],\n","        [9.9928e-01, 7.1520e-04],\n","        [9.9921e-01, 7.9201e-04],\n","        [9.9933e-01, 6.7431e-04],\n","        [9.9926e-01, 7.4109e-04],\n","        [9.9915e-01, 8.5230e-04],\n","        [9.9939e-01, 6.1456e-04],\n","        [9.9921e-01, 7.9449e-04],\n","        [9.9928e-01, 7.2449e-04],\n","        [9.9924e-01, 7.5686e-04],\n","        [9.9936e-01, 6.4146e-04],\n","        [9.9921e-01, 7.8704e-04],\n","        [9.9931e-01, 6.8983e-04],\n","        [9.9928e-01, 7.2071e-04],\n","        [9.9929e-01, 7.1440e-04],\n","        [9.9926e-01, 7.3545e-04],\n","        [9.9932e-01, 6.8262e-04],\n","        [9.9938e-01, 6.2010e-04],\n","        [9.9923e-01, 7.6560e-04],\n","        [9.9919e-01, 8.1005e-04],\n","        [9.9931e-01, 6.9467e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xDdsFS7CXmXk"},"source":["### DatasetとDataLoaderの実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgmPvIq6XmXk"},"outputs":[],"source":["# 辞書オブジェクトにまとめる\n","dataloaders_dict = {\"train\": train_loader, \"val\": test_loader}"]},{"cell_type":"markdown","source":["ネットワークの初期化として、 He の方法 (一様分布)を用いる\n","- 初期化については後で概要についてまとめる"],"metadata":{"id":"ziBFndlzLAQX"}},{"cell_type":"code","source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        # Liner層の初期化\n","        nn.init.kaiming_normal_(m.weight)\n","        if m.bias is not None:\n","            nn.init.constant_(m.bias, 0.0)\n","# 訓練モードに設定\n","model.train()\n","# TransformerBlockモジュールを初期化実行\n","model.net3_1.apply(weights_init)\n","model.net3_2.apply(weights_init)\n","model.to(device)  # モデルをGPUへ"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSvPRAloQTFR","executionInfo":{"status":"ok","timestamp":1724614043205,"user_tz":-540,"elapsed":3,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"035f92b8-d7f5-4587-c3cc-418a324b4a1d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerClassification(\n","  (net1): Embedder(\n","    (embeddings): Embedding(30522, 300)\n","  )\n","  (net2): PositionalEncoder()\n","  (net3_1): TransformerBlock(\n","    (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (attn): Attention(\n","      (q_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (v_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (k_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (out): Linear(in_features=300, out_features=300, bias=True)\n","    )\n","    (ff): FeedForward(\n","      (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n","    )\n","    (dropout_1): Dropout(p=0.1, inplace=False)\n","    (dropout_2): Dropout(p=0.1, inplace=False)\n","  )\n","  (net3_2): TransformerBlock(\n","    (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n","    (attn): Attention(\n","      (q_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (v_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (k_linear): Linear(in_features=300, out_features=300, bias=True)\n","      (out): Linear(in_features=300, out_features=300, bias=True)\n","    )\n","    (ff): FeedForward(\n","      (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n","    )\n","    (dropout_1): Dropout(p=0.1, inplace=False)\n","    (dropout_2): Dropout(p=0.1, inplace=False)\n","  )\n","  (net4): ClassificationHead(\n","    (linear): Linear(in_features=300, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"5hd6Ov0xXmXm"},"source":["### 損失関数と最適化手法を定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyeBmPrMXmXm"},"outputs":[],"source":["# 損失関数の設定\n","criterion = nn.CrossEntropyLoss()\n","# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n","\n","# 最適化手法の設定\n","learning_rate = 2e-5\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"yqIzemptXmXm"},"source":["### 学習・検証"]},{"cell_type":"markdown","source":["モデルを学習させる関数を作成"],"metadata":{"id":"0bSozxc_CTso"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq6aBPOTX9cB"},"outputs":[],"source":["def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    torch.backends.cudnn.benchmark = True  # ネットワークがある程度固定であれば高速化る\n","    for epoch in range(num_epochs):  # epochのループ\n","        for phase in ['train', 'val']:  # epochごとの訓練と検証のループ\n","            if phase == 'train':\n","                model.train()  # モデルを訓練モードに\n","            else:\n","                model.eval()  # モデルを検証モードに\n","            epoch_loss = 0.0  # epochの損失和\n","            epoch_corrects = 0  # epochの正解数\n","            for batch in (dataloaders_dict[phase]):  # データローダーからミニバッチを取り出す\n","                inputs = batch['input_ids'].to(device)  # 文章を可能ならばGPUへ\n","                labels = batch['label'].to(device)  # ラベルを可能ならばGPUへ\n","                optimizer.zero_grad()  # optimizerを初期化\n","                with torch.set_grad_enabled(phase == 'train'):  # 順伝搬の計算\n","                    # mask作成\n","                    input_mask = (inputs != input_pad)\n","                    input_mask = input_mask.to(device)\n","                    outputs, _, _ = model(inputs, input_mask)  # Transformerに入力\n","                    loss = criterion(outputs, labels)  # 損失を計算\n","                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n","                    if phase == 'train':  # 訓練時のみ勾配計算と更新\n","                        loss.backward()\n","                        optimizer.step()\n","                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n","            # epochごとのlossと正解率\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double(\n","            ) / len(dataloaders_dict[phase].dataset)\n","            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n","                phase, epoch_loss, epoch_acc))\n","    return net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80KEK8XJXmXn","outputId":"5fce389a-570a-416b-a5b1-e09e3c043ca6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724615109507,"user_tz":-540,"elapsed":1064450,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 | train |  Loss: 0.7219 Acc: 0.5844\n","Epoch 1/10 |  val  |  Loss: 0.5853 Acc: 0.6901\n","Epoch 2/10 | train |  Loss: 0.5767 Acc: 0.6960\n","Epoch 2/10 |  val  |  Loss: 0.5382 Acc: 0.7254\n","Epoch 3/10 | train |  Loss: 0.5395 Acc: 0.7264\n","Epoch 3/10 |  val  |  Loss: 0.5509 Acc: 0.7149\n","Epoch 4/10 | train |  Loss: 0.5045 Acc: 0.7539\n","Epoch 4/10 |  val  |  Loss: 0.4805 Acc: 0.7642\n","Epoch 5/10 | train |  Loss: 0.4624 Acc: 0.7773\n","Epoch 5/10 |  val  |  Loss: 0.4726 Acc: 0.7721\n","Epoch 6/10 | train |  Loss: 0.4473 Acc: 0.7888\n","Epoch 6/10 |  val  |  Loss: 0.4542 Acc: 0.7843\n","Epoch 7/10 | train |  Loss: 0.4330 Acc: 0.7990\n","Epoch 7/10 |  val  |  Loss: 0.4461 Acc: 0.7890\n","Epoch 8/10 | train |  Loss: 0.4180 Acc: 0.8074\n","Epoch 8/10 |  val  |  Loss: 0.4352 Acc: 0.7986\n","Epoch 9/10 | train |  Loss: 0.4098 Acc: 0.8116\n","Epoch 9/10 |  val  |  Loss: 0.4306 Acc: 0.8010\n","Epoch 10/10 | train |  Loss: 0.3962 Acc: 0.8207\n","Epoch 10/10 |  val  |  Loss: 0.4700 Acc: 0.7752\n"]}],"source":["# 学習・検証を実行する 15分ほどかかります\n","num_epochs = 10\n","net_trained = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"vpzS-7IqXmXn"},"source":["正解率の計算と表示"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xXiZX_UXmXn"},"outputs":[],"source":["net_trained.eval()   # モデルを検証モードに\n","net_trained.to(device)\n","epoch_corrects = 0  # epochの正解数\n","for batch in (test_loader):  # testデータのDataLoader\n","    inputs = batch['input_ids'].to(device)\n","    labels = batch['label'].to(device)\n","    with torch.set_grad_enabled(False):  # 順伝搬のみ計算\n","        input_mask = (inputs != input_pad)  # mask作成\n","        outputs, _, _ = net_trained(inputs, input_mask)  # Transformerに入力\n","        _, preds = torch.max(outputs, 1)  # ラベルを予測\n","        epoch_corrects += torch.sum(preds == labels.data)  # 結果の計算"]},{"cell_type":"markdown","source":["正解率の出力"],"metadata":{"id":"iNkXv2uL50cq"}},{"cell_type":"code","source":["\n","epoch_acc = epoch_corrects.double() / len(test_loader.dataset)\n","print('テストデータ{}個での正解率：{:.4f}'.format(len(test_loader.dataset),epoch_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEWYEqR-vUtF","executionInfo":{"status":"ok","timestamp":1724615142465,"user_tz":-540,"elapsed":9,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"8dcbcb36-11da-4ce3-8d10-4ec84a5c1945"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["テストデータ25000個での正解率：0.7752\n"]}]},{"cell_type":"markdown","metadata":{"id":"ezyhNu5_XmXn"},"source":["## Attentionの可視化と判定根拠の判断\n","\n"]},{"cell_type":"markdown","source":["### HTML作成関数の実装\n","\n","なぜレピュー文章の内容をポジテイプもしくはネガティブとモデルが判定したのか、判定する際に強くAttentionをかけた単語を可視化することで、その判定根拠を探る\n","- XAI(Explainable Artificial Intelligence :説明可能AI)の議論への対応として、説明性を持たせる判定根拠の可視化は重要\n","- 自然言語処理における判定根拠を明確に示す手法は確立していない\n","- Attentionが判定根拠になるかどうかは議論となっている\n","\n","文章の各単語についてAttentionの影響が強い単語の背景(HTMLのbackground-colorスタイル)ほどより赤くハイライトする\n","- Jupyter NotebookはHTML表示に対応するため、HTMLデータとして作成して表示する\n","- 文章の1単語目に埋め込まれているである<cls>の特徴量が分類の判断材料であるため、この特徴量を作成するために利用したSelf-Attentionをnormlized_weights から取り出して仕様する\n","  - TransformerBlockモジュールが2つあるため、1つ目と2つ目のttention\n","が存在する\n","\n","関数は次の2つ\n","- highlight\n","  Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\n","- mk_html\n","  実際にHTMLデータを作成する\n"],"metadata":{"id":"kRk0d28JHpfQ"}},{"cell_type":"code","source":["def highlight(word, attn):\n","    if (word == input_pad or word == 0):\n","        return ''\n","    wordc = tokenizer.convert_ids_to_tokens([word])[0]\n","    html_color = '#%02X%02X%02X' % (\n","        255, int(255*(1 - attn)), int(255*(1 - attn)))\n","    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, wordc)\n","\n","def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2):\n","    # indexの結果を抽出\n","    sentence = batch['input_ids'][index]  # 文章\n","    label = batch['label'][index]  # ラベル\n","    pred = preds[index]  # 予測\n","    # indexのAttentionを抽出と規格化\n","    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n","    attens1 /= attens1.max()\n","    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n","    attens2 /= attens2.max()\n","    # ラベルと予測結果を文字に置き換え\n","    if label == 0:\n","        label_str = \"Negative\"\n","    else:\n","        label_str = \"Positive\"\n","    if pred == 0:\n","        pred_str = \"Negative\"\n","    else:\n","        pred_str = \"Positive\"\n","    # 表示用のHTMLを作成する\n","    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n","    # 1段目のAttention\n","    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n","    for word, attn in zip(sentence, attens1):\n","        html += highlight(word, attn)\n","    html += \"<br><br>\"\n","    # 2段目のAttention\n","    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n","    for word, attn in zip(sentence, attens2):\n","        html += highlight(word, attn)\n","    html += \"<br><br>\"\n","    return html"],"metadata":{"id":"F0_auT52jZRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["実行するたびに異なる文章を評価できる\n","\n","- Positiveな文章ではPositiveな単語が、Negativeでは逆の単語に注目していることがわかる\n","- 結果を見てどのような文章で誤解しているのかなどを解析し、さらなる工夫を施すことが考えられる"],"metadata":{"id":"IVm70SXWqfpp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5yH0WycXmXo","outputId":"513f80ab-b7ae-4bb6-a477-0107ff8a1310","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1724615142465,"user_tz":-540,"elapsed":6,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["正解ラベル：Negative<br>推論ラベル：Negative<br><br>[TransformerBlockの1段目のAttentionを可視化]<br><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> if</span><span style=\"background-color: #FFDEDE\"> only</span><span style=\"background-color: #FFFDFD\"> disney</span><span style=\"background-color: #FFFEFE\"> had</span><span style=\"background-color: #FFFEFE\"> stayed</span><span style=\"background-color: #FFFAFA\"> away</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> see</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFFEFE\"> think</span><span style=\"background-color: #FFFAFA\"> that</span><span style=\"background-color: #FFFEFE\"> this</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> has</span><span style=\"background-color: #FFF0F0\"> some</span><span style=\"background-color: #FF5050\"> potential</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> well</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> main</span><span style=\"background-color: #FFFEFE\"> character</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF3F3\"> s</span><span style=\"background-color: #FFFEFE\"> situation</span><span style=\"background-color: #FFFEFE\"> does</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF4F4\"> at</span><span style=\"background-color: #FFF3F3\"> least</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF9F9\"> take</span><span style=\"background-color: #FFFDFD\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF1F1\"> whole</span><span style=\"background-color: #FFFEFE\"> jordan</span><span style=\"background-color: #FFFEFE\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FFF7F7\"> thing</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFDFD\"> you</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFFEFE\"> ve</span><span style=\"background-color: #FFFEFE\"> got</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> beginnings</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFBFBF\"> decent</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFDFD\"> !</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> course</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> you</span><span style=\"background-color: #FFFEFE\"> also</span><span style=\"background-color: #FFFCFC\"> lose</span><span style=\"background-color: #FFFCFC\"> more</span><span style=\"background-color: #FFFEFE\"> than</span><span style=\"background-color: #FFFDFD\"> half</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFAFA\"> but</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> oh</span><span style=\"background-color: #FFFEFE\"> well</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFD5D5\"> not</span><span style=\"background-color: #FFF9F9\"> that</span><span style=\"background-color: #FFFEFE\"> much</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFF9F9\"> loss</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFBFB\"> <</span><span style=\"background-color: #FFFBFB\"> br</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFDFD\"> ></span><span style=\"background-color: #FFFBFB\"> <</span><span style=\"background-color: #FFFBFB\"> br</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFDFD\"> ></span><span style=\"background-color: #FFFCFC\"> so</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> here</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFEFE\"> goes</span><span style=\"background-color: #FFFEFE\"> :</span><span style=\"background-color: #FFFDFD\"> you</span><span style=\"background-color: #FFF9F9\"> take</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFBFB\"> typical</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF2F2\"> prep</span><span style=\"background-color: #FF9898\"> ##py</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> suburban</span><span style=\"background-color: #FFFBFB\"> teenage</span><span style=\"background-color: #FFFEFE\"> girl</span><span style=\"background-color: #FFF5F5\"> (</span><span style=\"background-color: #FFFEFE\"> danielle</span><span style=\"background-color: #FFFEFE\"> pan</span><span style=\"background-color: #FFA9A9\"> ##aba</span><span style=\"background-color: #FFFEFE\"> ##ker</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> who</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF2F2\"> s</span><span style=\"background-color: #FFFEFE\"> actually</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFBCBC\"> decent</span><span style=\"background-color: #FFFEFE\"> actress</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> whose</span><span style=\"background-color: #FFB0B0\"> best</span><span style=\"background-color: #FFFEFE\"> friends</span><span style=\"background-color: #FFFEFE\"> sc</span><span style=\"background-color: #FFFEFE\"> ##ree</span><span style=\"background-color: #FFFEFE\"> ##ch</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFE4E4\"> lot</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> mostly</span><span style=\"background-color: #FFFEFE\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> \"</span><span style=\"background-color: #FFE8E8\"> pop</span><span style=\"background-color: #FFFDFD\"> sensation</span><span style=\"background-color: #FFFEFE\"> \"</span><span style=\"background-color: #FFF5F5\"> (</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFFEFE\"> m</span><span style=\"background-color: #FFF8F8\"> assuming</span><span style=\"background-color: #FFFEFE\"> it</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF1F1\"> s</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> direct</span><span style=\"background-color: #FFFDFD\"> quote</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFF7F7\"> ;</span><span style=\"background-color: #FFFDFD\"> movie</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF2F2\"> s</span><span style=\"background-color: #FFFEFE\"> like</span><span style=\"background-color: #FFFEFE\"> this</span><span style=\"background-color: #FFFEFE\"> almost</span><span style=\"background-color: #FFA9A9\"> always</span><span style=\"background-color: #FFFCFC\"> involve</span><span style=\"background-color: #FFF9F9\"> that</span><span style=\"background-color: #FFFAFA\"> particular</span><span style=\"background-color: #FFFDFD\"> phrase</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFF4F4\"> named</span><span style=\"background-color: #FFFEFE\"> jordan</span><span style=\"background-color: #FFFEFE\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFB4B4\"> except</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> course</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFE9E9\"> t</span><span style=\"background-color: #FFFEFE\"> ##ps</span><span style=\"background-color: #FFF0F0\"> ##t</span><span style=\"background-color: #FFFCFC\"> ##g</span><span style=\"background-color: #FFFCFC\"> wants</span><span style=\"background-color: #FFFCFC\"> more</span><span style=\"background-color: #FFFDFD\"> out</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFAFA\"> life</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> enter</span><span style=\"background-color: #FFC5C5\"> brenda</span><span style=\"background-color: #FFFEFE\"> song</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF2F2\"> s</span><span style=\"background-color: #FFFEFE\"> character</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFAFA\"> sophisticated</span><span style=\"background-color: #FFFEFE\"> individual</span><span style=\"background-color: #FFFEFE\"> who</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> just</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFEBEB\"> t</span><span style=\"background-color: #FFFEFE\"> ##ps</span><span style=\"background-color: #FFF1F1\"> ##t</span><span style=\"background-color: #FFFDFD\"> ##g</span><span style=\"background-color: #FFFEFE\"> needs</span><span style=\"background-color: #FFF6F6\"> (</span><span style=\"background-color: #FFFDFD\"> honestly</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFF3F3\"> don</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFEAEA\"> t</span><span style=\"background-color: #FFFEFE\"> care</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> character</span><span style=\"background-color: #FFFEFE\"> '</span><span style=\"background-color: #FFF3F3\"> s</span><span style=\"background-color: #FFEEEE\"> real</span><span style=\"background-color: #FFFEFE\"> name</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFFEFE\"> like</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FF0000\"> acronym</span><span style=\"background-color: #FFE0E0\"> better</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF1F1\"> two</span><span style=\"background-color: #FFFEFE\"> new</span><span style=\"background-color: #FFFDFD\"> friends</span><span style=\"background-color: #FFF5F5\"> go</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> see</span><span style=\"background-color: #FFFEFE\"> jordan</span><span style=\"background-color: #FFFEFE\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FFF4F4\"> (</span><span style=\"background-color: #FFFAFA\"> one</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> dr</span><span style=\"background-color: #FFFEFE\"> ##ool</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFAFA\"> one</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFCFC\"> make</span><span style=\"background-color: #FFAFAF\"> fun</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> dr</span><span style=\"background-color: #FFFEFE\"> ##ool</span><span style=\"background-color: #FFFCFC\"> ##ers</span><span style=\"background-color: #FFFEFE\"> )</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> [SEP]</span><br><br>[TransformerBlockの2段目のAttentionを可視化]<br><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFEFE\"> if</span><span style=\"background-color: #FFFEFE\"> only</span><span style=\"background-color: #FFF6F6\"> disney</span><span style=\"background-color: #FFFDFD\"> had</span><span style=\"background-color: #FFFDFD\"> stayed</span><span style=\"background-color: #FFFEFE\"> away</span><span style=\"background-color: #FFF9F9\"> from</span><span style=\"background-color: #FFE4E4\"> it</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFDFD\"> see</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFFDFD\"> think</span><span style=\"background-color: #FFF7F7\"> that</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFE7E7\"> movie</span><span style=\"background-color: #FFF5F5\"> has</span><span style=\"background-color: #FFFDFD\"> some</span><span style=\"background-color: #FFFEFE\"> potential</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFDFD\"> well</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFD4D4\"> main</span><span style=\"background-color: #FFF3F3\"> character</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFFEFE\"> situation</span><span style=\"background-color: #FFD1D1\"> does</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFFEFE\"> least</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFCFC\"> take</span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFBFB\"> whole</span><span style=\"background-color: #FFFDFD\"> jordan</span><span style=\"background-color: #FFFDFD\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FF9A9A\"> thing</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF0F0\"> and</span><span style=\"background-color: #FFD8D8\"> you</span><span style=\"background-color: #FFF4F4\"> '</span><span style=\"background-color: #FFFAFA\"> ve</span><span style=\"background-color: #FFFEFE\"> got</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFBFB\"> beginnings</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FF0909\"> decent</span><span style=\"background-color: #FFE9E9\"> movie</span><span style=\"background-color: #FFFBFB\"> !</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFEEEE\"> course</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFD5D5\"> you</span><span style=\"background-color: #FFADAD\"> also</span><span style=\"background-color: #FFFBFB\"> lose</span><span style=\"background-color: #FFFEFE\"> more</span><span style=\"background-color: #FFFAFA\"> than</span><span style=\"background-color: #FF6666\"> half</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> but</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> oh</span><span style=\"background-color: #FFFDFD\"> well</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFEFE\"> not</span><span style=\"background-color: #FFF6F6\"> that</span><span style=\"background-color: #FFFDFD\"> much</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFBFB\"> loss</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFAFA\"> <</span><span style=\"background-color: #FFFBFB\"> br</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFCFC\"> ></span><span style=\"background-color: #FFFAFA\"> <</span><span style=\"background-color: #FFFBFB\"> br</span><span style=\"background-color: #FFFEFE\"> /</span><span style=\"background-color: #FFFCFC\"> ></span><span style=\"background-color: #FFEBEB\"> so</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> here</span><span style=\"background-color: #FFE4E4\"> it</span><span style=\"background-color: #FFFEFE\"> goes</span><span style=\"background-color: #FFF2F2\"> :</span><span style=\"background-color: #FFD9D9\"> you</span><span style=\"background-color: #FFFCFC\"> take</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FF8888\"> typical</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> prep</span><span style=\"background-color: #FFFEFE\"> ##py</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> suburban</span><span style=\"background-color: #FFFEFE\"> teenage</span><span style=\"background-color: #FFFDFD\"> girl</span><span style=\"background-color: #FFFEFE\"> (</span><span style=\"background-color: #FFFEFE\"> danielle</span><span style=\"background-color: #FFF8F8\"> pan</span><span style=\"background-color: #FFFAFA\"> ##aba</span><span style=\"background-color: #FFFDFD\"> ##ker</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF4F4\"> who</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFFEFE\"> actually</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FF0000\"> decent</span><span style=\"background-color: #FFF3F3\"> actress</span><span style=\"background-color: #FFFDFD\"> )</span><span style=\"background-color: #FF3232\"> whose</span><span style=\"background-color: #FFFDFD\"> best</span><span style=\"background-color: #FFCACA\"> friends</span><span style=\"background-color: #FFFEFE\"> sc</span><span style=\"background-color: #FFFEFE\"> ##ree</span><span style=\"background-color: #FFFCFC\"> ##ch</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> lot</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> mostly</span><span style=\"background-color: #FFFBFB\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFF1F1\"> \"</span><span style=\"background-color: #FFFEFE\"> pop</span><span style=\"background-color: #FFFEFE\"> sensation</span><span style=\"background-color: #FFF2F2\"> \"</span><span style=\"background-color: #FFFEFE\"> (</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFF4F4\"> '</span><span style=\"background-color: #FFF5F5\"> m</span><span style=\"background-color: #FFFDFD\"> assuming</span><span style=\"background-color: #FFE4E4\"> it</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> direct</span><span style=\"background-color: #FFFEFE\"> quote</span><span style=\"background-color: #FFF9F9\"> from</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFE9E9\"> movie</span><span style=\"background-color: #FFFCFC\"> ;</span><span style=\"background-color: #FFE9E9\"> movie</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFE0E0\"> like</span><span style=\"background-color: #FFFEFE\"> this</span><span style=\"background-color: #FFF7F7\"> almost</span><span style=\"background-color: #FFF4F4\"> always</span><span style=\"background-color: #FFE9E9\"> involve</span><span style=\"background-color: #FFF7F7\"> that</span><span style=\"background-color: #FFFEFE\"> particular</span><span style=\"background-color: #FFEBEB\"> phrase</span><span style=\"background-color: #FFFDFD\"> )</span><span style=\"background-color: #FFFEFE\"> named</span><span style=\"background-color: #FFFDFD\"> jordan</span><span style=\"background-color: #FFFDFD\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFEFE\"> except</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFF0F0\"> course</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> t</span><span style=\"background-color: #FFE0E0\"> ##ps</span><span style=\"background-color: #FFF8F8\"> ##t</span><span style=\"background-color: #FFFDFD\"> ##g</span><span style=\"background-color: #FFAEAE\"> wants</span><span style=\"background-color: #FFFEFE\"> more</span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FF5151\"> life</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFD6D6\"> enter</span><span style=\"background-color: #FFFDFD\"> brenda</span><span style=\"background-color: #FFD3D3\"> song</span><span style=\"background-color: #FFF6F6\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFF4F4\"> character</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> sophisticated</span><span style=\"background-color: #FFE7E7\"> individual</span><span style=\"background-color: #FFF5F5\"> who</span><span style=\"background-color: #FFE7E7\"> is</span><span style=\"background-color: #FFFEFE\"> just</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFFBFB\"> t</span><span style=\"background-color: #FFE0E0\"> ##ps</span><span style=\"background-color: #FFF8F8\"> ##t</span><span style=\"background-color: #FFFDFD\"> ##g</span><span style=\"background-color: #FFFCFC\"> needs</span><span style=\"background-color: #FFFEFE\"> (</span><span style=\"background-color: #FFFEFE\"> honestly</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFFEFE\"> don</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFBFB\"> t</span><span style=\"background-color: #FFEEEE\"> care</span><span style=\"background-color: #FFFEFE\"> what</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF2F2\"> character</span><span style=\"background-color: #FFF5F5\"> '</span><span style=\"background-color: #FFFAFA\"> s</span><span style=\"background-color: #FFFEFE\"> real</span><span style=\"background-color: #FFFEFE\"> name</span><span style=\"background-color: #FFE8E8\"> is</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> i</span><span style=\"background-color: #FFE1E1\"> like</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFCFC\"> acronym</span><span style=\"background-color: #FFFAFA\"> better</span><span style=\"background-color: #FFFDFD\"> )</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> two</span><span style=\"background-color: #FFF4F4\"> new</span><span style=\"background-color: #FFCBCB\"> friends</span><span style=\"background-color: #FFEDED\"> go</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFDFD\"> see</span><span style=\"background-color: #FFFDFD\"> jordan</span><span style=\"background-color: #FFFDFD\"> ca</span><span style=\"background-color: #FFFEFE\"> ##hill</span><span style=\"background-color: #FFFEFE\"> (</span><span style=\"background-color: #FFFEFE\"> one</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FF6060\"> dr</span><span style=\"background-color: #FFFBFB\"> ##ool</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> one</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFEFE\"> make</span><span style=\"background-color: #FFFEFE\"> fun</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FF5959\"> dr</span><span style=\"background-color: #FFFBFB\"> ##ool</span><span style=\"background-color: #FFFEFE\"> ##ers</span><span style=\"background-color: #FFFDFD\"> )</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> [SEP]</span><br><br>"]},"metadata":{},"execution_count":33}],"source":["from IPython.display import HTML\n","\n","# Transformerで処理\n","\n","# ミニバッチの用意\n","batch = next(iter(test_loader))\n","\n","# GPUが使えるならGPUにデータを送る\n","inputs = batch['input_ids'].to(device)  # 文章\n","labels = batch['label'].to(device)  # ラベル\n","\n","# mask作成\n","input_mask = (inputs != input_pad)\n","\n","# Transformerに入力\n","outputs, normlized_weights_1, normlized_weights_2 = net_trained(\n","    inputs, input_mask)\n","_, preds = torch.max(outputs, 1)  # ラベルを予測\n","\n","\n","#index = 3  # 出力させたいデータ\n","index = random.randint(0, batch_size-1)\n","html_output = mk_html(index, batch, preds, normlized_weights_1,\n","                      normlized_weights_2)  # HTML作成\n","HTML(html_output)  # HTML形式で出力\n"]},{"cell_type":"markdown","source":["# 課題(Transformer)\n","\n","- 説明文中で言及した次の2つの課題について実際に試しなさい\n","  - Embeddingの値を学習させた場合とさせない場合の結果の違い\n","  - ClassificationHeadにおける特徴量の扱い方における結果の違い\n","    - この場合ハイライティングは言及しなくてよい\n","\n","- 日本語で実験してみよう\n","  - https://github.com/amazon-research/amazon-multilingual-counterfactual-dataset\n","  - こちらのデータセット利用してトライする\n","    - ただ、中身を見るとわかるが、ちょっとつまらないかも\n","\n","- Embeddingによる内部の単語表現ベクトルの次元を変えたとき、精度にどのように影響するかを調査しなさい\n","  - できれば減らせるようにしよう\n","\n","- Transformerの段数を増やし、精度が向上するか確認してみよう\n","\n","- LightGBMと精度を比較してみなさい\n","  - 落胆する結果にならないとよいですが…"],"metadata":{"id":"JniR0P2CQuIz"}},{"cell_type":"markdown","metadata":{"id":"2UbJrC7Fq43T"},"source":["# PyTorch Transformerを用いた単語予測\n","\n","Transformerは複雑な構造をもっており、これそのものをPytorchで記述することも可能であるが、CNNやRNNと同様、PyTorchが提供するライブラリを利用することで簡単に利用できる\n","- ここではPytorch Transformerが提供するTransformerを構築するに必要な層の要素を組み合わせて設計する\n","\n","Pytorh TransformerとTorchTextを用い、先に学んだsequence-to-sequenceモデルを使って機械翻訳モデルを実装する\n","- この内容はPyTorchのチュートリアルドキュメントに準拠する\n","\n","WikiText2から取得した文章を用いて単語系列であるsequenceを入力、次に来る単語の予測を行う\n"]},{"cell_type":"markdown","metadata":{"id":"m5z7ICe5cTN8"},"source":["## モデル定義"]},{"cell_type":"markdown","metadata":{"id":"7QGqYTRpDIbs"},"source":["単語、つまりトークンの並びであるシーケンスがモデルに入力されると、位置エンコーディング層で単語の順序情報が加えられる\n","\n","言語モデルタスクでは、入力シーケンスと共に、アテンション・マスクを利用する\n","- nn.TransformerEncoderのSelf-Attention層では、シーケンスにおけるその単語以前の単語のみ知ることができる\n","  - 普通は、未来に登場するはずの単語は考慮できないため\n","- そこで、言語モデルタスクでは、後で登場するトークンは未知のトークンとして扱う必要があるため、これらをマスクする\n"]},{"cell_type":"markdown","metadata":{"id":"XX10cXx_DWd2"},"source":["nn.TransfomerEncoderについて\n","- nn.TransformerEncoderモデルの出力は、最終的に全結合層に送られlog-Softmax関数を介することで予測結果を得ることができる\n","- nn.TransformerEncoderは、複数のnn.TransformerEncoderLayer層で構成されており並列的に動作できるためRNNよりも計算効率が良い\n","\n","TransformerModelの引き数\n","- src: [seq_len, batch_size]のTensor型\n","- src_mask: [seq_len, seq_len]のTensor型\n","戻り値: [seq_len, batch_size, ntoken]のTensor型\n"]},{"cell_type":"code","metadata":{"id":"vD0wRqmlPPd6"},"source":["import math\n","from typing import Tuple\n","\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n","                 nlayers: int, dropout: float = 0.5):\n","        super().__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding(d_model, dropout)\n","        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, d_model)\n","        self.d_model = d_model\n","        self.decoder = nn.Linear(d_model, ntoken)\n","\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, src_mask)\n","        output = self.decoder(output)\n","        return output\n","\n","def generate_square_subsequent_mask(sz: int) -> Tensor:\n","    # -infの上三角行列を生成し，対角線上に0を置く。\n","    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hX1-8iMzkrcm"},"source":["## PositionalEncoding\n","\n","PosigionalEncodingモジュールは、シーケンス内のトークンの相対的な位置、もしくは絶対的な位置に関する情報を与える\n","\n","オリジナルの実装と同様、入力と出力は同じ次元である\n","- つまり、もともとの入力$x$に対して、PositionalEncodingの値$p(x)$が得られたとすると、出力は$x+p(x)$となる"]},{"cell_type":"code","metadata":{"id":"B94dp7t9PPd9"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        # [seq_len, batch_size, embedding_dim]型Tensorを引数とする\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkurbIT-cbN5"},"source":["## データの読み込みとバッチ処理\n"]},{"cell_type":"markdown","metadata":{"id":"dhPLp8qLmpkS"},"source":["ここでは、torchtextのWikitext-2データセットを利用する\n","\n","vocabはトークン（単語）をテンソル形式の数値に変換する\n","- 訓練データセットを元に構築されており、データセット依存である\n","\n","- batchify()関数は、トークンが左から右に一つずつ並んだシーケンス形式のデータを束ねて、batch処理ができるようにする\n","  - 変換にはデータをbatch_size 変数のサイズで分割し、余ったトークンは廃棄する\n","\n","例えば、アルファベット26文字をシーケンスとしたとき、バッチサイズが4であれば、アルファベットを長さ6の4つのシーケンスに分割することが考えられる\n","\n","この時次のような変換が行なわれる\n","\n","\\begin{align}\\begin{bmatrix}\n","  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n","  \\end{bmatrix}\n","  \\Rightarrow\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n","  \\end{bmatrix}\\end{align}\n","\n","なお、各バッチ、つまり各列はモデル内ではそれぞれ独立しており、その境界を越えて依存関係を学習することはできない\n","- 例えばFとGの依存関係を学習することはできない\n","- それでも大量にデータを入力して学習させるため問題とはならない\n","- バッチ処理を有効に活用した方が計算効率が高く、その方がメリットが大きい\n"]},{"cell_type":"markdown","source":["Google Colaboratoryにはtorchdataがないので、インストール\n","\n","ランタイムを再起動させる必要があるかもしれないので注意"],"metadata":{"id":"g2bddT_ckgOJ"}},{"cell_type":"code","source":["!pip install torchdata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO-sHgYUkay5","executionInfo":{"status":"ok","timestamp":1724615202846,"user_tz":-540,"elapsed":60385,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"ce602578-d130-41cb-ac8f-ea013b2765e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchdata\n","  Downloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n","Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n","Downloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torchdata-0.8.0\n"]}]},{"cell_type":"markdown","source":["必要なライブラリの読み込み\n","\n"],"metadata":{"id":"840dAvHobWkf"}},{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXflwEXKY5Gk","executionInfo":{"status":"ok","timestamp":1724615207336,"user_tz":-540,"elapsed":4499,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"1e6ebb67-c61f-4792-9969-51d4fcca0451"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.20)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"]}]},{"cell_type":"markdown","source":["`portalocker.Lock`でエラーが発生するため、torchtext.datasetsを読み込む前にportalockerをインストールする必要がある\n","- こういうノウハウ的なところはひとまず気にしなくてよい\n"],"metadata":{"id":"RgUg9xlpgrkA"}},{"cell_type":"code","source":["from torchtext.datasets import WikiText2\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator"],"metadata":{"id":"xF_hfy6Jkd82","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724615207336,"user_tz":-540,"elapsed":5,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"c581584d-d085-4136-a87f-89aa0adfcda7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"markdown","source":["`data_process`は、テキストをテンソル列に変換する\n","\n","`batchfy`は、データをbsz個のシーケンスに分割し，きれいに収まらない余分な要素を削除する\n","- data: Tensor, shape [N]\n","- bsz: int, batch size\n","- 戻り値: Tensor of shape [N // bsz, bsz]"],"metadata":{"id":"T0MO7YeoZi-P"}},{"cell_type":"markdown","source":["**<font color=\"red\">+++注意+++</font>**\n","\n","次のエラーが出力される場合、ランタイムを再起動して対処してください。\n","\n","最初からやり直すと時間がかかるため、「PyTorch Transformerを用いた単語予測」から以降をランタイムを再起動(Ctrl+M .[ピリオド])して、再実行(Ctrl+F10)するとよい\n","\n","\n","```\n","AttributeError: 'NoneType' object has no attribute 'Lock'\n","This exception is thrown by __iter__ of _MemoryCellIterDataPipe(remember_elements=1000, source_datapipe=_ChildDataPipe)\n","```\n","\n"],"metadata":{"id":"udO417RDdtbO"}},{"cell_type":"code","source":["!pip install portalocker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlYcnBLGwfnB","executionInfo":{"status":"ok","timestamp":1724615211658,"user_tz":-540,"elapsed":4324,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"249cb7da-1b4b-44b3-a71b-356db88b810c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting portalocker\n","  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n","Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n","Installing collected packages: portalocker\n","Successfully installed portalocker-2.10.1\n"]}]},{"cell_type":"markdown","source":["https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip"],"metadata":{"id":"7iS236jPFzH-"}},{"cell_type":"code","source":["import torch\n","from torchtext.datasets import WikiText2\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Assuming you have downloaded and extracted the WikiText2 dataset to './wikitext-2'\n","# Adjust the path if necessary\n","data_dir = './wikitext-2'\n","\n","# Load the data from local files\n","def load_wikitext2_from_file(split):\n","    with open(f'{data_dir}/wiki.{split}.tokens', 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    return text.splitlines()\n","\n","train_text = load_wikitext2_from_file('train')\n","val_text = load_wikitext2_from_file('valid')\n","test_text = load_wikitext2_from_file('test')\n","\n","# Tokenize and build vocabulary\n","tokenizer = get_tokenizer('basic_english')\n","vocab = build_vocab_from_iterator(map(tokenizer, train_text), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])\n","\n","# Create a custom dataset class\n","class WikiText2Dataset(Dataset):\n","    def __init__(self, text, tokenizer, vocab):\n","        self.text = text\n","        self.tokenizer = tokenizer\n","        self.vocab = vocab\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.text[idx])\n","        return torch.tensor(self.vocab(tokens), dtype=torch.long)\n","\n","# Create datasets\n","train_dataset = WikiText2Dataset(train_text, tokenizer, vocab)\n","val_dataset = WikiText2Dataset(val_text, tokenizer, vocab)\n","test_dataset = WikiText2Dataset(test_text, tokenizer, vocab)\n","\n","# Create dataloaders\n","# (You can adjust batch size and other parameters as needed)\n","train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=10)\n","test_dataloader = DataLoader(test_dataset, batch_size=10)"],"metadata":{"id":"S8NVA_ImJRSw","executionInfo":{"status":"error","timestamp":1724616363357,"user_tz":-540,"elapsed":759,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"f527639e-ea7a-4b7e-f7ac-a44b15ea9b77","colab":{"base_uri":"https://localhost:8080/","height":332}},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './wikitext-2/wiki.train.tokens'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-d7703b57ac64>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wikitext2_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wikitext2_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wikitext2_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-d7703b57ac64>\u001b[0m in \u001b[0;36mload_wikitext2_from_file\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the data from local files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_wikitext2_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{data_dir}/wiki.{split}.tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './wikitext-2/wiki.train.tokens'"]}]},{"cell_type":"code","source":["train_iter = WikiText2(split='train')\n","tokenizer = get_tokenizer('basic_english')\n","vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])\n","\n","def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n","    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n","    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n","\n","# train_iter was \"consumed\" by the process of building the vocab,\n","# so we have to create it again\n","train_iter, val_iter, test_iter = WikiText2()\n","train_data = data_process(train_iter)\n","val_data = data_process(val_iter)\n","test_data = data_process(test_iter)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def batchify(data: Tensor, bsz: int) -> Tensor:\n","    seq_len = data.size(0) // bsz\n","    data = data[:seq_len * bsz]\n","    data = data.view(bsz, seq_len).t().contiguous()\n","    return data.to(device)\n","\n","batch_size = 20\n","eval_batch_size = 10\n","train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n","val_data = batchify(val_data, eval_batch_size)\n","test_data = batchify(test_data, eval_batch_size)"],"metadata":{"id":"ap-_QY18YqBH","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1724616271549,"user_tz":-540,"elapsed":1055,"user":{"displayName":"西宏章","userId":"00237858890977261979"}},"outputId":"9cd91654-dbc9-4a89-d3a4-1c26b82bc304"},"execution_count":null,"outputs":[{"output_type":"error","ename":"HTTPError","evalue":"403 Client Error: Forbidden for url: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\nThis exception is thrown by __iter__ of HTTPReaderIterDataPipe(skip_on_error=False, source_datapipe=OnDiskCacheHolderIterDataPipe, timeout=None)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-ecddafa23f00>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikiText2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vocab_factory.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/sharding.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_of_instances\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combinatorics.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/plain_text_reader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStr_Or_Bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStr_Or_Bytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_newline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/fileopener.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# a subtype would cause mypy error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mget_file_binaries_from_pathnames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\u001b[0m in \u001b[0;36mget_file_binaries_from_pathnames\u001b[0;34m(pathnames, mode, encoding)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpathname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpathnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected string type for pathname, but got {type(pathname)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/cacheholder.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mrec_uuid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_cell_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0moriginal_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_filepath_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/saver.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/callable.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/callable.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/selecting.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_returnIfTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/ziparchiveloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBufferedIOBase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mvalidate_pathname_binary_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/fileopener.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# a subtype would cause mypy error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mget_file_binaries_from_pathnames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py\u001b[0m in \u001b[0;36mget_file_binaries_from_pathnames\u001b[0;34m(pathnames, mode, encoding)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpathname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpathnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected string type for pathname, but got {type(pathname)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/cacheholder.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_caching_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;31m# In case of BC breaking, use RuntimeError for now. Warning is another option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/cacheholder.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mitem_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember_elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Decided against using `contextlib.nullcontext` for performance reasons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36m_get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Return next with logic related to iterator validity, profiler, and incrementation of samples yielded.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0m_check_iterator_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_and_has_next_method\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_of_samples_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36mget_next_element_by_instance\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_stop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36m_find_next\u001b[0;34m(self, instance_id)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;34m\"_datapipe_iterator has not been set, likely because this private method is called directly \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \"without invoking get_next_element_by_instance() first.\")\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datapipe_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/cacheholder.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mrec_uuid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_cell_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0moriginal_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_filepath_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/saver.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/hashchecker.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStreamWrapper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sha256\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mhash_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/callable.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/callable.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/_hook_iterator.py\u001b[0m in \u001b[0;36mwrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/load/online.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_datapipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0m_get_response_from_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/load/online.py\u001b[0m in \u001b[0;36m_get_response_from_http\u001b[0;34m(url, timeout, **query_params)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStreamWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\nThis exception is thrown by __iter__ of HTTPReaderIterDataPipe(skip_on_error=False, source_datapipe=OnDiskCacheHolderIterDataPipe, timeout=None)"]}]},{"cell_type":"markdown","metadata":{"id":"_lu9vP19sP9W"},"source":["## 入力シーケンスとTargetシーケンスの生成\n","\n","``get_batch()``関数を用いてTransformerモデルの入力シーケンスと、Targetシーケンスを生成する\n","- ソースデータを変数``bptt``の長さのチャンクデータに細分化する\n","- 今回の言語モデルタスクでは、入力シーケンスの続きとなる単語をTargetとして学習させる\n","\n","例えば``bptt``が2の場合は後続する2 つの要素を取得する\n","\n","- ``get_batch()``関数の返り値``data``の0次元目がチャンクの長さでを表し、Transformerモデルの次元Sと一致する\n","- dataの1次元目はバッチサイズを示す次元数Nである\n","\n","- 引数\n","  - source: [full_seq_len, batch_size]のテンソル\n","  - i: 整数\n","- 返り値\n","  - (data, target)\n","  - [seq_len, batch_size]のテンソルであるdataと、[seq_len * batch_size]のテンソルであるtarget"]},{"cell_type":"code","metadata":{"id":"vwlWJxBHPPeE"},"source":["bptt = 35\n","def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].reshape(-1)\n","    return data, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1crAe63wcYAF"},"source":["インスタンスの初期化\n","--------------------"]},{"cell_type":"markdown","metadata":{"id":"UMeu_Zh7cbH7"},"source":["モデルは、以下のハイパーパラメータを使用して設定されます。vocabのサイズは、vocabオブジェクトの長さと同じです。"]},{"cell_type":"code","metadata":{"id":"n8uZqoGWPPeI"},"source":["ntokens = len(vocab)  # 語彙サイズ\n","emsize = 200  # 埋め込み次元数\n","d_hid = 200  # forwardネットワークモデルのnn.TransformerEncoderの次元\n","nlayers = 2  # nn.TransformerEncoderのnn.TransformerEncoderLayerの数\n","nhead = 2  # nn.MultiheadAttentionにおけるheadの数\n","dropout = 0.2  # dropoutの割合\n","model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["語彙数を確認する"],"metadata":{"id":"syHb9YFHmReQ"}},{"cell_type":"code","source":["ntokens"],"metadata":{"id":"aA58Lil2mPNS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xx-EHjPUf7bE"},"source":["モデルの実行\n","-------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aoF0qhd7f7nX"},"source":["- CrossEntropyLossとSDGを用いる\n","- 初期の学習率は5.0としている\n","- エポック単位で学習率を調整する\n","  - StepLRを使用して学習率を調整する\n","  - 訓練中は、勾配爆発を防ぐためnn.utils.clip_grad_normを用いて学習率を調整する"]},{"cell_type":"code","metadata":{"id":"RjC4emUHPPeL"},"source":["import copy\n","import time\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = 5.0  # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","def train(model: nn.Module) -> None:\n","    model.train()  # turn on train mode\n","    total_loss = 0.\n","    log_interval = 200\n","    start_time = time.time()\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","\n","    num_batches = len(train_data) // bptt\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, i)\n","        seq_len = data.size(0)\n","        if seq_len != bptt:  # only on last batch\n","            src_mask = src_mask[:seq_len, :seq_len]\n","        output = model(data, src_mask)\n","        loss = criterion(output.view(-1, ntokens), targets)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if batch % log_interval == 0 and batch > 0:\n","            lr = scheduler.get_last_lr()[0]\n","            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n","            cur_loss = total_loss / log_interval\n","            ppl = math.exp(cur_loss)\n","            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n","                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n","                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n","    model.eval()  # turn on evaluation mode\n","    total_loss = 0.\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","    with torch.no_grad():\n","        for i in range(0, eval_data.size(0) - 1, bptt):\n","            data, targets = get_batch(eval_data, i)\n","            seq_len = data.size(0)\n","            if seq_len != bptt:\n","                src_mask = src_mask[:seq_len, :seq_len]\n","            output = model(data, src_mask)\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += seq_len * criterion(output_flat, targets).item()\n","    return total_loss / (len(eval_data) - 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgE9j1KumZTK"},"source":["エポックを繰り返す\n","\n","- 検証データの損失がそれまでの実行のなかで最も良い(低い)場合は当該モデルを保存する\n","- 各エポックの後に学習率を調整する(小さくする)"]},{"cell_type":"code","metadata":{"id":"4Ws6fSWbPPeO"},"source":["best_val_loss = float('inf')\n","epochs = 10\n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train(model)\n","    val_loss = evaluate(model, val_data)\n","    val_ppl = math.exp(val_loss)\n","    elapsed = time.time() - epoch_start_time\n","    print('-' * 89)\n","    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n","          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = copy.deepcopy(model)\n","\n","    scheduler.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVn3Ydd3nLWm"},"source":["テストデータセットでモデルを評価する\n","-------------------------------------\n","\n","結果を確認するために、ベストモデルでテスト用データセットを評価する\n"]},{"cell_type":"code","metadata":{"id":"2sL5nPiHPPeU"},"source":["test_loss = evaluate(best_model, test_data)\n","test_ppl = math.exp(test_loss)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | '\n","      f'test ppl {test_ppl:8.2f}')\n","print('=' * 89)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaltext = \"I have enjoyed learning about machine learning so much that I want to continue learning even after the class is over.\"\n","evaltext = tokenizer(evaltext)\n","evaltext"],"metadata":{"id":"vzutwiIEqrN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evalvcab = vocab(evaltext)\n","evallen = len(evalvcab)\n","evalvcab"],"metadata":{"id":"zMS0q6rqqt8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evalmask = torch.zeros(evallen)\n","evalmask = generate_square_subsequent_mask(len(evalvcab)).to(device)"],"metadata":{"id":"oxSTq8By34vC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","evalout = model(torch.tensor(evalvcab).to(device), evalmask.to(device))"],"metadata":{"id":"eF3DlgIKq-UN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evalcode = list()\n","for i in range(evallen):\n","  evalcode.append(torch.argmax(evalout[i], dim=1)[i])"],"metadata":{"id":"TMTdtTYgwt8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","evalcode = list()\n","#np.argsort(evalout[0][0])\n","for i in range(evallen):\n","  evalline = evalout[i][i].to('cpu').detach().numpy().copy()\n","  evalsort = np.argsort(-evalline)\n","  evalcode.append(evalsort)\n","evalcode"],"metadata":{"id":"h1tW6Q5K0rsh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["実はあまりうまくいっていない\n","- 気持ちはわからなくもない\n","- 一般にTransformerは大量のデータを用いて時間をかけて学習することで精度が良くなることが知られている"],"metadata":{"id":"ogkE2yYJypcm"}},{"cell_type":"code","source":["import pandas as pd\n","tbl = list()\n","for i, id in enumerate(evalcode):\n","  tbl.append([evalvcab[i], vocab.lookup_token(evalvcab[i]),\n","    id[0].tolist(), vocab.lookup_token(id[0]),\n","    id[1].tolist(), vocab.lookup_token(id[1]),\n","    id[2].tolist(), vocab.lookup_token(id[2]),\n","    id[3].tolist(), vocab.lookup_token(id[3]),\n","    id[4].tolist(), vocab.lookup_token(id[4])])\n","df = pd.DataFrame(tbl)\n","df"],"metadata":{"id":"t4L_LDsgxjt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","> The language of poetry is so dense, so multivalent, that it demands a concentrated act of attention\n","> — and offers its greatest rewards only to those who reread.\n",">\n","> (Ezra Pound)\n",">\n","> 詩の言葉はあまりに密集し、様々な意味を持つため、集中的な意識を払うことが求められる。そして、\n","> 再読をする者にだけ最大の報酬を提供するのである。\n",">\n","> (エズラ・パウンド)\n","---"],"metadata":{"id":"wA0Xs4PhR43f"}},{"cell_type":"markdown","source":["# 課題(Multi-Head Attention)\n","次のMulti-Head Attentionのコードを参考に、Single-Head AttentionをMulti-Head Attentionに入れ替えて実行させて制度などの変化を確認しなさい"],"metadata":{"id":"S-8tc9Wzy5Xv"}},{"cell_type":"code","source":["def scaled_dot_product(q, k, v, mask=None):\n","    d_k = q.size()[-1]\n","    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n","    attn_logits = attn_logits / math.sqrt(d_k)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)\n","    return values, attention\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension cannot be equal to the number of heads as modulo.\"\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n","        self.o_proj = nn.Linear(embed_dim, embed_dim)\n","        self._reset_parameters()\n","    def forward(self, x, mask=None, return_attention=False):\n","        batch_size, seq_length, _ = x.size()\n","        qkv = self.qkv_proj(x)\n","        # Separate Q, K, V from linear output\n","        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n","        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n","        q, k, v = qkv.chunk(3, dim=-1)\n","        # Determine value outputs\n","        values, attention = scaled_dot_product(q, k, v, mask=mask)\n","        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n","        values = values.reshape(batch_size, seq_length, self.embed_dim)\n","        o = self.o_proj(values)\n","        if return_attention:\n","            return o, attention\n","        else:\n","            return o"],"metadata":{"id":"XYkoQ03Qy7F4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 課題(PyTorch Transformerを用いた単語予測)\n","\n","- 日本語による単語補完にトライしてみよう\n"],"metadata":{"id":"U1Px28MVROMY"}}]}