{"cells":[{"cell_type":"markdown","metadata":{"id":"kIeTGMf-5FLz"},"source":["---\n",">「見えるぞ！私にも敵が見える！」\\\n",">シャア・アズナブル\n","---"]},{"cell_type":"markdown","metadata":{"id":"L60c_B8JzPsZ"},"source":["# 物体検出\n"]},{"cell_type":"markdown","metadata":{"id":"6FAe2Jv6WpaL"},"source":["## 物体検出手法\n","\n","画像の中の複数の物体を認識し、その場所をBounding Boxと呼ばれる四角で示す\n","- さらにセグメンテーションという手法もあり、こちらは塗り分ける\n","\n","物体検出手法には著名な手法が多く、シンプルにまとめると次のようになる\n","\n","- Yolo\n","  - 一連の物体検出手法YOLO（You Only Look Once）の1つで、v2と比較して特に小さいものの検出精度が改善された\n","  - Yolo V1からYolo V3まで同一作者により設計され、その後は異なる作者によりYolo V4およびYolo V5が提案されている\n","    - 従って、Yolo V3までがYoloという研究者もいる\n","  - 傾向として、FPS(認識フレームレート)が速く、その割に精度が高いことを特徴としている\n","  - 小さい解像度から大きい解像度に情報を伝搬して物体を検出する構造を有する\n","\n","- SSD\n","  - YOLOと並び称されることが多いが、SSDではそれぞれの解像度から直接物体を検出する構造を有する\n","  - 構造的にはSSDもYOLO V3も大きな違いはなく、SSDは情報を一方向のみにしか伝えていないが、YoloV3はGlobal情報をLocal情報の組み合わせを実現している\n","\n","論文記載のそれぞれのネットワーク構造は次のとおりである\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/ssd-yolo.png\" width=500>\n","\n","ここでは、YoloV3を扱い、実際に実装する\n"]},{"cell_type":"markdown","metadata":{"id":"yc1lcfMf6STm"},"source":["# YoLoの設計"]},{"cell_type":"markdown","metadata":{"id":"1nEH4XeruUIL"},"source":["## YoLoの原理\n","\n","画像ごと異なる対象物体の大きさの違いに対応するため、小スケール(Scale1)$13 \\times 13$、中スケール$26 \\times 26$、大スケール$52 \\times 52$の3つの対象の異なる3次元テンソルを得るように学習する\n","- 例えば、図のScale 1に相当する$13 \\times 13$のスケールでは、$13 \\times 13$であることから入力画像を$13\\times 13$のグリッドで分割する\n","- さらに各グリッドで3つのBoxを予測するため、これをBox1, Box2, Box3とする\n","- 各Boxで得られる(得るべき)情報は、Bounding Boxやラベルなどの情報である\n","  - Bouding boxは、$(tx, ty, tw, th)$で要素数4\n","  - 検出のObjectness Score(検出対称であるかどうかを表すフラグ)で要素数1\n","  - 80クラス分類でワンホットのため要素数80\n","- Box一つで$3\\times(4+1+80)$つまり255次元の情報が獲得され、これが$13 \\times 13$個存在するため、結果的に$13 \\times 13 \\times 255$となる\n","- 他も同様である\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yololearn.png\" width=800>\n","\n","上図において、仮にGround TruthのBouding Boxの中心があるグリッドセルの中、例えば、鳥の画像の赤いBoxにあったばあい、該当する物体のBoxの予測はこの赤いグリッドセルが担当する\n","- つまり、同じグリッドセルは3つの物質以上を担当できないという制限がある\n","- Ground Truth とは学習やテストに使用する実データのこと\n","\n","赤いセルは鳥のBoxを予測することになり、Objectness Scoreが1に、ほかのセルは0になる\n","- 各セルは予めサイズの異なる3つのPrior Box(S, M, L)が割り当てられており、Ground Truthと一番大きさの近いPrior Boxを選択する\n","  - 実際は、Ground truthであるBounding Boxと重なった面積が一番大きいPrior Boxを選択する\n","- 更に座標オフセットの調整も行う"]},{"cell_type":"markdown","metadata":{"id":"-xbyVz9RW47W"},"source":["## Yoloのモデル\n","\n","### 全体の構造\n","まず、YoLoV3のモデル構造を示す\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yolov3.png\" witdh=600>\n","\n","このように、主に75層の畳み込み層から構成されており、先に述べた物体の特徴量を抽出する\n","- Fully connected層を利用していないため、入力画像のサイズを任意に設定できる\n","- Pooling Layerを用いず、Strideが2の畳み込み層を用いることでダウンサンプリングを行う\n","  - スケール不変な特徴量を次の層までへ伝播できる\n","- ResNetとFPN構造を利用して検出精度を向上している\n","  - 図のResidual BlockがResNet構造に相当し、Shortcut Pathを有する構造である\n","  - 層が深くなると学習が困難となるが、Shortcut Pathによりある層における最適な出力を学習するのではなく、前層の入力を参照した残差関数を学習するようになり、特徴量の学習をしやすくなる\n","  - これにより、複雑な特徴量H(x)が古い特徴量xに新しく学習した残差F(x)を足し合わせる形で構成される(階差数列と似た概念)\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloresnet.png\" width=500>\n","\n","### 異なるスケールへの対応\n","特徴マップはネットワークの深さとともに縮小するため、小さい物体の検出もそれに呼応して難易度が上がる\n","- 直感的に大きさによって異なる段階で物体を検出するとよいと考えるのがSSD\n","\n","しかしながら、各特徴マップにおける特徴量は深さによって変化しているため、そう簡単ではない\n","- 最初は低レベルなエッジ、カラー、物体の大まかな位置などの特徴量を学習しているが、層の深くなるにつれてハイレベルな物体情報である犬、猫、車といった特徴量の学習に変化するため、検出にはどうしてもハイレベルな特徴量が必要となる\n","- これがSSDの弱点\n","\n","そこで、YOLOv3では、スケール対策としてFeature Pylamid Network(FPN)構造を利用する\n","- 全体ネットワーク図において、Conv Blockの$13\\times 13$から、$26 \\times 26$にConcatenateするパスや、さらにそれを$52 \\times 52$にConcatenateするパスが存在するなど、大から小のパスと、小から大の双方向のパスが存在する\n","\n","### Softmaxを利用しない\n","\n","多値分類では、ワンホット化しSoftmaxを利用する場合が殆どである\n","- しかしながら、分類数が多くなると例えばwomanとpersonのようにラベルが重なることがあり、完全にワンホットにすることが困難となる場合も想定される\n","- そこで、logistic関数を利用している"]},{"cell_type":"markdown","metadata":{"id":"J08CAylYHfX4"},"source":["# YoLoの実装"]},{"cell_type":"markdown","metadata":{"id":"Ko6azS8vHjAK"},"source":["## データの準備\n","\n","今回は、COCO128を利用する\n","- データセットとしてはそれほど大きくない"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5728,"status":"ok","timestamp":1765091462997,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"N7fE2yGJrUXu","outputId":"1b12ae21-4a03-49ae-b9ea-fb588690d96f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-07 07:10:57--  https://drive.google.com/uc?export=download&id=141bVQDo-x421CMN0o9vS3SkIY75kbi54\n","Resolving drive.google.com (drive.google.com)... 142.251.2.100, 142.251.2.113, 142.251.2.139, ...\n","Connecting to drive.google.com (drive.google.com)|142.251.2.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=141bVQDo-x421CMN0o9vS3SkIY75kbi54&export=download [following]\n","--2025-12-07 07:10:57--  https://drive.usercontent.google.com/download?id=141bVQDo-x421CMN0o9vS3SkIY75kbi54&export=download\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22088551 (21M) [application/octet-stream]\n","Saving to: ‘COCO128.zip’\n","\n","COCO128.zip         100%[===================>]  21.06M  56.1MB/s    in 0.4s    \n","\n","2025-12-07 07:11:02 (56.1 MB/s) - ‘COCO128.zip’ saved [22088551/22088551]\n","\n","Archive:  COCO128.zip\n","  inflating: coco128/LICENSE         \n","  inflating: coco128/README.txt      \n","  inflating: coco128/images/train2017/000000000009.jpg  \n","  inflating: coco128/images/train2017/000000000025.jpg  \n","  inflating: coco128/images/train2017/000000000030.jpg  \n","  inflating: coco128/images/train2017/000000000034.jpg  \n","  inflating: coco128/images/train2017/000000000036.jpg  \n","  inflating: coco128/images/train2017/000000000042.jpg  \n","  inflating: coco128/images/train2017/000000000049.jpg  \n","  inflating: coco128/images/train2017/000000000061.jpg  \n","  inflating: coco128/images/train2017/000000000064.jpg  \n","  inflating: coco128/images/train2017/000000000071.jpg  \n","  inflating: coco128/images/train2017/000000000072.jpg  \n","  inflating: coco128/images/train2017/000000000073.jpg  \n","  inflating: coco128/images/train2017/000000000074.jpg  \n","  inflating: coco128/images/train2017/000000000077.jpg  \n","  inflating: coco128/images/train2017/000000000078.jpg  \n","  inflating: coco128/images/train2017/000000000081.jpg  \n","  inflating: coco128/images/train2017/000000000086.jpg  \n","  inflating: coco128/images/train2017/000000000089.jpg  \n","  inflating: coco128/images/train2017/000000000092.jpg  \n","  inflating: coco128/images/train2017/000000000094.jpg  \n","  inflating: coco128/images/train2017/000000000109.jpg  \n","  inflating: coco128/images/train2017/000000000110.jpg  \n","  inflating: coco128/images/train2017/000000000113.jpg  \n","  inflating: coco128/images/train2017/000000000127.jpg  \n","  inflating: coco128/images/train2017/000000000133.jpg  \n","  inflating: coco128/images/train2017/000000000136.jpg  \n","  inflating: coco128/images/train2017/000000000138.jpg  \n","  inflating: coco128/images/train2017/000000000142.jpg  \n","  inflating: coco128/images/train2017/000000000143.jpg  \n","  inflating: coco128/images/train2017/000000000144.jpg  \n","  inflating: coco128/images/train2017/000000000149.jpg  \n","  inflating: coco128/images/train2017/000000000151.jpg  \n","  inflating: coco128/images/train2017/000000000154.jpg  \n","  inflating: coco128/images/train2017/000000000164.jpg  \n","  inflating: coco128/images/train2017/000000000165.jpg  \n","  inflating: coco128/images/train2017/000000000192.jpg  \n","  inflating: coco128/images/train2017/000000000194.jpg  \n","  inflating: coco128/images/train2017/000000000196.jpg  \n","  inflating: coco128/images/train2017/000000000201.jpg  \n","  inflating: coco128/images/train2017/000000000208.jpg  \n","  inflating: coco128/images/train2017/000000000241.jpg  \n","  inflating: coco128/images/train2017/000000000247.jpg  \n","  inflating: coco128/images/train2017/000000000250.jpg  \n","  inflating: coco128/images/train2017/000000000257.jpg  \n","  inflating: coco128/images/train2017/000000000260.jpg  \n","  inflating: coco128/images/train2017/000000000263.jpg  \n","  inflating: coco128/images/train2017/000000000283.jpg  \n","  inflating: coco128/images/train2017/000000000294.jpg  \n","  inflating: coco128/images/train2017/000000000307.jpg  \n","  inflating: coco128/images/train2017/000000000308.jpg  \n","  inflating: coco128/images/train2017/000000000309.jpg  \n","  inflating: coco128/images/train2017/000000000312.jpg  \n","  inflating: coco128/images/train2017/000000000315.jpg  \n","  inflating: coco128/images/train2017/000000000321.jpg  \n","  inflating: coco128/images/train2017/000000000322.jpg  \n","  inflating: coco128/images/train2017/000000000326.jpg  \n","  inflating: coco128/images/train2017/000000000328.jpg  \n","  inflating: coco128/images/train2017/000000000332.jpg  \n","  inflating: coco128/images/train2017/000000000338.jpg  \n","  inflating: coco128/images/train2017/000000000349.jpg  \n","  inflating: coco128/images/train2017/000000000357.jpg  \n","  inflating: coco128/images/train2017/000000000359.jpg  \n","  inflating: coco128/images/train2017/000000000360.jpg  \n","  inflating: coco128/images/train2017/000000000368.jpg  \n","  inflating: coco128/images/train2017/000000000370.jpg  \n","  inflating: coco128/images/train2017/000000000382.jpg  \n","  inflating: coco128/images/train2017/000000000384.jpg  \n","  inflating: coco128/images/train2017/000000000387.jpg  \n","  inflating: coco128/images/train2017/000000000389.jpg  \n","  inflating: coco128/images/train2017/000000000394.jpg  \n","  inflating: coco128/images/train2017/000000000395.jpg  \n","  inflating: coco128/images/train2017/000000000397.jpg  \n","  inflating: coco128/images/train2017/000000000400.jpg  \n","  inflating: coco128/images/train2017/000000000404.jpg  \n","  inflating: coco128/images/train2017/000000000415.jpg  \n","  inflating: coco128/images/train2017/000000000419.jpg  \n","  inflating: coco128/images/train2017/000000000428.jpg  \n","  inflating: coco128/images/train2017/000000000431.jpg  \n","  inflating: coco128/images/train2017/000000000436.jpg  \n","  inflating: coco128/images/train2017/000000000438.jpg  \n","  inflating: coco128/images/train2017/000000000443.jpg  \n","  inflating: coco128/images/train2017/000000000446.jpg  \n","  inflating: coco128/images/train2017/000000000450.jpg  \n","  inflating: coco128/images/train2017/000000000459.jpg  \n","  inflating: coco128/images/train2017/000000000471.jpg  \n","  inflating: coco128/images/train2017/000000000472.jpg  \n","  inflating: coco128/images/train2017/000000000474.jpg  \n","  inflating: coco128/images/train2017/000000000486.jpg  \n","  inflating: coco128/images/train2017/000000000488.jpg  \n","  inflating: coco128/images/train2017/000000000490.jpg  \n","  inflating: coco128/images/train2017/000000000491.jpg  \n","  inflating: coco128/images/train2017/000000000502.jpg  \n","  inflating: coco128/images/train2017/000000000508.jpg  \n","  inflating: coco128/images/train2017/000000000510.jpg  \n","  inflating: coco128/images/train2017/000000000514.jpg  \n","  inflating: coco128/images/train2017/000000000520.jpg  \n","  inflating: coco128/images/train2017/000000000529.jpg  \n","  inflating: coco128/images/train2017/000000000531.jpg  \n","  inflating: coco128/images/train2017/000000000532.jpg  \n","  inflating: coco128/images/train2017/000000000536.jpg  \n","  inflating: coco128/images/train2017/000000000540.jpg  \n","  inflating: coco128/images/train2017/000000000542.jpg  \n","  inflating: coco128/images/train2017/000000000544.jpg  \n","  inflating: coco128/images/train2017/000000000560.jpg  \n","  inflating: coco128/images/train2017/000000000562.jpg  \n","  inflating: coco128/images/train2017/000000000564.jpg  \n","  inflating: coco128/images/train2017/000000000569.jpg  \n","  inflating: coco128/images/train2017/000000000572.jpg  \n","  inflating: coco128/images/train2017/000000000575.jpg  \n","  inflating: coco128/images/train2017/000000000581.jpg  \n","  inflating: coco128/images/train2017/000000000584.jpg  \n","  inflating: coco128/images/train2017/000000000589.jpg  \n","  inflating: coco128/images/train2017/000000000590.jpg  \n","  inflating: coco128/images/train2017/000000000595.jpg  \n","  inflating: coco128/images/train2017/000000000597.jpg  \n","  inflating: coco128/images/train2017/000000000599.jpg  \n","  inflating: coco128/images/train2017/000000000605.jpg  \n","  inflating: coco128/images/train2017/000000000612.jpg  \n","  inflating: coco128/images/train2017/000000000620.jpg  \n","  inflating: coco128/images/train2017/000000000623.jpg  \n","  inflating: coco128/images/train2017/000000000625.jpg  \n","  inflating: coco128/images/train2017/000000000626.jpg  \n","  inflating: coco128/images/train2017/000000000629.jpg  \n","  inflating: coco128/images/train2017/000000000634.jpg  \n","  inflating: coco128/images/train2017/000000000636.jpg  \n","  inflating: coco128/images/train2017/000000000641.jpg  \n","  inflating: coco128/images/train2017/000000000643.jpg  \n","  inflating: coco128/images/train2017/000000000650.jpg  \n","  inflating: coco128/labels/train2017/000000000009.txt  \n","  inflating: coco128/labels/train2017/000000000025.txt  \n","  inflating: coco128/labels/train2017/000000000030.txt  \n","  inflating: coco128/labels/train2017/000000000034.txt  \n","  inflating: coco128/labels/train2017/000000000036.txt  \n","  inflating: coco128/labels/train2017/000000000042.txt  \n","  inflating: coco128/labels/train2017/000000000049.txt  \n","  inflating: coco128/labels/train2017/000000000061.txt  \n","  inflating: coco128/labels/train2017/000000000064.txt  \n","  inflating: coco128/labels/train2017/000000000071.txt  \n","  inflating: coco128/labels/train2017/000000000072.txt  \n","  inflating: coco128/labels/train2017/000000000073.txt  \n","  inflating: coco128/labels/train2017/000000000074.txt  \n","  inflating: coco128/labels/train2017/000000000077.txt  \n","  inflating: coco128/labels/train2017/000000000078.txt  \n","  inflating: coco128/labels/train2017/000000000081.txt  \n","  inflating: coco128/labels/train2017/000000000086.txt  \n","  inflating: coco128/labels/train2017/000000000089.txt  \n","  inflating: coco128/labels/train2017/000000000092.txt  \n","  inflating: coco128/labels/train2017/000000000094.txt  \n","  inflating: coco128/labels/train2017/000000000109.txt  \n","  inflating: coco128/labels/train2017/000000000110.txt  \n","  inflating: coco128/labels/train2017/000000000113.txt  \n","  inflating: coco128/labels/train2017/000000000127.txt  \n","  inflating: coco128/labels/train2017/000000000133.txt  \n","  inflating: coco128/labels/train2017/000000000136.txt  \n","  inflating: coco128/labels/train2017/000000000138.txt  \n","  inflating: coco128/labels/train2017/000000000142.txt  \n","  inflating: coco128/labels/train2017/000000000143.txt  \n","  inflating: coco128/labels/train2017/000000000144.txt  \n","  inflating: coco128/labels/train2017/000000000149.txt  \n","  inflating: coco128/labels/train2017/000000000151.txt  \n","  inflating: coco128/labels/train2017/000000000154.txt  \n","  inflating: coco128/labels/train2017/000000000164.txt  \n","  inflating: coco128/labels/train2017/000000000165.txt  \n","  inflating: coco128/labels/train2017/000000000192.txt  \n","  inflating: coco128/labels/train2017/000000000194.txt  \n","  inflating: coco128/labels/train2017/000000000196.txt  \n","  inflating: coco128/labels/train2017/000000000201.txt  \n","  inflating: coco128/labels/train2017/000000000208.txt  \n","  inflating: coco128/labels/train2017/000000000241.txt  \n","  inflating: coco128/labels/train2017/000000000247.txt  \n","  inflating: coco128/labels/train2017/000000000250.txt  \n","  inflating: coco128/labels/train2017/000000000257.txt  \n","  inflating: coco128/labels/train2017/000000000260.txt  \n","  inflating: coco128/labels/train2017/000000000263.txt  \n","  inflating: coco128/labels/train2017/000000000283.txt  \n","  inflating: coco128/labels/train2017/000000000294.txt  \n","  inflating: coco128/labels/train2017/000000000307.txt  \n","  inflating: coco128/labels/train2017/000000000308.txt  \n","  inflating: coco128/labels/train2017/000000000309.txt  \n","  inflating: coco128/labels/train2017/000000000312.txt  \n","  inflating: coco128/labels/train2017/000000000315.txt  \n","  inflating: coco128/labels/train2017/000000000321.txt  \n","  inflating: coco128/labels/train2017/000000000322.txt  \n","  inflating: coco128/labels/train2017/000000000326.txt  \n","  inflating: coco128/labels/train2017/000000000328.txt  \n","  inflating: coco128/labels/train2017/000000000332.txt  \n","  inflating: coco128/labels/train2017/000000000338.txt  \n","  inflating: coco128/labels/train2017/000000000349.txt  \n","  inflating: coco128/labels/train2017/000000000357.txt  \n","  inflating: coco128/labels/train2017/000000000359.txt  \n","  inflating: coco128/labels/train2017/000000000360.txt  \n","  inflating: coco128/labels/train2017/000000000368.txt  \n","  inflating: coco128/labels/train2017/000000000370.txt  \n","  inflating: coco128/labels/train2017/000000000382.txt  \n","  inflating: coco128/labels/train2017/000000000384.txt  \n","  inflating: coco128/labels/train2017/000000000387.txt  \n","  inflating: coco128/labels/train2017/000000000389.txt  \n","  inflating: coco128/labels/train2017/000000000394.txt  \n","  inflating: coco128/labels/train2017/000000000395.txt  \n","  inflating: coco128/labels/train2017/000000000397.txt  \n","  inflating: coco128/labels/train2017/000000000400.txt  \n","  inflating: coco128/labels/train2017/000000000404.txt  \n","  inflating: coco128/labels/train2017/000000000415.txt  \n","  inflating: coco128/labels/train2017/000000000419.txt  \n","  inflating: coco128/labels/train2017/000000000428.txt  \n","  inflating: coco128/labels/train2017/000000000431.txt  \n","  inflating: coco128/labels/train2017/000000000436.txt  \n","  inflating: coco128/labels/train2017/000000000438.txt  \n","  inflating: coco128/labels/train2017/000000000443.txt  \n","  inflating: coco128/labels/train2017/000000000446.txt  \n","  inflating: coco128/labels/train2017/000000000450.txt  \n","  inflating: coco128/labels/train2017/000000000459.txt  \n","  inflating: coco128/labels/train2017/000000000471.txt  \n","  inflating: coco128/labels/train2017/000000000472.txt  \n","  inflating: coco128/labels/train2017/000000000474.txt  \n","  inflating: coco128/labels/train2017/000000000486.txt  \n","  inflating: coco128/labels/train2017/000000000488.txt  \n","  inflating: coco128/labels/train2017/000000000490.txt  \n","  inflating: coco128/labels/train2017/000000000491.txt  \n","  inflating: coco128/labels/train2017/000000000502.txt  \n","  inflating: coco128/labels/train2017/000000000508.txt  \n","  inflating: coco128/labels/train2017/000000000510.txt  \n","  inflating: coco128/labels/train2017/000000000514.txt  \n","  inflating: coco128/labels/train2017/000000000520.txt  \n","  inflating: coco128/labels/train2017/000000000529.txt  \n","  inflating: coco128/labels/train2017/000000000531.txt  \n","  inflating: coco128/labels/train2017/000000000532.txt  \n","  inflating: coco128/labels/train2017/000000000536.txt  \n","  inflating: coco128/labels/train2017/000000000540.txt  \n","  inflating: coco128/labels/train2017/000000000542.txt  \n","  inflating: coco128/labels/train2017/000000000544.txt  \n","  inflating: coco128/labels/train2017/000000000560.txt  \n","  inflating: coco128/labels/train2017/000000000562.txt  \n","  inflating: coco128/labels/train2017/000000000564.txt  \n","  inflating: coco128/labels/train2017/000000000569.txt  \n","  inflating: coco128/labels/train2017/000000000572.txt  \n","  inflating: coco128/labels/train2017/000000000575.txt  \n","  inflating: coco128/labels/train2017/000000000581.txt  \n","  inflating: coco128/labels/train2017/000000000584.txt  \n","  inflating: coco128/labels/train2017/000000000589.txt  \n","  inflating: coco128/labels/train2017/000000000590.txt  \n","  inflating: coco128/labels/train2017/000000000595.txt  \n","  inflating: coco128/labels/train2017/000000000597.txt  \n","  inflating: coco128/labels/train2017/000000000599.txt  \n","  inflating: coco128/labels/train2017/000000000605.txt  \n","  inflating: coco128/labels/train2017/000000000612.txt  \n","  inflating: coco128/labels/train2017/000000000620.txt  \n","  inflating: coco128/labels/train2017/000000000623.txt  \n","  inflating: coco128/labels/train2017/000000000625.txt  \n","  inflating: coco128/labels/train2017/000000000626.txt  \n","  inflating: coco128/labels/train2017/000000000629.txt  \n","  inflating: coco128/labels/train2017/000000000634.txt  \n","  inflating: coco128/labels/train2017/000000000636.txt  \n","  inflating: coco128/labels/train2017/000000000641.txt  \n","  inflating: coco128/labels/train2017/000000000643.txt  \n","  inflating: coco128/labels/train2017/000000000650.txt  \n"]}],"source":["import os\n","if not os.path.exists('COCO128.zip'):\n","  !wget \"https://drive.google.com/uc?export=download&id=141bVQDo-x421CMN0o9vS3SkIY75kbi54\" -O COCO128.zip\n","  !unzip COCO128.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1765091463003,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"H6pLjEuMnCqN"},"outputs":[],"source":["  obj_labels = (\n","    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n","    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n","    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n","    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n","    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n","    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n","    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')"]},{"cell_type":"markdown","metadata":{"id":"ce_fxoG_Hw1J"},"source":["必要なライブラリを読み込む"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":12710,"status":"ok","timestamp":1765091475739,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"bDiYYmULHwUo"},"outputs":[],"source":["from torch.nn.modules.batchnorm import BatchNorm2d\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset,DataLoader\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as FF\n","from torchvision.ops.boxes import box_iou\n","from torchvision.io import read_image\n","from torchvision.utils import draw_bounding_boxes\n","import glob\n","import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from six import b\n","import cv2\n","from PIL import Image\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"mC84g_0uLLtF"},"source":["各種設定"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1765091475752,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"0lUpOKdqLLJn"},"outputs":[],"source":["image_path = \"coco128/images/train2017\"\n","label_path = \"coco128/labels/train2017\"\n","img_list = sorted(glob.glob(os.path.join(image_path,\"*\")))\n","label_list = sorted(glob.glob(os.path.join(label_path,\"*\")))\n","img_size = 416\n","class_n = 80"]},{"cell_type":"markdown","metadata":{"id":"X_pJu-LbKUTR"},"source":["## データの前処理\n","\n","YoLov3で用いるデータセットを準備する\n","- 様々な工夫が施されている\n","- 前処理の説明であるが、YoLov3の本質が詰まっている"]},{"cell_type":"markdown","metadata":{"id":"rqeK8iwxMSuw"},"source":["### AnchorBoxの定義\n","YoLoは通常AnchorBoxを利用するため、これを設定する\n","- アンカーボックスは、高さと幅が事前に決められた境界ボックスのセットのことで、一般にメッシュを複数個組み合わせた矩形となる\n","  - 検出する特定の物体のスケールおよび縦横比を取得するため利用し、学習データセットに含まれるオブジェクトサイズに基づいて選択、事前作成されるのが一般的である\n","- ここではk-Meansを利用してデータセットのアノテーションをクラスタリングし、その結果をAnchorBoxとする\n","  - 3つのPrior Boxの入手にこのAncorBoxが関わっている\n","  - K-means clusteringを利用してCOCOデータセットにある全てのBounding Boxを9個に分類する\n","    - スケールが3つ、BOXが3つであるため9個\n","\n","データからクラスタリングを行い、各データがどのクラスに属するか決定しておく\n","- YoLov3はAnchorBoxが9個あるため、9つのクラスタリングを行う"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1765091475877,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"AcbUgD3RDfMM","outputId":"a1c0aa2c-9925-4295-ab50-f6dac4627549"},"outputs":[{"output_type":"stream","name":"stdout","text":["      width    height\n","0  0.955609  0.595500\n","1  0.498875  0.476417\n","2  0.494125  0.510583\n","3  0.678875  0.781500\n","4  0.118047  0.096937\n","      width    height  cluster\n","0  0.955609  0.595500        4\n","1  0.498875  0.476417        6\n","2  0.494125  0.510583        6\n","3  0.678875  0.781500        0\n","4  0.118047  0.096937        2\n"]}],"source":["bbox_dict = {'width':[] , 'height':[]}\n","bbox_list = []\n","for path in label_list:\n","  with open(path , 'r',newline='\\n') as f:\n","      for s_line in f: # Bounding Boxが順に記述されたファイル\n","        bbox = [float(x) for x in s_line.rstrip('\\n').split(' ')] # 改行で行を分けて各Bounding Boxの行を取得、さらにスペースで分割する\n","        bbox_dict['width'].append(bbox[3])\n","        bbox_dict['height'].append(bbox[4])\n","        bbox_list.append(bbox[3:5])\n","df = pd.DataFrame(bbox_dict)\n","print(df.head()) # 先頭5このファイルについてwidthとhightの抽出結果を表示\n","km = KMeans(n_clusters=9,\n","  init='random',\n","  n_init=10,\n","  max_iter=300,\n","  tol=1e-04,\n","  random_state=0)\n","y_km = km.fit_predict(bbox_list)\n","df['cluster'] = y_km\n","print(df.head()) # 先頭5このファイルについてどこのクラスタに配属されたかを表示"]},{"cell_type":"markdown","metadata":{"id":"BSDSq96xQBS4"},"source":["9個のAnchorBoxを決定\n","- 大きいAnchorBox(先頭行)は大きな物体の検出に用いるため、大きな物体を検出する出力(Scale 3)に割り当てる\n","- 逆に小さいAnchorBoxは小さな物体の検出に用いるため、小さな物体を検出する出力(Scale 1)に割り当てる"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83,"status":"ok","timestamp":1765091475986,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"wIpImq0eDhMR","outputId":"9c2369b3-917a-4ad1-f375-fbecf453cf77"},"outputs":[{"output_type":"stream","name":"stdout","text":["      width    height      area  type\n","4  0.953643  0.856002  0.816320    13\n","0  0.591571  0.828589  0.490169    13\n","5  0.777525  0.379511  0.295079    13\n","6  0.414800  0.443691  0.184043    26\n","7  0.245450  0.694372  0.170434    26\n","8  0.152484  0.359089  0.054755    26\n","3  0.245576  0.165619  0.040672    52\n","2  0.087694  0.152691  0.013390    52\n","1  0.040143  0.047590  0.001910    52\n"]}],"source":["anchor_dict = {\"width\":[],\"height\":[],\"area\":[]}\n","for i in range(9):\n","  anchor_dict[\"width\"].append(df[df[\"cluster\"] == i].mean()[\"width\"])\n","  anchor_dict[\"height\"].append(df[df[\"cluster\"] == i].mean()[\"height\"])\n","  anchor_dict[\"area\"].append(df[df[\"cluster\"] == i].mean()[\"width\"]*df[df[\"cluster\"] == i].mean()[\"height\"])\n","anchor = pd.DataFrame(anchor_dict).sort_values('area', ascending=False)\n","anchor[\"type\"] = [int(img_size/32) ,int(img_size/32) ,int(img_size/32) ,  int(img_size/16) ,int(img_size/16) ,int(img_size/16) , int(img_size/8), int(img_size/8), int(img_size/8)]\n","print(anchor)"]},{"cell_type":"markdown","metadata":{"id":"z6j5qV-8RoNy"},"source":["### グリッド、アンカーボックス、Bouding Boxの関係\n","\n","YoLov3におけるグリッドとAnchor Box、Bounding Boxは、次の図のように定義される\n","\n","ここで\n","- $C_x, C_y$は1ピクセルの幅、\n","- $t_x , t_y , t_w , t_h$はYOLOv3の予測\n","- $P_w , P_h$はAnchorBoxサイズ\n","\n","をそれぞれ表す\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yologrid.png\" width=300>\n"]},{"cell_type":"markdown","metadata":{"id":"QMFKQ5IRTqyr"},"source":["### 幅と高さの推定\n","\n","YoLoV3では次の式で幅と高さを推定する\n","- $b_w = P_w e^{t_w}$\n","- $b_h = P_h e^{t_h}$\n","\n","YOLOv3の出力にある$t_w,_h$の表現として、小さいときは正確に、大きい場合は例えば1ドットのずれなどは殆ど気にならないため、指数関数を用いてAnchorBoxを表現する\n","- 直接物体の幅や高さをピクセル値予測せず、AnchorBoxを調整するという表現にすることで、学習コストを削減できる"]},{"cell_type":"markdown","metadata":{"id":"7Yi_oqd_UhH5"},"source":["### 中心位置の推定方法\n","YoLov3では次の式で中心位置を推定する\n","- $b_x = \\sigma(t_x) + C_x$\n","- $b_y = \\sigma(t_y) + C_y$\n","\n","直接的に推定してもよいが、なぜか複雑な式で予測している\n","\n","- 例えばサイズが$416 \\times 416$の画像に物体が写っており、その物体の中心が$(200 ,250)$であったとする\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloapple.png\" width=300>\n","\n","- 対称物体が大きい場合、小さい特徴マップで予測することになる\n","  - $13 \\times 13$の特徴マップでは画像サイズに対する中心の位置は$\\frac{200}{416} = 0.481, \\frac{250}{416} = 0.600$となるため、$13 \\times 13$pxでは最も近いのは(6.253 , 7.800)となり、結果として小数点を省いた$13\\times 13$メッシュにおける座標値としての(6, 7)が中央となる\n","\n","このままでは、特徴マップが小さくなるにつれて誤差が大きくなる\n","- これを防ぐため、YoLov3ではピクセル内のどこに中心があるかを予測する\n","- YoLov3の出力$t_x , t_y$をシグモイド関数に入力して0から1の値に変換\n","  - メッシュよりも細かい場所の指定(メッシュに対して何割のところに中心があるか)を知ることができる\n","- $(6.253 , 7.800)$は、対象とするメッシュでピクセル(7 , 6)であり、そこから(0.253 , 0.800)の場所に中心があると考えることができる\n","- これを受けて、$0.253 = \\sigma(t_x)$となるような$x$を推論する\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/yoloapplegrid.png\" width=300>\n"]},{"cell_type":"markdown","metadata":{"id":"pJrp9g5YXOHT"},"source":["### YoLov3の本質\n","\n","YoLov3の予測はAnchorBoxの微調整やピクセル間の誤差補正予測など、微調整をふんだんに行っていることがわかる\n","- そのために、少々値が加工された入力を必要とする\n","\n","アノテーションデータは真の中心位置と幅・高さを保持しているため、これをYoLov3用に変換する必要がある\n","\n","- 幅と高さについて、\n","  - $b_w = P_w e^{t_w}$\n","  - $b_h = P_h e^{t_h}$\n","  \n","  であるため、\n","\n","  $t_w , t_h$は\n","  - $t_w = \\ln\\frac{b_w}{P_w}$\n","  - $t_h = \\ln\\frac{b_h}{P_h}$\n","\n","  となる\n","  - この処理は`def wh2twth`で記述されている\n","\n","- 中心位置についても、\n","  - $b_x = \\sigma(t_x) + C_x$\n","  - $b_y = \\sigma(t_y) + C_y$\n","\n","  であるため、\n","  - $b_x - C_x = \\sigma(t_x)$\n","  - $b_y - C_y = \\sigma(t_y)$\n","\n","  となり、この逆関数を求めると$t_x$と$t_y$を得ることができる\n","\n","  シグモイド関数の逆関数はロジット関数であるため、\n","  - $t_x = \\ln\\frac{b_x - C_x}{1-(b_x - C_x)}$\n","  - $t_y = \\ln\\frac{b_y - C_y}{1-(b_y- C_y)}$\n","\n","  となる\n","  - この処理は`def cxcy2txty`で記述されている\n","\n","以上を踏まえて実際にデータセットの前処理部分を記述する\n","- 基本的にはDataset関数の準備であり、その核心は、`__getitem__`によりデータを取得することと、`__len__`により全データ数を返すところ\n","- その他はすべて`__getitem__`のための副次的な処理関数"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1765091476016,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"12a-peb9DxYi"},"outputs":[],"source":["class YOLOv3_Dataset(Dataset):\n","  def __init__(self, img_list, label_list, class_n, img_size, anchor_dict, transform):\n","    super().__init__()\n","    self.img_list = img_list\n","    self.label_list = label_list\n","    self.anchor_dict = anchor_dict\n","    self.class_n = class_n\n","    self.img_size = img_size\n","    self.transform = transform\n","    self.anchor_iou = torch.cat([torch.zeros(9,2) , torch.tensor(self.anchor_dict[[\"width\",\"height\"]].values)] ,dim = 1)\n","    self.map_size = [int(self.img_size/32) , int(self.img_size/16) , int(self.img_size/8)]\n","\n","  def get_label(self, path):  # coco128/labelsにあるラベルとBBoxがセットになったファイルを読み出す\n","    bbox_list = []\n","    with open(path , 'r',newline='\\n') as f:\n","      for s_line in f:\n","        bbox = [float(x) for x in s_line.rstrip('\\n').split(' ')]\n","        bbox_list.append(bbox)\n","    return bbox_list\n","\n","  def wh2twth(self, wh):\n","    twth = []\n","    for i in range(9):\n","      anchor = self.anchor_dict.iloc[i]\n","      aw = anchor[\"width\"]\n","      ah = anchor[\"height\"]\n","      twth.append([math.log(wh[0]/aw) , math.log(wh[1]/ah)])\n","    return twth\n","\n","  def cxcy2txty(self,cxcy):\n","    txty = []\n","    for size in self.map_size:\n","      grid_x = int(cxcy[0]*size)\n","      grid_y = int(cxcy[1]*size)\n","      tx = math.log((cxcy[0]*size - grid_x + 1e-10) / (1 - cxcy[0]*size +grid_x+ 1e-10))\n","      ty = math.log((cxcy[1]*size - grid_y+ 1e-10) / (1 - cxcy[1]*size + grid_y+ 1e-10))\n","      txty.append([grid_x , tx , grid_y ,ty])\n","    return txty\n","\n","  def label2tensor(self, bbox_list):\n","    tensor_list = []\n","    for size in self.map_size:\n","      for x in range(3):\n","        tensor_list.append(torch.zeros((4 + 1 + self.class_n,size,size)))\n","    for bbox in bbox_list:\n","      cls_n = int(bbox[0])\n","      txty_list = self.cxcy2txty(bbox[1:3])\n","      twth_list = self.wh2twth(bbox[3:])\n","      label_iou = torch.cat([torch.zeros((1,2)), torch.tensor(bbox[3:]).unsqueeze(0)],dim=1)\n","      iou = box_iou(label_iou, self.anchor_iou)[0]\n","      obj_idx = torch.argmax(iou).item()\n","#      print(cls_n, txty_list, twth_list, label_iou, iou, obj_idx)\n","      for i, twth in enumerate(twth_list):\n","        tensor = tensor_list[i]\n","        txty = txty_list[int(i/3)]\n","        if i == obj_idx:\n","          tensor[0, txty[2], txty[0]] = txty[1]\n","          tensor[1, txty[2], txty[0]] = txty[3]\n","          tensor[2, txty[2], txty[0]] = twth[0]\n","          tensor[3, txty[2], txty[0]] = twth[1]\n","          tensor[4, txty[2], txty[0]] = 1\n","          tensor[5+cls_n, txty[2], txty[0]] = 1\n","    scale3_label = torch.cat(tensor_list[0:3] , dim = 0)\n","    scale2_label = torch.cat(tensor_list[3:6] , dim = 0)\n","    scale1_label = torch.cat(tensor_list[6:] , dim = 0)\n","    return scale3_label , scale2_label , scale1_label\n","\n","  def __getitem__(self, idx):\n","    img_path = self.img_list[idx]\n","    label_path = self.label_list[idx]\n","    bbox_list = self.get_label(label_path)\n","    img = cv2.imread(img_path)\n","    img = cv2.resize(img , (self.img_size , self.img_size))\n","    img = Image.fromarray(img)\n","    img = self.transform(img)\n","    scale3_label , scale2_label , scale1_label = self.label2tensor(bbox_list)\n","    return img , scale3_label , scale2_label , scale1_label\n","\n","  def __len__(self):\n","    return len(self.img_list)"]},{"cell_type":"markdown","metadata":{"id":"zM6wGkGJ3WaE"},"source":["学習のためにBouding Boxや中心位置などを操作しているため、元に戻す関数`visualization`を定義する"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1765091476017,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"bX7GENGx3WCu"},"outputs":[],"source":["def visualization(y_pred, anchor, img_size,conf = 0.5, is_label = False):\n","  size = y_pred.shape[2]\n","  anchor_size = anchor[anchor[\"type\"] == size]\n","  bbox_list = []\n","  obj_list = []\n","  for i in range(3):\n","    a = anchor_size.iloc[i]\n","    grid = img_size/size\n","    y_pred_cut = y_pred[0,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ].cpu()\n","    if is_label:\n","      y_pred_conf = y_pred_cut[4,:,:].numpy()\n","    else:\n","      y_pred_conf = torch.sigmoid(y_pred_cut[4,:,:]).numpy()\n","    index = np.where(y_pred_conf > conf)\n","    for y,x in zip(index[0],index[1]):\n","      cx = x*grid + torch.sigmoid(y_pred_cut[0,y,x]).numpy()*grid\n","      cy = y*grid + torch.sigmoid(y_pred_cut[1,y,x]).numpy()*grid\n","      width = a[\"width\"]*torch.exp(y_pred_cut[2,y,x]).numpy()*img_size\n","      height = a[\"height\"]*torch.exp(y_pred_cut[3,y,x]).numpy()*img_size\n","      xmin,ymin,xmax,ymax = cx - width/2 , cy - height/2 ,cx + width/2 , cy + height/2\n","      bbox_list.append([xmin,ymin,xmax,ymax])\n","      obj_list.append(y_pred_cut[5:,y,x].argmax()) # 正解ラベル\n","  return bbox_list, obj_list"]},{"cell_type":"markdown","metadata":{"id":"FVKznFmG3jZG"},"source":["画像を表示するための関数`show`を定義する"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1765091476019,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"1CFAgxo-3nO8"},"outputs":[],"source":["def show(imgs):\n","    if not isinstance(imgs, list):\n","        imgs = [imgs]\n","    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, dpi=150)\n","    for i, img in enumerate(imgs):\n","        img = img.detach()\n","        img = FF.to_pil_image(img)\n","        axs[0, i].imshow(np.asarray(img))\n","        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"]},{"cell_type":"markdown","metadata":{"id":"u0mi53iZ5It1"},"source":["試しに、学習データを表示させる\n","- Bounding Boxの色は、Scaleの違いを表している\n","  - 赤：大サイズとして予測（Scale3）\n","  - 緑：中サイズとして予測（Scale2）\n","  - 青：小サイズとして予測（Scale1）"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gbel2nmQEv5nsekGk7QOqo_yNKteGt94"},"executionInfo":{"elapsed":4168,"status":"ok","timestamp":1765091480257,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"BBLm_UTt9q2d","outputId":"3f86c400-f719-46e3-9a80-7c28be3c4be5"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["transform = T.Compose([T.ToTensor()])\n","train_data = YOLOv3_Dataset(img_list, label_list, 80, img_size , anchor, transform)\n","train_loader = DataLoader(train_data, batch_size = 1)\n","\n","for n, (img, scale3_label, scale2_label, scale1_label) in enumerate(train_loader):\n","  path = img_list[n]\n","  img = cv2.imread(path)[:,:,::-1]\n","  img = cv2.resize(img, (img_size, img_size))\n","  img = torch.tensor(img.transpose(2,0,1))\n","  preds = [scale3_label, scale2_label, scale1_label]\n","  for color,pred in zip([\"red\",\"green\",\"blue\"],preds):\n","    bbox_list, obj_list = visualization(pred, anchor, img_size, conf=0.9, is_label=True)\n","    if len(bbox_list) != 0:\n","      obj_label_list = []\n","      for obj_item in obj_list:\n","        obj_label_list.append(obj_labels[obj_item])\n","      img = draw_bounding_boxes(img, torch.tensor(bbox_list), colors=color, width=1, labels=obj_label_list)\n","  show(img)\n","  if n == 10:\n","    break\n","print(scale3_label.shape, scale2_label.shape, scale1_label.shape)"]},{"cell_type":"markdown","metadata":{"id":"SKEZhBI5HpmQ"},"source":["## YoLov3ネットワーク\n"]},{"cell_type":"markdown","metadata":{"id":"s0r6a5fvI6kU"},"source":["ちょっと厄介ではあるが、図の通りに実装する\n","- n=80つまり、80クラス分類であるが、nであるため自由に変更できるようにしている"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":610,"status":"ok","timestamp":1765091480889,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"KaUMrYESDSoC"},"outputs":[],"source":["class YOLOv3(nn.Module):\n","  def __init__(self,class_n = 80):\n","    super(YOLOv3 , self).__init__()\n","    self.class_n = class_n\n","    self.first_block = nn.Sequential(\n","        nn.Conv2d(3 , 32 , 3 , 1 , 1),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(32 , 64 , 3 , 2 , 1),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(),\n","      )\n","    self.residual_block_1 = self.MakeResidualBlock(64)\n","    self.conv_1 = nn.Conv2d(64 , 128  , 3 , 2 , 1)\n","    self.residual_block_2 = nn.Sequential(self.MakeResidualBlock(128),self.MakeResidualBlock(128))\n","    self.conv_2 = nn.Conv2d(128 , 256  , 3 , 2 , 1)\n","    self.residual_block_3 = nn.Sequential(self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256),self.MakeResidualBlock(256))\n","    self.conv_3 = nn.Conv2d(256 , 512  , 3 , 2 , 1)\n","    self.residual_block_4 = nn.Sequential(self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512),self.MakeResidualBlock(512))\n","    self.conv_4 = nn.Conv2d(512 , 1024  , 3 , 2 , 1)\n","    self.residual_block_5 = nn.Sequential(self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),)\n","    self.conv_block = nn.Sequential(self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),self.MakeResidualBlock(1024),)\n","    self.scale3_output = nn.Conv2d(1024 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.scale2_upsample = nn.Conv2d(1024 , 256 , 1 , 1 )\n","    self.scale2_convblock = nn.Sequential(nn.Sequential(nn.Conv2d(768 , 256 , 1 , 1),\n","                          nn.BatchNorm2d(256),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(256 , 512 , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(512),\n","                          nn.LeakyReLU(),\n","                          ),self.MakeResidualBlock(512),self.MakeResidualBlock(512),)\n","    self.scale2_output = nn.Conv2d(512 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.scale1_upsample = nn.Conv2d(512 , 128 , 1 , 1 )\n","    self.scale1_convblock = nn.Sequential(nn.Sequential(nn.Conv2d(384 , 128 , 1 , 1),\n","                          nn.BatchNorm2d(128),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(128 , 256 , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(256),\n","                          nn.LeakyReLU(),\n","                          ),self.MakeResidualBlock(256),self.MakeResidualBlock(256),)\n","    self.scale1_output =  nn.Conv2d(256 , (3 * (4 + 1 +self.class_n)) , 1 , 1 )\n","    self.upsample = nn.Upsample(scale_factor = 2)\n","  def MakeResidualBlock(self,fn):\n","    block = nn.Sequential(nn.Conv2d(fn , int(fn/2) , 1 , 1),\n","                          nn.BatchNorm2d(int(fn/2)),\n","                          nn.LeakyReLU(),\n","                          nn.Conv2d(int(fn/2) , fn , 3 , 1 ,  1),\n","                          nn.BatchNorm2d(fn),\n","                          nn.LeakyReLU(),\n","                          )\n","    return block\n","  def forward(self,x):\n","    x = self.first_block(x)\n","    x_res = self.residual_block_1(x)\n","    x = x + x_res\n","    x = self.conv_1(x)\n","    for layer in self.residual_block_2:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x = self.conv_2(x)\n","    for layer in self.residual_block_3:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x1 = x\n","    x = self.conv_3(x)\n","    for layer in self.residual_block_4:\n","      x_res = layer(x)\n","      x = x + x_res\n","    x2 = x\n","    x = self.conv_4(x)\n","    for layer in self.residual_block_5:\n","      x_res = layer(x)\n","      x = x + x_res\n","    for layer in self.conv_block:\n","      x = layer(x)\n","    scale3_result = self.scale3_output(x)\n","    scale2_up = self.upsample(self.scale2_upsample(x))\n","    x = torch.cat([x2 , scale2_up],dim = 1)\n","    for layer in self.scale2_convblock :\n","      x = layer(x)\n","    x2 = x\n","    scale2_result = self.scale2_output(x)\n","    scale1_up = self.upsample(self.scale1_upsample(x2))\n","    x = torch.cat([x1 , scale1_up],dim = 1)\n","    for layer in self.scale1_convblock :\n","      x = layer(x)\n","    scale1_result = self.scale1_output(x)\n","    return  scale3_result , scale2_result , scale1_result\n","model =YOLOv3().cuda()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":800,"status":"ok","timestamp":1765091481690,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"zD5pRNdfDYQg","outputId":"c09f46a3-01d9-4cd7-e44b-b01762504895"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 255, 13, 13])\n","torch.Size([1, 255, 26, 26])\n","torch.Size([1, 255, 52, 52])\n"]}],"source":["with torch.no_grad():\n","  output = model(torch.zeros((1,3,416,416)).cuda())\n","for i in range(3):\n","  print(output[i].shape)"]},{"cell_type":"markdown","metadata":{"id":"toC0stGeJjO2"},"source":["modelとしてインスタンス化\n","- ここで、実際に出力を確認してみる\n","- $13 \\times 13$、$26 \\times 26$、$52 \\times 52$のグリッドそれぞれについて、255要素のデータが出力されており、設計通りであることがわかる"]},{"cell_type":"markdown","metadata":{"id":"Ku1AYBmW6A8A"},"source":["## ロス関数\n","\n","YoLov3のロス関数はBounding Boxをラベルの両方を考慮しており、かなり複雑\n","\n","論文では次の式が与えられている\n","- 説明のため、順番を並び替えている\n","\n","$$\\lambda_{coord} \\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{obj}_{i,j}[(t_x−\\hat{t}_x)^2+(t_y−\\hat{t}_y)^2+(t_w−\\hat{t}_w)^2+(_th−\\hat{t}_h)^2]\\\\ +\\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{obj}_{i,j}[-log(\\sigma(t_o))+\\lambda_{noobj}\\sum^{S^2}_{i=0} \\sum^B_{j=0}\\mathfrak{1}^{noobj}_{i,j}[-log(1-\\sigma(t_o))]\\\\+\\sum^C_{k=1}BCE(\\hat{y}_k,\\sigma(s_k))] $$\n","\n","簡単に説明する\n","- 1行目はBounding Boxの$x, y, w, h$それぞれの誤差を表しており、単純に二乗誤差が用いられている\n","- 2行目はobjective scoreに沿うとして、オブジェクトがセルの中に存在するかどうかで2つの項に分かれている\n","  - 存在することを正しく判定できたか、また、存在しないことを正しく判定できたかをそれぞれ表している\n","  - 検出対象の物体がない矩形は objectness score が0になるように学習する必要があり、これに対応する\n","- 3行目はクラスつまり物体のラベルを正しく推定できたかを意味しており、BCEロスを利用している\n","  - なお、コード上では`BCEWithLogitsLoss`を利用している\n","  - `BCELoss`はシグモイドをとった後の値を入力として受け付け、`BCEWithLogitsLoss`はシグモイドをとる前の値を入力として受け付けつける点に注意\n","\n","なお、論文では、$t_x, t_y$の損失は二乗誤差で計算すると書いてあり、ここではその通りに実装している\n","- 論文で示されているdarknet実装では、シグモイド関数適用後の$\\sigma(t_x), \\sigma(t_y)$の値との差分を求めているため、論文とは異なっている\n","- この実装に従う場合は、$t_x, t_y$はバイナリクロスエントロピーで損失を計算することになる"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1765091481694,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"krF9tUElD1Se"},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters())\n","criterion_bce = torch.nn.BCEWithLogitsLoss()\n","def bbox_metric(y_pred , y_true,class_n = 80):\n","  for i in range(3):\n","    y_pred_cut = y_pred[:,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ]\n","    y_true_cut = y_true[:,i*(4 + 1 + class_n) :(i+1)*(4 + 1 + class_n) ]\n","    loss_coord = torch.sum(torch.square(y_pred_cut[:,0:4] - y_true_cut[:,0:4])*y_true_cut[:,4])\n","    loss_obj = torch.sum((-1 * torch.log(torch.sigmoid(y_pred_cut[:,4] )+ 1e-10) + criterion_bce(y_pred_cut[:,5:],y_true_cut[:,5:]))*y_true_cut[:,4])\n","    loss_noobj =  torch.sum((-1 * torch.log(1 - torch.sigmoid(y_pred_cut[:,4])+ 1e-10))*(1 - y_true_cut[:,4]))\n","    return loss_coord , loss_obj , loss_noobj"]},{"cell_type":"markdown","metadata":{"id":"Jrm4G74JV92F"},"source":["## 学習\n","最新GPU(RTX3090Ti)でも1時間弱かかる\n","- Google Colaboratoryでは、ほぼ実行時間限界の半分(10時間)程度必要となる可能性がある\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4491787,"status":"ok","timestamp":1765095973482,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"Yh7LqG-4D5MS","outputId":"baef7b70-f5c6-4c9b-82ac-167cdc6a0832"},"outputs":[{"output_type":"stream","name":"stderr","text":["[train] Epoch 0 loss 310.361805 loss_coord 11.109880 loss_obj 5.190571 loss_noobj 247.346218: 100%|██████████| 128/128 [04:29<00:00,  2.11s/it]\n","[train] Epoch 1 loss 101.061266 loss_coord 9.206684 loss_obj 6.557993 loss_noobj 26.274648: 100%|██████████| 128/128 [00:25<00:00,  5.02it/s]\n","[train] Epoch 2 loss 96.989635 loss_coord 9.770466 loss_obj 6.930159 loss_noobj 17.917575: 100%|██████████| 128/128 [00:13<00:00,  9.17it/s]\n","[train] Epoch 3 loss 94.435701 loss_coord 9.242186 loss_obj 6.958298 loss_noobj 15.610530: 100%|██████████| 128/128 [00:13<00:00,  9.84it/s]\n","[train] Epoch 4 loss 96.417622 loss_coord 10.503327 loss_obj 7.029346 loss_noobj 15.620833: 100%|██████████| 128/128 [00:12<00:00,  9.87it/s]\n","[train] Epoch 5 loss 96.110168 loss_coord 9.917613 loss_obj 7.231951 loss_noobj 13.873048: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 6 loss 103.465761 loss_coord 16.345316 loss_obj 7.151467 loss_noobj 15.605770: 100%|██████████| 128/128 [00:12<00:00, 10.38it/s]\n","[train] Epoch 7 loss 93.421861 loss_coord 8.448796 loss_obj 7.140876 loss_noobj 13.564309: 100%|██████████| 128/128 [00:12<00:00, 10.06it/s]\n","[train] Epoch 8 loss 91.977798 loss_coord 7.637808 loss_obj 7.011271 loss_noobj 14.227277: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 9 loss 89.575083 loss_coord 7.696873 loss_obj 6.788133 loss_noobj 13.996883: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 10 loss 88.643117 loss_coord 7.027212 loss_obj 6.738667 loss_noobj 14.229233: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 11 loss 88.282495 loss_coord 7.556533 loss_obj 6.655505 loss_noobj 14.170916: 100%|██████████| 128/128 [00:12<00:00,  9.92it/s]\n","[train] Epoch 12 loss 87.360190 loss_coord 7.127612 loss_obj 6.632586 loss_noobj 13.906721: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 13 loss 86.474424 loss_coord 7.206487 loss_obj 6.623486 loss_noobj 13.033074: 100%|██████████| 128/128 [00:15<00:00,  8.27it/s]\n","[train] Epoch 14 loss 82.337406 loss_coord 6.926239 loss_obj 6.228118 loss_noobj 13.129990: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 15 loss 83.514380 loss_coord 6.996620 loss_obj 6.322846 loss_noobj 13.289299: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 16 loss 82.347107 loss_coord 6.789278 loss_obj 6.224755 loss_noobj 13.310278: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 17 loss 78.258579 loss_coord 6.579110 loss_obj 5.853016 loss_noobj 13.149307: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 18 loss 77.650292 loss_coord 6.299655 loss_obj 5.811034 loss_noobj 13.240295: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 19 loss 76.504420 loss_coord 6.578205 loss_obj 5.707750 loss_noobj 12.848712: 100%|██████████| 128/128 [00:17<00:00,  7.25it/s]\n","[train] Epoch 20 loss 74.185020 loss_coord 6.341111 loss_obj 5.499872 loss_noobj 12.845193: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 21 loss 73.656726 loss_coord 5.915015 loss_obj 5.478874 loss_noobj 12.952969: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 22 loss 74.550889 loss_coord 6.243607 loss_obj 5.584842 loss_noobj 12.458859: 100%|██████████| 128/128 [00:18<00:00,  7.02it/s]\n","[train] Epoch 23 loss 68.175297 loss_coord 5.504170 loss_obj 5.016354 loss_noobj 12.507585: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 24 loss 67.877244 loss_coord 5.658237 loss_obj 4.994853 loss_noobj 12.270479: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 25 loss 64.024225 loss_coord 5.081600 loss_obj 4.673139 loss_noobj 12.211230: 100%|██████████| 128/128 [00:14<00:00,  9.13it/s]\n","[train] Epoch 26 loss 59.596898 loss_coord 4.729116 loss_obj 4.365598 loss_noobj 11.211797: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 27 loss 60.449307 loss_coord 4.896851 loss_obj 4.397370 loss_noobj 11.578756: 100%|██████████| 128/128 [00:12<00:00,  9.87it/s]\n","[train] Epoch 28 loss 58.457408 loss_coord 5.006220 loss_obj 4.249025 loss_noobj 10.960943: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 29 loss 57.263824 loss_coord 4.693042 loss_obj 4.137820 loss_noobj 11.192581: 100%|██████████| 128/128 [00:12<00:00,  9.89it/s]\n","[train] Epoch 30 loss 53.114077 loss_coord 4.504391 loss_obj 3.800292 loss_noobj 10.606764: 100%|██████████| 128/128 [00:12<00:00,  9.85it/s]\n","[train] Epoch 31 loss 50.835216 loss_coord 3.944014 loss_obj 3.630574 loss_noobj 10.585466: 100%|██████████| 128/128 [00:13<00:00,  9.77it/s]\n","[train] Epoch 32 loss 52.862776 loss_coord 4.263827 loss_obj 3.743391 loss_noobj 11.165043: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 33 loss 44.701378 loss_coord 4.023079 loss_obj 3.010055 loss_noobj 10.577748: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 34 loss 41.131610 loss_coord 3.775328 loss_obj 2.811623 loss_noobj 9.240053: 100%|██████████| 128/128 [00:23<00:00,  5.37it/s]\n","[train] Epoch 35 loss 36.884660 loss_coord 3.504089 loss_obj 2.512610 loss_noobj 8.254475: 100%|██████████| 128/128 [00:16<00:00,  7.76it/s]\n","[train] Epoch 36 loss 33.449615 loss_coord 2.983499 loss_obj 2.275287 loss_noobj 7.713249: 100%|██████████| 128/128 [00:13<00:00,  9.85it/s]\n","[train] Epoch 37 loss 26.186583 loss_coord 2.651924 loss_obj 1.662381 loss_noobj 6.910849: 100%|██████████| 128/128 [00:27<00:00,  4.64it/s]\n","[train] Epoch 38 loss 25.386510 loss_coord 2.841183 loss_obj 1.641057 loss_noobj 6.134757: 100%|██████████| 128/128 [00:19<00:00,  6.57it/s]\n","[train] Epoch 39 loss 40.062820 loss_coord 3.367630 loss_obj 2.763580 loss_noobj 9.059395: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 40 loss 36.801007 loss_coord 3.128349 loss_obj 2.411047 loss_noobj 9.562188: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 41 loss 30.682581 loss_coord 2.753236 loss_obj 1.996862 loss_noobj 7.960727: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 42 loss 28.161521 loss_coord 2.742178 loss_obj 1.787209 loss_noobj 7.547256: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 43 loss 18.863946 loss_coord 2.188978 loss_obj 1.110474 loss_noobj 5.570224: 100%|██████████| 128/128 [00:20<00:00,  6.30it/s]\n","[train] Epoch 44 loss 17.527751 loss_coord 2.095994 loss_obj 1.025029 loss_noobj 5.181469: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 45 loss 15.971713 loss_coord 1.921218 loss_obj 0.955202 loss_noobj 4.498475: 100%|██████████| 128/128 [00:27<00:00,  4.68it/s]\n","[train] Epoch 46 loss 13.971005 loss_coord 1.670284 loss_obj 0.817006 loss_noobj 4.130661: 100%|██████████| 128/128 [00:17<00:00,  7.33it/s]\n","[train] Epoch 47 loss 13.800504 loss_coord 1.827895 loss_obj 0.767498 loss_noobj 4.297627: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 48 loss 12.374448 loss_coord 1.658996 loss_obj 0.664501 loss_noobj 4.070438: 100%|██████████| 128/128 [00:24<00:00,  5.18it/s]\n","[train] Epoch 49 loss 16.482888 loss_coord 2.058151 loss_obj 0.964473 loss_noobj 4.780005: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 50 loss 14.312595 loss_coord 1.657362 loss_obj 0.786227 loss_noobj 4.792967: 100%|██████████| 128/128 [00:12<00:00, 10.24it/s]\n","[train] Epoch 51 loss 15.419309 loss_coord 1.600062 loss_obj 0.887489 loss_noobj 4.944353: 100%|██████████| 128/128 [00:14<00:00,  8.56it/s]\n","[train] Epoch 52 loss 12.562047 loss_coord 1.785318 loss_obj 0.707106 loss_noobj 3.705672: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 53 loss 7.739833 loss_coord 1.265018 loss_obj 0.385757 loss_noobj 2.617247: 100%|██████████| 128/128 [00:39<00:00,  3.26it/s]\n","[train] Epoch 54 loss 6.018752 loss_coord 1.136886 loss_obj 0.259713 loss_noobj 2.284733: 100%|██████████| 128/128 [00:13<00:00,  9.77it/s]\n","[train] Epoch 55 loss 3.747858 loss_coord 0.903903 loss_obj 0.161218 loss_noobj 1.231778: 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n","[train] Epoch 56 loss 4.106866 loss_coord 1.197542 loss_obj 0.159342 loss_noobj 1.315910: 100%|██████████| 128/128 [00:12<00:00, 10.14it/s]\n","[train] Epoch 57 loss 6.144953 loss_coord 1.278261 loss_obj 0.289957 loss_noobj 1.967123: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 58 loss 9.744890 loss_coord 1.393192 loss_obj 0.524093 loss_noobj 3.110770: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 59 loss 7.992213 loss_coord 1.356242 loss_obj 0.372693 loss_noobj 2.909043: 100%|██████████| 128/128 [00:12<00:00, 10.38it/s]\n","[train] Epoch 60 loss 7.302732 loss_coord 1.165156 loss_obj 0.346261 loss_noobj 2.674968: 100%|██████████| 128/128 [00:12<00:00, 10.40it/s]\n","[train] Epoch 61 loss 4.283314 loss_coord 0.905191 loss_obj 0.179649 loss_noobj 1.581631: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 62 loss 8.883930 loss_coord 0.998530 loss_obj 0.515209 loss_noobj 2.733314: 100%|██████████| 128/128 [00:25<00:00,  5.12it/s]\n","[train] Epoch 63 loss 9.932360 loss_coord 1.346433 loss_obj 0.503641 loss_noobj 3.549515: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 64 loss 5.008230 loss_coord 1.023471 loss_obj 0.251436 loss_noobj 1.470401: 100%|██████████| 128/128 [00:12<00:00, 10.18it/s]\n","[train] Epoch 65 loss 2.923248 loss_coord 0.730961 loss_obj 0.123460 loss_noobj 0.957689: 100%|██████████| 128/128 [00:31<00:00,  4.05it/s]\n","[train] Epoch 66 loss 1.798544 loss_coord 0.728840 loss_obj 0.053523 loss_noobj 0.534476: 100%|██████████| 128/128 [00:23<00:00,  5.52it/s]\n","[train] Epoch 67 loss 1.192027 loss_coord 0.674215 loss_obj 0.018703 loss_noobj 0.330783: 100%|██████████| 128/128 [00:25<00:00,  4.95it/s]\n","[train] Epoch 68 loss 0.726119 loss_coord 0.408209 loss_obj 0.016799 loss_noobj 0.149917: 100%|██████████| 128/128 [00:27<00:00,  4.71it/s]\n","[train] Epoch 69 loss 0.735481 loss_coord 0.450626 loss_obj 0.008366 loss_noobj 0.201195: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 70 loss 0.509243 loss_coord 0.335959 loss_obj 0.009813 loss_noobj 0.075158: 100%|██████████| 128/128 [00:12<00:00,  9.90it/s]\n","[train] Epoch 71 loss 0.452572 loss_coord 0.328596 loss_obj 0.005641 loss_noobj 0.067571: 100%|██████████| 128/128 [00:36<00:00,  3.49it/s]\n","[train] Epoch 72 loss 0.384897 loss_coord 0.285709 loss_obj 0.005552 loss_noobj 0.043666: 100%|██████████| 128/128 [00:17<00:00,  7.23it/s]\n","[train] Epoch 73 loss 0.414431 loss_coord 0.330045 loss_obj 0.004430 loss_noobj 0.040081: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 74 loss 0.357549 loss_coord 0.282357 loss_obj 0.003813 loss_noobj 0.037058: 100%|██████████| 128/128 [00:24<00:00,  5.30it/s]\n","[train] Epoch 75 loss 0.356102 loss_coord 0.289478 loss_obj 0.003666 loss_noobj 0.029965: 100%|██████████| 128/128 [00:12<00:00,  9.92it/s]\n","[train] Epoch 76 loss 0.407497 loss_coord 0.343214 loss_obj 0.003469 loss_noobj 0.029594: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 77 loss 0.434958 loss_coord 0.371215 loss_obj 0.003787 loss_noobj 0.025875: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 78 loss 0.469560 loss_coord 0.409040 loss_obj 0.003184 loss_noobj 0.028677: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 79 loss 0.706395 loss_coord 0.619133 loss_obj 0.005725 loss_noobj 0.030007: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 80 loss 1.056183 loss_coord 0.862108 loss_obj 0.011275 loss_noobj 0.081321: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 81 loss 2.154415 loss_coord 1.017227 loss_obj 0.052361 loss_noobj 0.613579: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 82 loss 82.577245 loss_coord 7.291145 loss_obj 6.633724 loss_noobj 8.948863: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 83 loss 68.837097 loss_coord 7.007301 loss_obj 4.921641 loss_noobj 12.613386: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 84 loss 28.710779 loss_coord 2.720571 loss_obj 1.818787 loss_noobj 7.802338: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 85 loss 15.486151 loss_coord 1.730319 loss_obj 0.888451 loss_noobj 4.871326: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 86 loss 14.101551 loss_coord 1.472114 loss_obj 0.844604 loss_noobj 4.183398: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 87 loss 10.560484 loss_coord 1.402411 loss_obj 0.615445 loss_noobj 3.003621: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 88 loss 8.534665 loss_coord 1.205786 loss_obj 0.459550 loss_noobj 2.733376: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 89 loss 4.657863 loss_coord 1.058738 loss_obj 0.184728 loss_noobj 1.751842: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 90 loss 2.762552 loss_coord 1.082146 loss_obj 0.078806 loss_noobj 0.892345: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 91 loss 2.673855 loss_coord 0.969370 loss_obj 0.076502 loss_noobj 0.939467: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 92 loss 2.827061 loss_coord 0.851840 loss_obj 0.120066 loss_noobj 0.774558: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 93 loss 2.871641 loss_coord 0.724562 loss_obj 0.105571 loss_noobj 1.091369: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 94 loss 1.683287 loss_coord 0.825343 loss_obj 0.040225 loss_noobj 0.455691: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 95 loss 1.297027 loss_coord 0.763648 loss_obj 0.028675 loss_noobj 0.246629: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 96 loss 3.332185 loss_coord 0.841930 loss_obj 0.156147 loss_noobj 0.928781: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 97 loss 1.967379 loss_coord 0.873824 loss_obj 0.051769 loss_noobj 0.575864: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 98 loss 1.062755 loss_coord 0.801998 loss_obj 0.009834 loss_noobj 0.162415: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 99 loss 0.856404 loss_coord 0.632110 loss_obj 0.012482 loss_noobj 0.099470: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 100 loss 0.546579 loss_coord 0.380005 loss_obj 0.007008 loss_noobj 0.096497: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 101 loss 22.824774 loss_coord 1.691955 loss_obj 1.676325 loss_noobj 4.369564: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 102 loss 26.358489 loss_coord 2.516715 loss_obj 1.622795 loss_noobj 7.613827: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 103 loss 10.343534 loss_coord 1.521679 loss_obj 0.519578 loss_noobj 3.626080: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 104 loss 3.028574 loss_coord 0.645609 loss_obj 0.111680 loss_noobj 1.266163: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 105 loss 1.408608 loss_coord 0.534403 loss_obj 0.035934 loss_noobj 0.514869: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 106 loss 0.757462 loss_coord 0.378207 loss_obj 0.015598 loss_noobj 0.223277: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 107 loss 0.500973 loss_coord 0.269646 loss_obj 0.009432 loss_noobj 0.137002: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 108 loss 0.397556 loss_coord 0.203657 loss_obj 0.008204 loss_noobj 0.111862: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 109 loss 0.374527 loss_coord 0.229112 loss_obj 0.005799 loss_noobj 0.087430: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 110 loss 0.311640 loss_coord 0.187231 loss_obj 0.005491 loss_noobj 0.069499: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 111 loss 0.255645 loss_coord 0.149057 loss_obj 0.004310 loss_noobj 0.063483: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 112 loss 0.266416 loss_coord 0.168063 loss_obj 0.004467 loss_noobj 0.053684: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 113 loss 0.271337 loss_coord 0.183079 loss_obj 0.003512 loss_noobj 0.053140: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 114 loss 0.335738 loss_coord 0.260805 loss_obj 0.003457 loss_noobj 0.040359: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 115 loss 0.426616 loss_coord 0.352281 loss_obj 0.003465 loss_noobj 0.039685: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 116 loss 0.461608 loss_coord 0.373674 loss_obj 0.004448 loss_noobj 0.043459: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 117 loss 0.532981 loss_coord 0.457858 loss_obj 0.003141 loss_noobj 0.043717: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 118 loss 0.501158 loss_coord 0.439808 loss_obj 0.003079 loss_noobj 0.030558: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 119 loss 0.498544 loss_coord 0.439927 loss_obj 0.002851 loss_noobj 0.030105: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 120 loss 0.663831 loss_coord 0.597796 loss_obj 0.003147 loss_noobj 0.034561: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 121 loss 0.469814 loss_coord 0.393378 loss_obj 0.004286 loss_noobj 0.033573: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 122 loss 0.429475 loss_coord 0.375816 loss_obj 0.002544 loss_noobj 0.028222: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 123 loss 0.491724 loss_coord 0.438831 loss_obj 0.002750 loss_noobj 0.025392: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 124 loss 0.485221 loss_coord 0.432777 loss_obj 0.003164 loss_noobj 0.020807: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 125 loss 0.524904 loss_coord 0.479087 loss_obj 0.002369 loss_noobj 0.022126: 100%|██████████| 128/128 [00:18<00:00,  6.90it/s]\n","[train] Epoch 126 loss 0.543324 loss_coord 0.495770 loss_obj 0.002604 loss_noobj 0.021511: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 127 loss 0.385608 loss_coord 0.338567 loss_obj 0.002589 loss_noobj 0.021151: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 128 loss 0.377685 loss_coord 0.333639 loss_obj 0.002369 loss_noobj 0.020360: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 129 loss 1.887990 loss_coord 0.371663 loss_obj 0.077249 loss_noobj 0.743836: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 130 loss 51.465461 loss_coord 4.181027 loss_obj 4.014880 loss_noobj 7.135635: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 131 loss 90.034515 loss_coord 41.031915 loss_obj 3.858185 loss_noobj 10.420747: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 132 loss 37.069662 loss_coord 4.439573 loss_obj 2.493273 loss_noobj 7.697360: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 133 loss 19.695747 loss_coord 7.625897 loss_obj 0.824469 loss_noobj 3.825164: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 134 loss 19.562700 loss_coord 11.262188 loss_obj 0.557983 loss_noobj 2.720679: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 135 loss 6.764816 loss_coord 1.048789 loss_obj 0.326638 loss_noobj 2.449643: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 136 loss 5.030027 loss_coord 0.790285 loss_obj 0.226234 loss_noobj 1.977404: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 137 loss 2.209425 loss_coord 0.619621 loss_obj 0.065128 loss_noobj 0.938522: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 138 loss 0.997073 loss_coord 0.364644 loss_obj 0.025710 loss_noobj 0.375331: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 139 loss 0.615265 loss_coord 0.198555 loss_obj 0.018350 loss_noobj 0.233208: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 140 loss 0.593731 loss_coord 0.128293 loss_obj 0.014166 loss_noobj 0.323783: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 141 loss 0.346698 loss_coord 0.097387 loss_obj 0.011277 loss_noobj 0.136540: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 142 loss 0.323867 loss_coord 0.101933 loss_obj 0.007382 loss_noobj 0.148114: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 143 loss 0.246523 loss_coord 0.090936 loss_obj 0.007213 loss_noobj 0.083454: 100%|██████████| 128/128 [00:49<00:00,  2.61it/s]\n","[train] Epoch 144 loss 0.250030 loss_coord 0.101334 loss_obj 0.004841 loss_noobj 0.100282: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 145 loss 0.240242 loss_coord 0.137228 loss_obj 0.004619 loss_noobj 0.056826: 100%|██████████| 128/128 [00:12<00:00, 10.15it/s]\n","[train] Epoch 146 loss 0.250073 loss_coord 0.160172 loss_obj 0.003845 loss_noobj 0.051454: 100%|██████████| 128/128 [00:25<00:00,  4.98it/s]\n","[train] Epoch 147 loss 0.241984 loss_coord 0.160847 loss_obj 0.003542 loss_noobj 0.045715: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 148 loss 0.399322 loss_coord 0.317288 loss_obj 0.003311 loss_noobj 0.048926: 100%|██████████| 128/128 [00:12<00:00, 10.23it/s]\n","[train] Epoch 149 loss 0.453620 loss_coord 0.381656 loss_obj 0.003610 loss_noobj 0.035866: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 150 loss 0.525244 loss_coord 0.457422 loss_obj 0.002933 loss_noobj 0.038495: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 151 loss 0.686668 loss_coord 0.613392 loss_obj 0.003737 loss_noobj 0.035904: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 152 loss 0.505370 loss_coord 0.433768 loss_obj 0.003299 loss_noobj 0.038608: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 153 loss 0.410112 loss_coord 0.344071 loss_obj 0.002823 loss_noobj 0.037807: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 154 loss 0.298754 loss_coord 0.239883 loss_obj 0.002951 loss_noobj 0.029358: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 155 loss 0.295495 loss_coord 0.234873 loss_obj 0.002528 loss_noobj 0.035346: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 156 loss 0.355765 loss_coord 0.289674 loss_obj 0.003435 loss_noobj 0.031741: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 157 loss 0.488319 loss_coord 0.396148 loss_obj 0.003449 loss_noobj 0.057680: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 158 loss 0.353370 loss_coord 0.285653 loss_obj 0.002943 loss_noobj 0.038287: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 159 loss 14.093798 loss_coord 1.315300 loss_obj 1.034806 loss_noobj 2.430437: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 160 loss 40.981956 loss_coord 3.106211 loss_obj 2.768272 loss_noobj 10.193026: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 161 loss 9.110228 loss_coord 1.430787 loss_obj 0.427421 loss_noobj 3.405232: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 162 loss 2.547240 loss_coord 0.671846 loss_obj 0.080420 loss_noobj 1.071192: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 163 loss 0.932586 loss_coord 0.298983 loss_obj 0.023822 loss_noobj 0.395382: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 164 loss 0.512794 loss_coord 0.153147 loss_obj 0.012948 loss_noobj 0.230167: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 165 loss 0.312862 loss_coord 0.081958 loss_obj 0.009602 loss_noobj 0.134886: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 166 loss 0.231013 loss_coord 0.053395 loss_obj 0.007217 loss_noobj 0.105447: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 167 loss 0.179413 loss_coord 0.035026 loss_obj 0.005901 loss_noobj 0.085377: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 168 loss 0.146072 loss_coord 0.025554 loss_obj 0.004899 loss_noobj 0.071529: 100%|██████████| 128/128 [00:29<00:00,  4.35it/s]\n","[train] Epoch 169 loss 0.121089 loss_coord 0.018954 loss_obj 0.004135 loss_noobj 0.060786: 100%|██████████| 128/128 [00:48<00:00,  2.66it/s]\n","[train] Epoch 170 loss 0.106145 loss_coord 0.018219 loss_obj 0.003571 loss_noobj 0.052218: 100%|██████████| 128/128 [00:25<00:00,  5.00it/s]\n","[train] Epoch 171 loss 0.097334 loss_coord 0.021163 loss_obj 0.003076 loss_noobj 0.045414: 100%|██████████| 128/128 [00:30<00:00,  4.21it/s]\n","[train] Epoch 172 loss 0.098605 loss_coord 0.031452 loss_obj 0.002731 loss_noobj 0.039844: 100%|██████████| 128/128 [00:17<00:00,  7.50it/s]\n","[train] Epoch 173 loss 0.119150 loss_coord 0.059791 loss_obj 0.002380 loss_noobj 0.035555: 100%|██████████| 128/128 [00:12<00:00, 10.20it/s]\n","[train] Epoch 174 loss 0.194886 loss_coord 0.139843 loss_obj 0.002368 loss_noobj 0.031361: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 175 loss 0.307339 loss_coord 0.255920 loss_obj 0.002162 loss_noobj 0.029796: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 176 loss 0.428720 loss_coord 0.378274 loss_obj 0.002352 loss_noobj 0.026925: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 177 loss 0.464893 loss_coord 0.415009 loss_obj 0.002200 loss_noobj 0.027886: 100%|██████████| 128/128 [00:12<00:00, 10.38it/s]\n","[train] Epoch 178 loss 0.463420 loss_coord 0.413983 loss_obj 0.002234 loss_noobj 0.027093: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 179 loss 0.469069 loss_coord 0.415242 loss_obj 0.002472 loss_noobj 0.029105: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 180 loss 0.334858 loss_coord 0.289446 loss_obj 0.002192 loss_noobj 0.023491: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 181 loss 0.280692 loss_coord 0.241563 loss_obj 0.001777 loss_noobj 0.021360: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 182 loss 0.264382 loss_coord 0.225540 loss_obj 0.001877 loss_noobj 0.020070: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 183 loss 0.254684 loss_coord 0.217951 loss_obj 0.001590 loss_noobj 0.020831: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 184 loss 0.284228 loss_coord 0.248708 loss_obj 0.001785 loss_noobj 0.017669: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 185 loss 0.349678 loss_coord 0.315922 loss_obj 0.001736 loss_noobj 0.016397: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 186 loss 0.325720 loss_coord 0.287122 loss_obj 0.002379 loss_noobj 0.014808: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 187 loss 0.335492 loss_coord 0.296257 loss_obj 0.001568 loss_noobj 0.023558: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 188 loss 0.336773 loss_coord 0.305927 loss_obj 0.001489 loss_noobj 0.015957: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 189 loss 0.357569 loss_coord 0.321288 loss_obj 0.002045 loss_noobj 0.015828: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 190 loss 0.371124 loss_coord 0.335007 loss_obj 0.001833 loss_noobj 0.017785: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 191 loss 0.378512 loss_coord 0.340776 loss_obj 0.001703 loss_noobj 0.020709: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 192 loss 0.320640 loss_coord 0.279571 loss_obj 0.002463 loss_noobj 0.016444: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 193 loss 0.421975 loss_coord 0.274293 loss_obj 0.005495 loss_noobj 0.092734: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 194 loss 34.863501 loss_coord 1.845701 loss_obj 2.631361 loss_noobj 6.704189: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 195 loss 18.123439 loss_coord 1.455711 loss_obj 1.095870 loss_noobj 5.709032: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 196 loss 10.741349 loss_coord 1.350173 loss_obj 0.632538 loss_noobj 3.065800: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 197 loss 2.575016 loss_coord 0.699636 loss_obj 0.091255 loss_noobj 0.962829: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 198 loss 0.836812 loss_coord 0.368123 loss_obj 0.017910 loss_noobj 0.289592: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 199 loss 0.441947 loss_coord 0.165909 loss_obj 0.011105 loss_noobj 0.164984: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 200 loss 0.302427 loss_coord 0.104498 loss_obj 0.008107 loss_noobj 0.116862: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 201 loss 0.221642 loss_coord 0.066545 loss_obj 0.006386 loss_noobj 0.091235: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 202 loss 0.174326 loss_coord 0.047104 loss_obj 0.005259 loss_noobj 0.074630: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 203 loss 0.140340 loss_coord 0.033504 loss_obj 0.004444 loss_noobj 0.062398: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 204 loss 0.122226 loss_coord 0.031006 loss_obj 0.003798 loss_noobj 0.053241: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 205 loss 0.113845 loss_coord 0.035180 loss_obj 0.003241 loss_noobj 0.046252: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 206 loss 0.124861 loss_coord 0.055592 loss_obj 0.002915 loss_noobj 0.040119: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 207 loss 0.119777 loss_coord 0.058613 loss_obj 0.002515 loss_noobj 0.036017: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 208 loss 0.112306 loss_coord 0.058109 loss_obj 0.002310 loss_noobj 0.031094: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 209 loss 0.124268 loss_coord 0.076164 loss_obj 0.002000 loss_noobj 0.028100: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 210 loss 0.155249 loss_coord 0.111648 loss_obj 0.001817 loss_noobj 0.025431: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 211 loss 0.211777 loss_coord 0.170323 loss_obj 0.001815 loss_noobj 0.023308: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 212 loss 0.241013 loss_coord 0.203510 loss_obj 0.001531 loss_noobj 0.022192: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 213 loss 0.309257 loss_coord 0.272437 loss_obj 0.001576 loss_noobj 0.021059: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 214 loss 0.328035 loss_coord 0.292758 loss_obj 0.001595 loss_noobj 0.019329: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 215 loss 0.366100 loss_coord 0.330421 loss_obj 0.001761 loss_noobj 0.018067: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 216 loss 0.339836 loss_coord 0.303278 loss_obj 0.001723 loss_noobj 0.019327: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 217 loss 0.387320 loss_coord 0.352746 loss_obj 0.001639 loss_noobj 0.018181: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 218 loss 0.322656 loss_coord 0.291668 loss_obj 0.001404 loss_noobj 0.016947: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 219 loss 0.256846 loss_coord 0.226216 loss_obj 0.001549 loss_noobj 0.015138: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 220 loss 0.261127 loss_coord 0.231591 loss_obj 0.001457 loss_noobj 0.014964: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 221 loss 0.191346 loss_coord 0.163686 loss_obj 0.001372 loss_noobj 0.013937: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 222 loss 0.164373 loss_coord 0.141319 loss_obj 0.001154 loss_noobj 0.011511: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 223 loss 0.189972 loss_coord 0.166646 loss_obj 0.001151 loss_noobj 0.011818: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 224 loss 0.220669 loss_coord 0.198340 loss_obj 0.001169 loss_noobj 0.010641: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 225 loss 0.256483 loss_coord 0.233258 loss_obj 0.001260 loss_noobj 0.010627: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 226 loss 0.312927 loss_coord 0.289838 loss_obj 0.001261 loss_noobj 0.010475: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 227 loss 0.287809 loss_coord 0.258743 loss_obj 0.001305 loss_noobj 0.016018: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 228 loss 15.524280 loss_coord 1.000705 loss_obj 1.170736 loss_noobj 2.816219: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 229 loss 34.250629 loss_coord 2.845008 loss_obj 2.295679 loss_noobj 8.448836: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 230 loss 6.197650 loss_coord 1.211589 loss_obj 0.220917 loss_noobj 2.776895: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 231 loss 2.156044 loss_coord 0.536980 loss_obj 0.066670 loss_noobj 0.952363: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 232 loss 0.661672 loss_coord 0.267736 loss_obj 0.016339 loss_noobj 0.230545: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 233 loss 0.327927 loss_coord 0.117140 loss_obj 0.007781 loss_noobj 0.132981: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 234 loss 0.232501 loss_coord 0.079015 loss_obj 0.006440 loss_noobj 0.089083: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 235 loss 0.174716 loss_coord 0.051771 loss_obj 0.005027 loss_noobj 0.072671: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 236 loss 0.140704 loss_coord 0.039181 loss_obj 0.004178 loss_noobj 0.059745: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 237 loss 0.113114 loss_coord 0.027246 loss_obj 0.003515 loss_noobj 0.050718: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 238 loss 0.096515 loss_coord 0.023020 loss_obj 0.003009 loss_noobj 0.043406: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 239 loss 0.089087 loss_coord 0.025308 loss_obj 0.002610 loss_noobj 0.037678: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 240 loss 0.083866 loss_coord 0.027823 loss_obj 0.002290 loss_noobj 0.033145: 100%|██████████| 128/128 [00:19<00:00,  6.46it/s]\n","[train] Epoch 241 loss 0.074166 loss_coord 0.024421 loss_obj 0.002040 loss_noobj 0.029342: 100%|██████████| 128/128 [00:20<00:00,  6.10it/s]\n","[train] Epoch 242 loss 0.072029 loss_coord 0.027849 loss_obj 0.001840 loss_noobj 0.025778: 100%|██████████| 128/128 [00:19<00:00,  6.49it/s]\n","[train] Epoch 243 loss 0.094580 loss_coord 0.054334 loss_obj 0.001643 loss_noobj 0.023817: 100%|██████████| 128/128 [00:12<00:00, 10.19it/s]\n","[train] Epoch 244 loss 0.143508 loss_coord 0.106701 loss_obj 0.001538 loss_noobj 0.021432: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 245 loss 0.188159 loss_coord 0.154291 loss_obj 0.001451 loss_noobj 0.019356: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 246 loss 0.236903 loss_coord 0.203580 loss_obj 0.001441 loss_noobj 0.018915: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 247 loss 0.340978 loss_coord 0.309335 loss_obj 0.001349 loss_noobj 0.018152: 100%|██████████| 128/128 [00:12<00:00, 10.37it/s]\n","[train] Epoch 248 loss 0.438009 loss_coord 0.403565 loss_obj 0.001712 loss_noobj 0.017327: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 249 loss 0.309591 loss_coord 0.278027 loss_obj 0.001415 loss_noobj 0.017413: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 250 loss 0.293034 loss_coord 0.264202 loss_obj 0.001325 loss_noobj 0.015577: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 251 loss 0.282209 loss_coord 0.255141 loss_obj 0.001293 loss_noobj 0.014139: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 252 loss 0.201910 loss_coord 0.176077 loss_obj 0.001295 loss_noobj 0.012887: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 253 loss 0.193438 loss_coord 0.169330 loss_obj 0.001054 loss_noobj 0.013566: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 254 loss 0.176197 loss_coord 0.153840 loss_obj 0.001105 loss_noobj 0.011308: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 255 loss 0.188738 loss_coord 0.167756 loss_obj 0.001050 loss_noobj 0.010480: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 256 loss 0.165867 loss_coord 0.146039 loss_obj 0.001007 loss_noobj 0.009761: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 257 loss 0.186431 loss_coord 0.167503 loss_obj 0.001038 loss_noobj 0.008545: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 258 loss 0.196407 loss_coord 0.178099 loss_obj 0.000882 loss_noobj 0.009493: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 259 loss 0.247107 loss_coord 0.228395 loss_obj 0.001073 loss_noobj 0.007986: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 260 loss 0.273163 loss_coord 0.254606 loss_obj 0.000987 loss_noobj 0.008683: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 261 loss 0.252879 loss_coord 0.235398 loss_obj 0.000994 loss_noobj 0.007536: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 262 loss 0.286177 loss_coord 0.266027 loss_obj 0.001089 loss_noobj 0.009259: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 263 loss 0.280258 loss_coord 0.262531 loss_obj 0.000940 loss_noobj 0.008324: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 264 loss 0.241786 loss_coord 0.222208 loss_obj 0.001292 loss_noobj 0.006657: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 265 loss 0.202750 loss_coord 0.184944 loss_obj 0.000796 loss_noobj 0.009843: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 266 loss 0.254798 loss_coord 0.237436 loss_obj 0.001102 loss_noobj 0.006342: 100%|██████████| 128/128 [00:12<00:00, 10.36it/s]\n","[train] Epoch 267 loss 0.312760 loss_coord 0.283662 loss_obj 0.001418 loss_noobj 0.014913: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 268 loss 14.210774 loss_coord 0.972603 loss_obj 1.029784 loss_noobj 2.940336: 100%|██████████| 128/128 [00:12<00:00, 10.35it/s]\n","[train] Epoch 269 loss 23.595265 loss_coord 1.436788 loss_obj 1.576034 loss_noobj 6.398136: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 270 loss 10.774046 loss_coord 1.181144 loss_obj 0.643099 loss_noobj 3.161909: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 271 loss 2.437650 loss_coord 0.629913 loss_obj 0.108806 loss_noobj 0.719680: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 272 loss 0.649667 loss_coord 0.281932 loss_obj 0.013854 loss_noobj 0.229199: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 273 loss 0.341676 loss_coord 0.142292 loss_obj 0.008051 loss_noobj 0.118871: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 274 loss 0.221228 loss_coord 0.076632 loss_obj 0.005416 loss_noobj 0.090434: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 275 loss 0.158021 loss_coord 0.044374 loss_obj 0.005056 loss_noobj 0.063083: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 276 loss 0.123458 loss_coord 0.028799 loss_obj 0.003597 loss_noobj 0.058692: 100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n","[train] Epoch 277 loss 0.100445 loss_coord 0.023744 loss_obj 0.003376 loss_noobj 0.042938: 100%|██████████| 128/128 [00:12<00:00, 10.31it/s]\n","[train] Epoch 278 loss 0.088456 loss_coord 0.023882 loss_obj 0.002617 loss_noobj 0.038402: 100%|██████████| 128/128 [00:12<00:00, 10.28it/s]\n","[train] Epoch 279 loss 0.080948 loss_coord 0.025215 loss_obj 0.002385 loss_noobj 0.031879: 100%|██████████| 128/128 [00:15<00:00,  8.15it/s]\n","[train] Epoch 280 loss 0.074400 loss_coord 0.025495 loss_obj 0.002075 loss_noobj 0.028153: 100%|██████████| 128/128 [00:29<00:00,  4.34it/s]\n","[train] Epoch 281 loss 0.071894 loss_coord 0.028411 loss_obj 0.001863 loss_noobj 0.024852: 100%|██████████| 128/128 [00:47<00:00,  2.67it/s]\n","[train] Epoch 282 loss 0.072475 loss_coord 0.033842 loss_obj 0.001613 loss_noobj 0.022498: 100%|██████████| 128/128 [00:12<00:00, 10.57it/s]\n","[train] Epoch 283 loss 0.080755 loss_coord 0.045835 loss_obj 0.001448 loss_noobj 0.020444: 100%|██████████| 128/128 [00:12<00:00, 10.54it/s]\n","[train] Epoch 284 loss 0.115777 loss_coord 0.083807 loss_obj 0.001320 loss_noobj 0.018771: 100%|██████████| 128/128 [00:12<00:00, 10.49it/s]\n","[train] Epoch 285 loss 0.161285 loss_coord 0.131764 loss_obj 0.001208 loss_noobj 0.017438: 100%|██████████| 128/128 [00:12<00:00, 10.46it/s]\n","[train] Epoch 286 loss 0.199062 loss_coord 0.171375 loss_obj 0.001165 loss_noobj 0.016039: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 287 loss 0.281821 loss_coord 0.255252 loss_obj 0.001118 loss_noobj 0.015387: 100%|██████████| 128/128 [00:12<00:00, 10.21it/s]\n","[train] Epoch 288 loss 0.347123 loss_coord 0.321400 loss_obj 0.001150 loss_noobj 0.014227: 100%|██████████| 128/128 [00:12<00:00, 10.26it/s]\n","[train] Epoch 289 loss 0.339202 loss_coord 0.313170 loss_obj 0.001126 loss_noobj 0.014768: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 290 loss 0.268097 loss_coord 0.242745 loss_obj 0.001065 loss_noobj 0.014702: 100%|██████████| 128/128 [00:12<00:00, 10.34it/s]\n","[train] Epoch 291 loss 0.203927 loss_coord 0.180419 loss_obj 0.001107 loss_noobj 0.012442: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 292 loss 0.215834 loss_coord 0.195065 loss_obj 0.001014 loss_noobj 0.010627: 100%|██████████| 128/128 [00:12<00:00, 10.29it/s]\n","[train] Epoch 293 loss 0.211508 loss_coord 0.191737 loss_obj 0.000947 loss_noobj 0.010302: 100%|██████████| 128/128 [00:12<00:00, 10.33it/s]\n","[train] Epoch 294 loss 0.257666 loss_coord 0.237648 loss_obj 0.000909 loss_noobj 0.010931: 100%|██████████| 128/128 [00:12<00:00, 10.32it/s]\n","[train] Epoch 295 loss 0.231404 loss_coord 0.212157 loss_obj 0.000923 loss_noobj 0.010022: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 296 loss 0.222033 loss_coord 0.205452 loss_obj 0.000833 loss_noobj 0.008246: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 297 loss 0.202614 loss_coord 0.186986 loss_obj 0.000813 loss_noobj 0.007500: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 298 loss 0.192525 loss_coord 0.177529 loss_obj 0.000764 loss_noobj 0.007352: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n","[train] Epoch 299 loss 0.197461 loss_coord 0.182833 loss_obj 0.000834 loss_noobj 0.006293: 100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n"]}],"source":["from tqdm import tqdm\n","lambda_coord = 1\n","lambda_obj = 10\n","lambda_noobj = 1\n","conf = 0.5\n","best_loss = 99999\n","for epoch in range(300):\n","  total_train_loss = 0\n","  total_train_loss_coord = 0\n","  total_train_loss_obj = 0\n","  total_train_loss_noobj = 0\n","  with tqdm(train_loader) as pbar:\n","    pbar.set_description(\"[train] Epoch %d\" % epoch)\n","    for n , (img , scale3_label , scale2_label ,scale1_label) in enumerate(pbar):\n","      optimizer.zero_grad()\n","      img = img.cuda()\n","      scale1_label = scale1_label.cuda()\n","      scale2_label = scale2_label.cuda()\n","      scale3_label = scale3_label.cuda()\n","      labels = [scale3_label , scale2_label ,scale1_label]\n","      preds  = list(model(img))\n","      loss_coord = 0\n","      loss_obj = 0\n","      loss_noobj = 0\n","      for label , pred in zip(labels , preds):\n","        _loss_coord , _loss_obj , _loss_noobj = bbox_metric(pred , label)\n","        loss_coord += _loss_coord\n","        loss_obj += _loss_obj\n","        loss_noobj += _loss_noobj\n","      loss = lambda_coord*loss_coord + lambda_obj*loss_obj + lambda_noobj*loss_noobj\n","      total_train_loss += loss.item()\n","      total_train_loss_coord += loss_coord.item()\n","      total_train_loss_obj += loss_obj.item()\n","      total_train_loss_noobj += loss_noobj.item()\n","      loss.backward()\n","      optimizer.step()\n","      pbar.set_description(\"[train] Epoch %d loss %f loss_coord %f loss_obj %f loss_noobj %f\" % (epoch ,total_train_loss/(n+1),total_train_loss_coord/(n+1) , total_train_loss_obj/(n+1),total_train_loss_noobj/(n+1)))\n","      if best_loss > total_train_loss/(n+1):\n","        model_path = 'model.pth'\n","        torch.save(model.state_dict(), model_path)\n","        best_loss = total_train_loss/(n+1)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1765095973511,"user":{"displayName":"西宏章","userId":"00237858890977261979"},"user_tz":-540},"id":"ZYjXPB0ND8zQ"},"outputs":[],"source":["lambda_coord = 1\n","lambda_obj = 10\n","lambda_noobj = 1"]},{"cell_type":"markdown","metadata":{"id":"SrGwGrGUseUE"},"source":["## 結果確認\n","\n","まぁまぁといったところか\n","- 学習量も少なく、データ拡張も行っていないため、この程度であろう"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fnDHpXj3iQSI2I0oLNeWjg0EI9yFgiC7"},"id":"1rZUOYDvD-_y","outputId":"4e1508f6-e329-4658-c059-e1e5e95c3d00","executionInfo":{"status":"ok","timestamp":1765096026620,"user_tz":-540,"elapsed":53108,"user":{"displayName":"西宏章","userId":"00237858890977261979"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["model_path = 'model.pth'\n","model.load_state_dict(torch.load(model_path))\n","model = model.cuda()\n","for path in img_list:\n","  img = cv2.imread(path)\n","  img = cv2.resize(img , (img_size , img_size))\n","  img = Image.fromarray(img)\n","\n","  img = transform(img).unsqueeze(0).cuda()\n","  with torch.no_grad():\n","    preds  = list(model(img))\n","  img = cv2.imread(path)[:,:,::-1]\n","  img = cv2.resize(img , (img_size , img_size))\n","  img = torch.tensor(img.transpose(2,0,1))\n","  for color,pred in zip([\"red\",\"green\",\"blue\"],preds):\n","    bbox_list, obj_list = visualization(pred, anchor, img_size, conf = 0.9)\n","    if len(bbox_list) != 0:\n","      obj_label_list = []\n","      for obj_item in obj_list:\n","        obj_label_list.append(obj_labels[obj_item])\n","      img = draw_bounding_boxes(img, torch.tensor(bbox_list), colors=color, width=1, labels=obj_label_list)\n","  show(img)"]},{"cell_type":"markdown","metadata":{"id":"rUvcL8OH7q54"},"source":["# YoLoのその後\n","\n","YoLoの生みの親、Joseph Redmonは、先に述べた通りYoLo V3で開発をやめ、ソーシャルメディアも閉じている\n","\n","YoLov3の論文も、それまでとはことなり、口語調で記述されており、ある意味雑な論文である\n","- 例えば、次のような文章から論文は始まる\"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry.\"\n","- 訳(DeepL)：YOLO！のいくつかのアップデートを紹介します。より良いものにするために、たくさんの小さなデザイン変更を行いました。また、この新しいネットワークはとても素晴らしいものです。前回より少し大きくなりましたが、より正確になっています。それでもまだ速いので、ご心配なく。\n","\n","開発から退いた理由も、論文の最後に記載されている\n","\n","- \"What are we going to do with these detectors now that we have them?\"\n","- 訳：このような物体検出器を使って何をするんだい？\n","\n","- \"A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and deﬁnitely won't be used to harvest your personal information and sell it to…. wait, you're saying that's exactly what it will be used for?? Oh.\"\n","- 訳(DeepL)：この研究をしている人たちの多くは、GoogleやFacebookにいます。少なくとも私たちは、この技術が良い人たちの手に渡り、あなたの個人情報を採取して売るために使われることはないだろうと思っています。ああ。\n","\n","- \"Well the other people heavily funding vision research are the military and they've never done anything horrible like killing lots of people with new technology oh wait…..\"\n","- 訳(DeepL)：ビジョン研究に多額の資金を提供しているのは軍で、彼らは新しい技術で多くの人を殺すような恐ろしいことはしていません。おっとまてよ...\n","\n","- \"The author is funded by the Ofﬁce of Naval Research and Google.\"\n","- 訳(DeepL)：著者は、Ofﬁce of Naval ResearchとGoogleから資金提供を受けている。\n","\n","これらの記述から、研究成果の悪用が許せなかったと考えられます\n","- YoLoを生み出したことで、その責任が問われるような事態を想定したのかもしれません\n","\n","\n","最後に、次のようなtwitterを投稿している\n","\n","<img src=\"http://class.west.sd.keio.ac.jp/dataai/text/jrtwitter.png\" width=500>\n"]},{"cell_type":"markdown","metadata":{"id":"FFnWkuqTvETf"},"source":["# YOLOのさらにその後\n","\n","\n","YoLoという名前は物体検出において著名なため、その後もYoLoという名前で開発は継続されている\n","\n","## YOLOv4\n","\n","- YOLOv4はシングルショットで物体を検出するYOLOシリーズであり、YOLOv3までの作者のJoseph Redmonは2020年2月に開発終了を発表した後、YOLOv4はDarknetのWindows版を開発しているAlexey Bochkovskyによって開発された\n","\n","特長として、YOLOv4はBackboneにCSPDarknet53、NeckにSPP(Spatial pyramid pooling)、PAN(Path Aggregation Network)、HeadにYOLOv3を使用している\n","\n","- CPSDarknetの基本であるCSPNetはFeatureMapの一部のみをConvして残りをConcatすることで精度を落とさずに高速化する手法\n","\n","- SPPは複数のカーネルサイズ（1,5,9,13）で同時にPoolingすることで精細な情報と広域な情報の両方を取得する方法\n","\n","- PANは異なるバックボーンレベルから特徴量をDetectorに伝えることで、入力に近い層の情報を活用する手法\n","\n","YOLOv3と比較して、mAP(Mean Average Precision)が向上している\n","\n","## YOLOv5\n","\n","Ultralytics社により開発されたモデルであり、YOLOv4よりも性能が向上したとされている\n","\n","商用利用が限定されている\n","\n","## YOLOv6\n","\n","2023年1月に更新されYOLOv6(v3.0)となった\n","- YOLOv7やYOLOv8の性能を上回っている\n","\n","YOLO開発者間で競うという謎な状態\n","\n","## YOLOv7\n","\n","YOLOv7は、YOLOv4、Scaled-YOLOv4, YOLORと同じグループが開発した\n","\n","特長\n","- 基本アーキテクチャにELAN, E-ELANを使用  \n","DenseNetやCSPDarknetなどのconcatenateベースのモデルを進化させた構造\n","- 複合スケーリング  \n","concatenateモデルに適した複合スケーリング方法であり、モデルをスケーリングする際にはその一部だけではなく、モデルのdepth(層数)とwidth(ノード数)などの、全体を同時にスケーリングさせる手法である\n","- re-parameterizationの適用  \n","RepVGGなどと同じ手法を適用し、学習時は推論時と別の構造を持つネットワークを用いることで、ラベル割り当て戦略の最適化している\n","- ベースのラベル割り当て戦略にOTAを利用\n","- Auxiliary Lossを併用してラベル割り当てを行う\n","\n","## YOLOv8\n","\n","YOLOv8は、YOLOv5の公開元であるUltralytics社が公開\n","- 大規模データセットでの学習だけでなく、object detection、segmentation、classificationタスクで利用可能である\n","- モデルや損失関数が更新されており、nchor-free detection headが導入されている"]},{"cell_type":"markdown","metadata":{"id":"YbUTajSeEzwc"},"source":["# 課題\n","データオーグメンテーションを実装しなさい\n","- どのような形でも構わないが、ここではBounding Boxも変換することを忘れずに"]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyOmn3EVOgOudJBM3/zdVN69"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}